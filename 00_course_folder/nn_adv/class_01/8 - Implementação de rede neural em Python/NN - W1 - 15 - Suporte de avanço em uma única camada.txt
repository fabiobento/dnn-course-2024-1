se você tivesse que implementar a propagação direta do zero em python, como faria isso, além de obter intuição sobre o que realmente está acontecendo em bibliotecas como TensorFlow e PyTorch. Se algum dia você decidir criar algo ainda melhor do que o TensorFlow e o PyTorch, talvez agora tenha uma ideia melhor em casa, eu realmente não recomendo fazer isso para a maioria das pessoas.
Reproduza o vídeo começando em ::30 e siga a transcrição0:30
Mas talvez algum dia alguém crie uma estrutura ainda melhor do que o TensorFlow e o PyTorch, e quem fizer isso pode acabar tendo que implementar essas coisas do zero. Então, vamos dar uma olhada, neste slide, vou examinar um pouco de código e você verá todo esse código novamente mais tarde no laboratório opcional, assim como no laboratório prático. Portanto, não se preocupe em ter que fazer anotações em cada linha de código ou memorizar cada linha de código. Você vê esse código escrito no caderno de Júpiter no laboratório e o objetivo deste vídeo é apenas mostrar o código para garantir que você entenda o que ele está fazendo. Assim, quando você for ao laboratório opcional e ao laboratório prático e ver o código lá, saiba o que fazer, então não se preocupe em fazer anotações detalhadas em cada linha. Se você puder ler o código neste slide e entender o que ele está fazendo, é tudo o que você precisa. Então, vamos dar uma olhada em como você implementa o suporte dianteiro em uma única camada. Vamos continuar usando o modelo de torrefação de café mostrado aqui. E vamos ver como você pegaria um vetor de recurso de entrada x e implementaria a prop direta para obter essa saída a2. Nesta implementação de python, vou usar matrizes 1D para representar todos esses vetores e parâmetros, e é por isso que há apenas um único colchete aqui. Essa é uma matriz 1D em python em vez de uma matriz 2D, que é o que tínhamos quando tínhamos colchetes duplos. Então, o primeiro valor que você precisa calcular é um colchete super strip 1 subscrito 1, que é o primeiro valor de ativação de a1 e que é g dessa expressão aqui. Então, vou usar a convenção neste slide de que em um termo como w2, 1, vou representar como uma variável w2 e, em seguida, o subscrito 1. Esse sublinhado denota um subscrito, denota um subscrito, então w2 significa w sobrescrito 2 entre colchetes e, em seguida, subscrito 1.
Reproduza o vídeo começando em :2:36 e siga a transcrição2:36
Então, para calcular a1_1, temos os parâmetros w1_1 e b1_1, que são, digamos, 1_2 e -1.
Reproduza o vídeo começando em :2:54 e siga a transcrição2:54
Você então calcularia z1_1 como o produto escalar entre esse parâmetro w1_1 e a entrada x e adicionaria a b1_1 e, finalmente, a1_1 seria igual a g, a função sigmóide aplicada a z1_1.
Reproduza o vídeo começando em :3:11 e siga a transcrição3:11
Em seguida, vamos computar a1_2, que, novamente, pela convenção que descrevi aqui, será a1_2, escrito assim.
Reproduza o vídeo começando em :3:25 e siga a transcrição3:25
Tão parecido com o que fizemos à esquerda, w1_2 são dois parâmetros -3, 4, b1_2 é o termo, b 1, 2 ali, então você calcula z como esse termo no meio e então aplica a função sigmóide e então você acaba com um 1_2, e finalmente você faz a mesma coisa para calcular a1_3.
Reproduza o vídeo começando em :3:52 e siga a transcrição3:52
Agora, você calculou esses três valores, a1_1, a1_2 e a1_3, e gostamos de pegar esses três números e agrupá-los em uma matriz para obter a1 aqui em cima, que é a saída da primeira camada. E então você faz isso agrupando-os usando uma matriz np da seguinte forma, então agora que você calculou a_1, vamos implementar a segunda camada também.
Reproduza o vídeo começando em :4:21 e siga a transcrição4:21
Então você calcula a saída a2, então a2 é calculada usando essa expressão e, portanto, teríamos os parâmetros w2_1 e b2_1 correspondentes a esses parâmetros. E então você calcularia z como o produto escalar entre w2_1 e a1, adicionaria b2_1 e, em seguida, aplicaria a função sigmóide para obter a2_1 e pronto, é assim que você implementa forward prop usando apenas python e np. Agora, há muitas expressões nesta página de código que você acabou de ver. No próximo vídeo, vamos ver como você pode simplificar isso para implementar a prop direta para uma rede neural mais geral, em vez de codificá-la para cada neurônio, como acabamos de fazer. Então, vamos ver isso no próximo vídeo.
