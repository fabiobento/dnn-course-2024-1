Uma das razões pelas quais os pesquisadores de aprendizado profundo conseguiram ampliar as redes neurais e consideraram redes neurais realmente grandes na última década é porque as redes neurais podem ser vetorizadas. Eles podem ser implementados de forma muito eficiente usando multiplicações de matrizes. Acontece que o hardware de computação paralela, incluindo GPUs, mas também algumas funções da CPU são muito bons para fazer multiplicações de matrizes muito grandes. Neste vídeo, veremos como essas implementações vetorizadas de redes neurais funcionam. Sem essas ideias, não acho que o aprendizado profundo estaria nem perto de um sucesso e de uma escala hoje em dia. Aqui à esquerda está o código que você viu anteriormente de como você implementaria a prop direta, ou propagação direta, em uma única camada. X aqui está a entrada, W, os pesos do primeiro, segundo e terceiro neurônios, digamos, parâmetros B, e então esse é o mesmo código que vimos antes. Isso resultará em três números, digamos, desse tipo. Se você realmente implementar esse cálculo, obterá 1, 0, 1. Acontece que você pode desenvolver uma implementação vetorizada dessa função da seguinte maneira. Defina X para ser igual a isso. Observe os colchetes duplos. Agora é uma matriz 2D, como no TensorFlow. W é o mesmo de antes, e B, agora estou usando B, também é uma matriz 2D de um por três. Então, acontece que todas essas etapas, isto é, para o loop interno, podem ser substituídas por apenas algumas linhas de código, Z é igual a np.matmul. Matmul é como o NumPy realiza a multiplicação de matrizes. Onde agora X e W são matrizes, então você simplesmente os multiplica. Acontece que, para loop, todas essas linhas de código podem ser substituídas por apenas algumas linhas de código, o que fornece uma implementação vetorizada dessa função.
Reproduza o vídeo começando em :2:25 e siga a transcrição2:25
Você calcula Z, que agora é uma matriz novamente, como numpy.matmul entre A in e W, onde aqui A in e W são ambas matrizes, e matmul é como NumPy realiza uma multiplicação de matrizes. Ele multiplica duas matrizes e, em seguida, adiciona a matriz B a ela. Então A saída é igual à função de ativação g, que é a função sigmóide, aplicada elemento a elemento a essa matriz Z, e então você finalmente retorna A para fora. Essa é a aparência do código. Observe que, na implementação vetorizada, todas essas quantidades, x, que são inseridas no valor de A in e W, B, bem como Z e A out, agora são matrizes 2D. Tudo isso são matrizes. Isso acaba sendo uma implementação muito eficiente de uma etapa de propagação direta por meio de uma camada densa na rede neural. Este é o código para uma implementação vetorizada de prop direto em uma rede neural. Mas o que esse código está fazendo e como ele realmente funciona? O que esse matmul está realmente fazendo? Nos próximos dois vídeos, ambos também opcionais, veremos a multiplicação de matrizes e como ela funciona. Se você estiver familiarizado com álgebra linear, se estiver familiarizado com vetores, matrizes, transposições e multiplicações de matrizes, você pode dar uma olhada rápida nesses dois vídeos e ir para o último vídeo desta semana. Então, no último vídeo desta semana, também opcional, entraremos em mais detalhes para explicar como o matmul oferece essa implementação vetorizada. Vamos ao próximo vídeo, onde veremos o que é multiplicação de matrizes.
