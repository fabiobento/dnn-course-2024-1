Sem mais delongas, vamos entrar na implementação vetorizada de uma rede neural. Examinaremos o código que você viu em um vídeo anterior e, com sorte, Matmul, que é o cálculo da multiplicação de matrizes, faça mais sentido. Vamos começar. Você viu anteriormente como você pode pegar a matriz A e calcular A transposição vezes W, resultando nesta matriz aqui, Z. No código, se essa é a matriz A, essa é uma matriz NumPy com os elementos correspondentes ao que eu escrevi no topo, então a transposição A, que vou escrever como AT, será essa matriz aqui, com novamente as colunas de A agora dispostas em linhas. A propósito, em vez de configurar AT dessa forma, outra forma de calcular AT em NumPy, escreveremos AT igual a A.T. Essa é a função de transposição que pega as colunas de uma matriz e as coloca ao lado. Em código, veja como você inicializa a matriz W como outra matriz NumPy 2D.
Reproduza o vídeo começando em :1:19 e siga a transcrição1:19
Então, para calcular Z igual a A transposição vezes W, você escreverá Z igual a np.matmul, AT, W, e isso computará essa matriz Z aqui, fornecendo esse resultado aqui embaixo. A propósito, se você ler o código de outra pessoa, às vezes verá que Z é igual a AT e depois @ W. Essa é uma forma alternativa de chamar a função matmal. Embora eu ache que usar np.matmul é mais claro. Na chamada que você vê nesta classe, usamos apenas a função matmal assim em vez desta @. Vamos ver a aparência de uma implementação vetorizada de prop avançado. Vou definir A transposição para ser igual aos valores do recurso de entrada 217. Esses são apenas os valores usuais dos recursos de entrada, 200 graus de torrefação de café por 17 minutos. Essa é uma matriz de um por dois.
Reproduza o vídeo começando em :2:25 e siga a transcrição2:25
Vou pegar os parâmetros w_1, w_2 e w_3 e empilhá-los em colunas como essa para formar essa matriz W. Os valores b_1, b_2, b_3, vou colocá-los em uma matriz de um por três, que é essa matriz B da seguinte forma. Então, acontece que, se você computar Z igual a A, transpor W mais B, isso resultará nesses três números e isso será calculado tomando os valores do recurso de entrada e multiplicando-os pela primeira coluna e, em seguida, adicionando B para obter 165. Tomando esses valores de recursos, produzindo pontos com a segunda coluna, esse é um peso w_2 e adicionando b_2 para obter menos 531. Esses recursos valorizam o produto com os pesos w_3 mais b_3 para obter 900. Sinta-se à vontade para pausar o vídeo se quiser verificar esses cálculos.
Reproduza o vídeo começando em :3:30 e siga a transcrição3:30
Mas isso fornece os valores de z ^ 1_1, Z ^ 1_2 e Z ^ 1_3.
Reproduza o vídeo começando em :3:40 e siga a transcrição3:40
Então, finalmente, se a função g aplica a função sigmóide a esses três números elemento a elemento, ou seja, aplica a função sigmóide a 165, a menos 531 e a 900, então você acaba com A igual g dessa matriz Z acaba sendo 1,0,1. É 1,0,1 porque o sigmóide de 165 está tão próximo de um que até o arredondamento numérico é baseado em um e essas são as bases 0 e 1. Vamos ver como você implementa isso no código. Uma transposição é igual a isso, essa é uma matriz de um por dois de 217. A matriz W é essa matriz de dois por três, e B, essa é uma matriz de um por três. A forma como você pode implementar prop direto em uma camada é uma entrada densa. A transposição W b é igual a z é igual a matmul A transpose vezes W mais b. Isso apenas implementa essa linha de código. Então a_out, que é a saída dessa camada, é igual a g, a função de ativação aplicada elemento a elemento a essa matriz Z. Você retorna a_out e isso fornece esse valor. Caso você esteja comparando este slide com o slide de alguns vídeos atrás, havia apenas uma pequena diferença, que era, por convenção, a forma como isso é implementado no TensorFlow. Em vez de chamar essa variável A, T, a chamávamos de A_in, e é por isso que essa também é a implementação correta do código. Há uma convenção no TensorFlow de que exemplos individuais são, na verdade, dispostos em linhas na matriz X e não na transposição da matriz X, e é por isso que a implementação do código realmente se parece com isso no TensorFlow. Mas isso explica por que, com apenas algumas linhas de código, você pode implementar prop direto na rede neural e, além disso, obter um grande bônus porque os computadores modernos são muito bons em implementar multiplicações de matrizes, como o matmul, de forma eficiente. Esse é o último vídeo desta semana. Obrigado por ficar comigo até o final desses vídeos opcionais. Durante o resto desta semana, espero que você também dê uma olhada nos questionários e nos laboratórios práticos e também nos laboratórios opcionais para exercitar esse material ainda mais profundamente. Agora você sabe como fazer inferência e propulsão direta em uma rede neural, o que eu acho muito legal, então parabéns. Depois de passar pelos questionários e pelos laboratórios, volte também e, na próxima semana, veremos como realmente treinar uma rede neural. Estou ansioso para ver você na próxima semana.
