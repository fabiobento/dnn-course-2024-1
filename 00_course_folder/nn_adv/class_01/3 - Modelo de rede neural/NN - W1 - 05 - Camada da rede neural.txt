O alicerce fundamental da maioria das redes neurais modernas é uma camada de neurônios. Neste vídeo, você aprenderá a construir uma camada de neurônios e, depois de eliminá-la, poderá pegar esses blocos de construção e juntá-los para formar uma grande rede neural. Vamos dar uma olhada em como uma camada de neurônios funciona. Aqui está o exemplo que tivemos do exemplo de previsão de demanda, em que tínhamos quatro recursos de entrada que foram definidos para essa camada de três neurônios na camada oculta que, em seguida, envia sua saída para essa camada de saída com apenas um neurônio. Vamos ampliar a camada oculta para ver seus cálculos. Essa camada oculta insere quatro números e esses quatro números são entradas para cada um dos três neurônios. Cada um desses três neurônios está apenas implementando uma pequena unidade de regressão logística ou uma pequena função de regressão logística. Pegue esse primeiro neurônio. Ela tem dois parâmetros, w e b. Na verdade, para indicar que essa é a primeira unidade oculta, vou subscrevê-la como w_1, b_1. O que ele faz é gerar algum valor de ativação a, que é g de w_1 em um produto com x mais b_1, onde esse é o valor z familiar que você aprendeu na regressão logística no curso anterior, e g de z é a função logística familiar, 1 sobre 1 mais e elevado a menos z. Talvez isso acabe sendo um número 0,3 e esse seja o valor de ativação a do primeiro neurônio. Para indicar que este é o primeiro neurônio, eu também vou adicionar um subscrito a_1 aqui, então a_1 pode ser um número como 0,3. Há uma chance de 0,3 de que isso seja altamente acessível com base nos recursos de entrada. Agora vamos dar uma olhada no segundo neurônio. O segundo neurônio tem parâmetros w_2 e b_2, e esses w, b ou w_2, b_2 são os parâmetros da segunda unidade logística. Ele calcula a_2 igual à função logística g aplicada ao produto w_2 dot x mais b_2 e isso pode ser algum outro número, digamos 0,7. Porque neste exemplo, há uma chance de 0,7 de acharmos que os potenciais compradores conheçam essa camiseta. Da mesma forma, o terceiro neurônio tem um terceiro conjunto de parâmetros w_3, b_3. Da mesma forma, ele calcula um valor de ativação a_3 igual a g do produto w_3 dot x mais b_3 e isso pode ser, digamos, 0,2. Neste exemplo, esses três neurônios produzem 0,3, 0,7 e 0,2, e esse vetor de três números se torna o vetor dos valores de ativação a, que é então passado para a camada final de saída dessa rede neural. Agora, quando você constrói redes neurais com várias camadas, será útil dar números diferentes às camadas. Por convenção, essa camada é chamada de camada 1 da rede neural e essa camada é chamada de camada 2 da rede neural. Às vezes, a camada de entrada também é chamada de camada 0 e, hoje, existem redes neurais que podem ter dezenas ou até centenas de camadas. Mas para introduzir a notação para nos ajudar a distinguir entre as diferentes camadas, vou usar o colchete 1 sobrescrito para indexar em diferentes camadas.
Reproduza o vídeo começando em :4:17 e siga a transcrição4:17
Em particular, um sobrescrito entre colchetes 1, que vou usar, é uma notação para denotar a saída da camada 1 dessa camada oculta dessa rede neural e, da mesma forma, w_1, b_1 aqui estão os parâmetros da primeira unidade na camada 1 da rede neural, então também vou adicionar um sobrescrito entre colchetes 1 aqui, e w_2, b_2 são os parâmetros da segunda unidade oculta ou do segundo neurônio oculto na camada 1. Seus parâmetros também são indicados aqui w^ [1] assim. Da mesma forma, posso adicionar colchetes sobrescritos dessa forma para indicar que esses são os valores de ativação das unidades ocultas da camada 1 dessa rede neural. Eu sei que talvez essa notação esteja ficando um pouco confusa. Mas lembre-se de que sempre que você vê esse colchete 1 sobrescrito, isso se refere apenas a uma quantidade associada à camada 1 da rede neural. Se você ver o colchete sobrescrito 2, isso se refere a uma quantidade associada à camada 2 da rede neural e, da mesma forma, a outras camadas, incluindo a camada 3, a camada 4 e assim por diante, para redes neurais com mais camadas. Esse é o cálculo da camada 1 dessa rede neural. Sua saída é esse vetor de ativação, a^ [1] e vou copiá-lo aqui porque essa saída a_1 se torna a entrada para a camada 2. Agora vamos ampliar o cálculo da camada 2 dessa rede neural, que também é a camada de saída. A entrada para a camada 2 é a saída da camada 1, então a_1 é esse vetor 0,3, 0,7, 0,2 que acabamos de calcular na parte anterior deste slide.
Reproduza o vídeo começando em :6:35 e siga a transcrição6:35
Como a camada de saída tem apenas um único neurônio, tudo o que ela faz é calcular a_1, que é a saída desse primeiro e único neurônio, como g, a função sigmóide aplicada a w _1 em um produto com a^ [1], então essa é a entrada nessa camada e depois mais b_1. Aqui, essa é a quantidade z com a qual você está familiarizado e g, como antes, é a função sigmóide que você aplica a ela. Se isso resultar em um número, digamos 0,84, essa se torna a camada de saída da rede neural. Neste exemplo, como a camada de saída tem apenas um único neurônio, essa saída é apenas um escalar, é um número único em vez de um vetor de números.
Reproduza o vídeo começando em :7:31 e siga a transcrição7:31
Seguindo nossa convenção notacional de antes, usaremos um sobrescrito entre colchetes 2, para denotar as quantidades associadas à camada 2 dessa rede neural, então a^ [2] é a saída dessa camada e, portanto, também vou copiar isso aqui como a saída final da rede neural. Para tornar a notação consistente, você também pode adicionar esses sobrescritos (colchetes 2s) para indicar que esses são os parâmetros e valores de ativação associados à camada 2 da rede neural. Depois que a rede neural computa a_2, há uma etapa opcional final que você pode optar por implementar ou não, que é se você quiser uma previsão binária, 1 ou 0, essa é a mais vendida? Sim ou não? Como você pode pegar o número um sobrescrito, colchetes 2, subscrever 1, e esse é o número 0,84 que calculamos, e limitá-lo a 0,5. Se for maior que 0,5, você pode prever y que é igual a 1 e, se for menor que 0,5, preveja seu valor y igual a 0. Também vimos esse limite quando você aprendeu sobre regressão logística no primeiro curso da especialização. Se desejar, isso fornece a previsão final de um ou zero, se você não quiser apenas a probabilidade de ser um best-seller. Então é assim que uma rede neural funciona. Cada camada insere um vetor de números e aplica várias unidades de regressão logística a ele e, em seguida, computa outro vetor de números que é passado de camada em camada até chegar ao cálculo final das camadas de saída, que é a previsão da rede neural. Então, você pode definir o limite de 0,5 ou não chegar à previsão final. Com isso, vamos usar essa base que construímos agora para analisar alguns modelos de rede neural ainda mais complexos e ainda maiores. Espero que, ao ver mais exemplos, esse conceito de camadas e como juntá-las para construir uma rede neural se torne ainda mais claro. Então, vamos para o próximo vídeo.
(Obrigatória)
pt-BR
​

