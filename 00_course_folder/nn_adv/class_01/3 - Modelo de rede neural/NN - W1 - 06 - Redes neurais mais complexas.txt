No último vídeo, você aprendeu sobre a camada de rede neural e como ela insere um vetor de números e, por sua vez, gera outro vetor de números. Neste vídeo, vamos usar essa camada para criar uma rede neural mais complexa. Com isso, espero que a notação que estamos usando para redes neurais também se torne mais clara e concreta. Vamos dar uma olhada. Esse é o exemplo contínuo que vou usar ao longo deste vídeo como exemplo de uma rede neural mais complexa. Essa rede tem quatro camadas, sem contar a camada de entrada, que também é chamada de Camada 0, onde as camadas 1 , 2 e 3 são camadas ocultas, e a Camada 4 é a camada de saída, e a Camada 0, como de costume, é a camada de entrada. Por convenção, quando dizemos que uma rede neural tem quatro camadas, isso inclui todas as camadas ocultas na camada de saída, mas não contamos a camada de entrada. Esta é uma rede neural com quatro camadas na forma convencional de contar camadas na rede. Vamos ampliar a camada 3, que é a terceira e última camada oculta para ver os cálculos dessa camada. A camada 3 insere um vetor, um colchete sobrescrito 2 que foi calculado pela camada anterior, e gera a_3, que é outro vetor. Qual é o cálculo que a camada 3 faz para ir de a_2 para a_3?
Reproduza o vídeo começando em :1:45 e siga a transcrição1:45
Se ele tem três neurônios ou nós o chamamos de três unidades ocultas, então ele tem parâmetros w_1, b_1, w_2, b_2 e w_3, b_3 e calcula a_1 igual ao sigmóide de w_1. produto com essa entrada para a camada mais b_1, e calcula a_2 igual ao sigmóide de w_2. produto com novamente a_2, a entrada para a camada mais b_2 e assim por diante para obter a_3. Então, a saída dessa camada é um vetor composto por a_1, a_2 e a_3.
Reproduza o vídeo começando em :2:24 e siga a transcrição2:24
Novamente, por convenção, se quisermos denotar mais explicitamente que todas essas são quantidades associadas à Camada 3, adicionamos todos esses colchetes 3 sobrescritos aqui, para denotar que esses parâmetros w e b são os parâmetros associados aos neurônios na Camada 3 e que essas ativações são ativações com a Camada 3. Observe que esse termo aqui é w_1 colchete 3 sobrescrito, significando os parâmetros associados à Camada 3. produto com um colchete sobrescrito 2, que era a saída da camada 2, que se tornou a entrada para a camada 3. É por isso que tem a_3 aqui porque é um associador de parâmetros da Camada 3. produto com, e há a_2 lá porque é a saída da Camada 2. Agora, vamos fazer uma rápida verificação de nossa compreensão disso.
Reproduza o vídeo começando em :3:22 e siga a transcrição3:22
Vou esconder os sobrescritos e subscritos associados ao segundo neurônio e, sem retroceder neste vídeo, vá em frente e retroceda se quiser, mas prefiro que não. Mas sem rebobinar este vídeo, você consegue pensar em quais são os sobrescritos e inscritos que faltam nessa equação e preenchê-los você mesmo? Depois de dar uma olhada no questionário final do vídeo e ver se consegue descobrir quais são os sobrescritos e subscritos apropriados para essa equação aqui. Se você escolheu a 1ª opção, acertou! A ativação do segundo neurônio na camada 3 é indicada por 'a' três dois. Para aplicar a função de ativação, g, vamos usar os parâmetros desse mesmo neurônio. Portanto, w e b terão o mesmo subscrito 2 e o mesmo colchete sobrescrito 3. As feições de entrada serão o vetor de saída da camada anterior, que é a camada 2. Então esse será o vetor 'a' sobrescrito 2. A segunda opção é usar o vetor 'a' 3, que não é o vetor de saída da camada anterior. A entrada para essa camada é 'a' dois. E a 3ª opção tem dois dois como entrada, que é um único número em vez do vetor. Porque lembre-se de que a entrada correta é um vetor, dois, com a pequena seta no topo, e não um único número.
Reproduza o vídeo começando em :4:58 e siga a transcrição4:58
Recapitulando, a_3 é a ativação associada à camada 3 para o segundo neurônio, portanto, esse a_2 é um parâmetro associado à terceira camada. Para o segundo neurônio, isso é a_2, o mesmo que acima e depois mais b_3 também. Espero que isso faça sentido.
Reproduza o vídeo começando em :5:23 e siga a transcrição5:23
Apenas a forma mais geral dessa equação para uma camada 0 arbitrária e para uma unidade arbitrária j, que é que uma desativação sai da camada l, unidade j, como a32, essa será a função sigmóide aplicada a esse termo, que é o vetor de onda da camada l, como a camada 3 para a jª unidade, então há a_2 novamente, no exemplo acima, e isso é produzido por pontos com um valor de desativação.
Reproduza o vídeo começando em :5:57 e siga a transcrição5:57
Observe que isso não é l, isso é l menos 1, como a_2 acima, porque você está produzindo pontos com a saída da camada anterior e depois mais b, o parâmetro dessa camada para aquela unidade j. Isso dá a ativação da camada l unidade j, onde o sobrescrito entre colchetes l indica a camada l e um subscrito j denota a unidade j. Ao construir redes neurais, a unidade j se refere à jª neurônio, então usamos esses termos de forma um pouco intercambiável, onde cada unidade é um único neurônio na camada. G aqui está a função sigmóide. No contexto de uma rede neural, g tem outro nome , também chamado de função de ativação, porque g gera esse valor de ativação. Quando digo função de ativação, quero dizer essa função g aqui. Até agora, a única função de ativação que você viu, é uma função sigmóide, mas na próxima semana, veremos quando outras funções, a função sigmóide, também poderá ser conectada no lugar de g.. A função de ativação é exatamente aquela função que gera esses valores de ativação. Apenas uma última parte da notação.
Reproduza o vídeo começando em :7:20 e siga a transcrição7:20
Para tornar toda essa notação consistente, eu também vou dar o vetor de entrada X e outro nome que é a_0, então desta forma, a mesma equação também funciona para a primeira camada, onde quando l é igual a 1, as ativações da primeira camada, que é a_1, seriam o sigmóide multiplicado pelos pesos ponto-produto com a_0, que é apenas esse vetor de característica de entrada X. Com essa notação, você agora saiba como calcular os valores de ativação de qualquer camada em uma rede neural em função dos parâmetros, bem como do ativações da camada anterior. Agora você sabe como calcular as ativações de qualquer camada, dadas as ativações da camada anterior. Vamos colocar isso em um algoritmo de inferência para uma rede neural. Em outras palavras, como fazer com que uma rede neural faça previsões. Vamos ver isso no próximo vídeo.
