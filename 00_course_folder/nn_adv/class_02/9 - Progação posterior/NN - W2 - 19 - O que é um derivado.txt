Você viu como, no TensorFlow, você pode especificar uma arquitetura de rede neural para calcular a saída y em função da entrada x e também especificar uma função de custo, e o TensorFlow usará automaticamente a retropropagação para calcular derivadas e usará gradiente descendente ou Adam para treinar os parâmetros de uma rede neural. O algoritmo de retropropagação, que calcula os derivados de sua função de custo com relação aos parâmetros, é um algoritmo essencial no aprendizado de redes neurais. Mas como isso realmente funciona? Neste e nos próximos vídeos opcionais, tentaremos dar uma olhada em como a retropropagação calcula os derivados. Esses vídeos são totalmente opcionais e abordam um pouco o cálculo. Se você já está familiarizado com cálculo, espero que goste desses vídeos, mas se não, está tudo bem. Vamos nos basear no básico do cálculo para tentar garantir que você tenha toda a intuição necessária para entender como a retropropagação funciona. Vamos dar uma olhada. Vou usar uma função de custo simplificada, J de w é igual a w ao quadrado. A função de custo é uma função dos parâmetros w e, digamos, b e, para essa função de custo simplificada, vamos fingir que J de w é igual a w ao quadrado. Eu vou ignorar b neste exemplo. Digamos que o valor do parâmetro w seja igual a 3. J de w será igual a 9, w ao quadrado de 3 ao quadrado. Agora, se aumentássemos w em uma pequena quantidade, digamos Epsilon, que vou definir como 0,001. Como o valor de J de w muda? Se aumentarmos w em 0,001, w se torna 3 mais 0,001, então é 3,001. J de w, que é w ao quadrado, que definimos acima, agora é esse 3.001 quadrado, que é 9.006001. O que vemos é que se w subir 0,001, vou usar essa seta para cima aqui para indicar que w sobe em 0,001, onde 0,001 é esse pequeno valor Epsilon. Então J de w aumenta aproximadamente 6 vezes mais, 6 vezes 0,001. Isso não é exatamente. Na verdade, não sobe para 9.006, mas 9.006001. Mas acontece que se Epsilon era infinitesimalmente pequeno, e por infinitesimalmente pequeno, quero dizer muito pequeno. O Epsilon é bem pequeno, mas não é infinitesimalmente pequeno. Se o Epsilon fosse 0,00000, muitos zeros seguidos por um, isso se tornaria cada vez mais preciso. Neste exemplo, o que vemos é que se w sobe por Epsilon, então J sobe aproximadamente 6 vezes Epsilon. No cálculo, o que diríamos é que a derivada de J de w em relação a w é igual a 6. Tudo isso significa que se w sobe um pouquinho, J de w sobe seis vezes mais. E se o Epsilon assumisse um valor diferente? E se o Epsilon fosse 0,002. Nesse caso, w seria 3 mais 0,002 e w ao quadrado se tornaria 3,002 ao quadrado, o que é 9,012004.
Reproduza o vídeo começando em :4: e siga a transcrição4:00
Nesse caso, o que concluímos é que se w aumenta 0,002, então J de w sobe aproximadamente 6 vezes 0,002. Ele sobe aproximadamente para 9,012, e esse 0,012 é aproximadamente 6 vezes 0,002. Isso, novamente, está um pouco errado. Isso é 0,00004 extra aqui, porque o Epsilon não é infinitesimalmente pequeno. Mais uma vez, vemos essa proporção de seis para um entre o quanto w sobe e o quanto J de w sobe. É por isso que a derivada de J de w em relação a w é igual a seis. Um pequeno Epsilon é o mais preciso que isso se torna. A propósito, sinta-se à vontade para pausar o vídeo e experimentar esse cálculo agora mesmo com outros valores de Epsilon. A chave é que, desde que o Epsilon seja bem pequeno, a proporção pela qual J de w sobe versus a quantidade pela qual w sobe deve ser 6-1. Sinta-se à vontade para experimentar você mesmo com outros valores do Epsilon e verifique se isso realmente é verdade. Isso nos leva a uma definição informal da derivada, que é que sempre que w aumenta em uma pequena quantidade de Epsilon, isso faz com que J de w aumente k vezes Epsilon. No nosso exemplo, agora k era igual a seis. Então dizemos que a derivada de J de w em relação a w é igual a k, que era igual a 6 no exemplo de agora. Você deve se lembrar que, ao implementar o gradiente descendente, você usará repetidamente essa regra para atualizar o parâmetro w J, onde, como de costume, Alpha é a taxa de aprendizado. O que a descida de gradiente faz?
Reproduza o vídeo começando em :6:6 e siga a transcrição6:06
Observe que, se a derivada for pequena, isso resultará em uma pequena atualização no parâmetro W_j, enquanto que se esse termo derivado for grande, isso resultará em uma grande mudança no parâmetro W_j. Isso faz sentido porque isso significa essencialmente que, se a derivada for pequena, isso significa que alterar w não faz uma grande diferença no valor de j e, portanto, não vamos nos preocupar em fazer uma grande alteração em W_j J. Mas se a derivada for grande, isso significa que mesmo uma pequena alteração no W_j pode fazer uma grande diferença em quanto você pode alterar ou diminuir a função de custo j de w. Nesse caso, vamos fazer uma mudança maior em W_j, porque isso realmente fará uma grande diferença em quanto podemos reduzir a função de custo J. Vamos dar uma olhada em mais alguns exemplos de derivadas. O que você viu no exemplo agora foi que se w é igual a 3 e j de w é igual a w ao quadrado é igual a 9, então se w aumenta em Epsilon em 0,01, então j de w se torna j de 3,01 agora é 9,006001. Ou, em outras palavras, j aumentou cerca de 0,006, que é 6 vezes 0,001 ou 6 vezes Epsilon, e é por isso que a derivada de w em relação a W é igual a 6. Vamos ver qual será a derivada para outros valores de w, considere que w é igual a 2.
Reproduza o vídeo começando em :7:54 e siga a transcrição7:54
Nesse caso, j de w é, w ao quadrado agora é igual a 4, e se w aumenta em 0,001, então J de w se torna j de 2,001, que é igual a esses 4,004001 e então j de w subiu de quatro para esse valor aqui, que é aproximadamente quatro vezes Epsilon maior que quatro, e é por isso que agora a derivada é quatro. Porque w subindo por Epsilon fez com que j de w subisse quatro vezes mais. Novamente, há 0,001 extra porque não é muito preciso porque o Epsilon é infinitesimalmente pequeno. Ou vamos dar uma olhada em outro exemplo. E se w fosse igual a menos 3? J de w, que é w ao quadrado, ainda é igual a 9 porque menos 3 ao quadrado é 9. Se w fosse subir por Epsilon novamente, agora você teria w igual a menos 2,999, então isso é j de menos 2,999. O quadrado de menos 2,999 é igual a esse 8,994001, porque w é menos 3 mais 0,001. Observe aqui, j de w caiu cerca de 0,006, o que é seis vezes o Epsilon. O que temos neste exemplo é que j começa com 9, mas agora diminuiu. Observe esta seta para baixo aqui [inaudível] seta 6 vezes Epsilon ou equivalentemente, ela subiu menos 6 vezes Epsilon. É por isso que a derivada neste caso é igual a menos 6. Porque w subindo por Epsilon faz com que j de w aumente menos 6 vezes Epsilon quando Epsilon é pequeno. Outra forma de visualizar isso é traçar a função J de w, de forma que o eixo horizontal seja w e esse seja J de w, então quando w é igual a 3, J de w é igual a 9. Quando é menos 3, também é igual a 9, e quando é 2, J de w é igual a 4. Deixe-me fazer uma observação que pode ser relevante se você já fez uma aula de cálculo antes. Mas se você não tiver, o que eu digo nos próximos 60 segundos pode não fazer sentido, mas não se preocupe. Você precisará entendê-lo para acompanhar completamente o resto desses vídeos. Se você fez uma aula de cálculo em algum momento, pode reconhecer que as derivadas correspondem à inclinação de uma linha que apenas toca a função J de w neste ponto, digamos, onde w é igual a 3.
Reproduza o vídeo começando em :10:57 e siga a transcrição10:57
A inclinação dessa linha neste ponto, e a inclinação dessa altura sobre essa largura, acaba sendo igual a 6 quando w é igual a 3, a inclinação dessa linha é 4 quando w é igual a 2 e a inclinação dessa linha é menos 6 quando w é igual a menos 3. Acontece que, no cálculo, a inclinação dessas linhas corresponde à derivada da função. Mas se você nunca fez uma aula de cálculo antes e nunca viu esse conceito de inclinação antes, não se preocupe com isso. Agora, há uma última observação que eu quero fazer antes de prosseguir, que é que você vê em todos esses três exemplos, J de w é a mesma função, J de w é igual a w ao quadrado. Mas a derivada de J de w depende de w, quando w é três, a derivada é seis. Quando w é dois, a derivada é quatro. Quando w é menos 3, a derivada é menos 6. Acontece que, se você estiver familiarizado com o cálculo e, novamente, tudo bem se não estiver, o cálculo pode nos permitir calcular a derivada de J de w em relação a w como 2 vezes w. Em breve, mostrarei como você pode usar o Python para calcular essas derivadas usando um pacote Python bacana chamado SymPy. Mas como o cálculo nos diz que a derivada de w ao quadrado J de w é 2w, é por isso que a derivada quando w é três é 2 vezes 3 ou quando é dois é 2 vezes 2,
Reproduza o vídeo começando em :12:40 e siga a transcrição12:40
ou quando é menos 3 é 2 vezes menos 3, porque esse valor de w vezes 2 resulta na derivada. Vamos ver mais alguns exemplos antes de finalizarmos. Para esses exemplos, vou definir w igual a 2. Você viu no último slide, se J de w é w ao quadrado, então a derivada que eu disse seria 2 vezes w, que era 4. Se w aumenta 0,01, sendo Epsilon, J de w se torna isso, então aproximadamente J de w sobe 4 vezes Epsilon. Vamos dar uma olhada em algumas outras funções. E se J de w for igual a um cubo? Nesse caso, w ao cubo, 2 cubos seriam iguais a 8, ou se J de w fosse igual a w? Aqui, w será igual a 2. Ou se J de w fosse 1 sobre w? Nesse caso, 1 sobre w, 1 sobre 2 seria 1/2 ou 0,5. Qual é a derivada de J de w em relação a w quando a função de custo J de w é w ao cubo ou w ou 1 sobre w. Deixe-me mostrar como você mesmo pode calcular esses derivados usando uma biblioteca e um pacote chamados SymPy. Deixe-me primeiro importar o SymPy. O que vou fazer é dizer ao SymPy que vou usar J e w como símbolos para calcular derivados. Em nosso primeiro exemplo, tivemos que a função de custo J era igual a w ao quadrado. Observe como o SymPy realmente o renderiza nesta fonte bacana aqui também. Se usássemos SymPy para obter a derivada de J em relação a w, deveríamos fazer o seguinte. Você vê que o SymPy diz que esse derivado é 2w. Deixe-me realmente escolher uma variável, dJ, dw, definimos que seja igual a isso, basta digitá-la novamente aqui. Imprima isso. Há 2w. Se quiser inserir o valor de w nessa expressão para avaliá-la, você pode usar a derivada.subs w, 2. Isso significa inserir um valor de w igual a 2 nessa expressão e avaliá-la. Isso dá o valor de quatro, e é por isso que quando w é igual a 2, vimos que a derivada de J era igual a 4. Vamos ver alguns outros exemplos. E se eu fosse um cubo? Em seguida, a derivada se torna 3 vezes w ao quadrado. Resulta do cálculo, e é isso que o SymPy está calculando para nós, se J é w ao cubo, então a derivada de J em relação a w é 3w ao quadrado. Dependendo do que w é, o valor da derivada também muda. Podemos conectar se w for igual a 2, você obtém 12 neste caso. Ou se fosse J igual a w? Nesse caso, a derivada é igual a 1. Ou o exemplo final que temos foi e se J for igual a 1 sobre w? Nesse caso, a derivada acaba sendo menos 1 sobre w ao quadrado. Isso é menos 1 sobre 4. O que vou fazer é pegar os derivativos que elaboramos. Lembre-se de que para w ao quadrado, era 2w, para w cubo, era 3w ao quadrado. Pois w é apenas 1 e 1 sobre w é menos 1 sobre w ao quadrado. Vamos copiar isso de volta para o nosso outro slide.
Reproduza o vídeo começando em :16:48 e siga a transcrição16:48
O que o SymPy ou realmente o cálculo nos mostrou é que se J de w é um cubo, a derivada é 3w ao quadrado, o que é igual a 12 quando w é igual a 2, quando J de w é igual a w, a derivada é apenas igual a 1. Quando J de w é 1 sobre w é menos 1 sobre w ao quadrado, que é menos 1/4 quando w é igual a 2. Vamos começar. Verificaremos se essas expressões que obtivemos do SymPy estão corretas. Vamos tentar aumentar w por Epsilon, neste caso J de w. Sinta-se à vontade para pausar o vídeo e verificar essa matemática em sua própria calculadora, se quiser. Mas nesse caso, J de w a 0,001 cubo se torna isso. Então J subiu de 8 para 8.012 aproximadamente. Aumentou cerca de 12 vezes o Epsilon. Assim, a derivada é de fato 12. Ou se J de w for igual a w, então se w aumentar em Epsilon, então J de w, que é apenas w, agora é 2,001. Então, aumentou 0,01, que é exatamente o valor de Epsilon. Então J de w aumentou 1 vezes Epsilon. A derivada é de fato igual a 1. Observe que aqui, na verdade, é exatamente Epsilon, embora Epsilon seja infinitesimalmente pequeno. Em nosso último exemplo, se J de w é igual a 1/w, se w aumenta em Epsilon, então w é 1/2,001,
Reproduza o vídeo começando em :18:28 e siga a transcrição18:28
então acontece que J de w é aproximadamente 4,9975 com alguns dígitos extras truncados. Mas isso acaba sendo 0,5 menos 0,00025. J of w começou em 0,5 e caiu em 0,00025. Isso 0,00025, é 0,25 vezes Epsilon. Diminuiu nessa quantidade ou aumentou menos 0,25 vezes Epsilon porque menos 0,25 vezes Epsilon é igual a essa soma aqui. Vemos que se w aumenta em Epsilon, J de w aumenta em menos 1/4 ou menos 0,25 vezes Epsilon, e é por isso que a derivada neste caso é menos 1/4. Espero que com esses exemplos você tenha uma noção do que significa a derivada em relação a w de J de w. Só que se w sobe por Epsilon, quanto J de w sobe em alguma constante k vezes Epsilon. Essa constante k é a derivada. O valor de k dependerá tanto da função J de w quanto do valor de w. Antes de finalizarmos este vídeo, quero abordar brevemente a notação usada para escrever derivadas que você pode ver em outros textos.
Reproduza o vídeo começando em :20:15 e siga a transcrição20:15
Ou seja, se J de w é uma função de uma única variável, digamos w, então os matemáticos às vezes escrevem a derivada como d/dw de J de w. Observe aqui que essa notação está usando a letra minúscula d. Por outro lado, se J for uma função de mais de uma variável, os matemáticos às vezes usarão essa alternativa irregular d para denotar a derivada de J em relação a uma das os parâmetros w_i. Na minha opinião, essa notação que distingue entre essa letra regular d e esse símbolo derivado de cálculo estilizado d, faz pouco sentido para mim fazer essa distinção e essa notação, para minha mente, complica demais o cálculo e deriva essa notação. Mas, por razões históricas, o texto de cálculo usará essas duas notações diferentes, dependendo se J é uma função de uma única variável ou uma função de várias variáveis. Mas acho que, para fins práticos, essa convenção notacional tende a complicar demais as coisas, eu acho, de uma forma que eu não acho que seja realmente necessária. Para essa classe, vou usar essa notação em todos os lugares, mesmo quando há apenas uma única variável. Na verdade, para a maioria das nossas aplicações, a função J é uma função de mais de uma variável. Portanto, essa outra notação, que às vezes é chamada de notação derivada parcial, é na verdade a notação correta quase sempre, porque J geralmente tem mais de uma variável. Mas espero que o uso dessa notação ao longo dessas palestras simplifique a apresentação e torne os derivados um pouco mais fáceis de entender. Na verdade, essa notação é a que você viu nos vídeos anteriores. Para ser conciso, em vez de escrever essa expressão completa aqui, às vezes você também a vê encurtada como derivada ou derivada parcial de J em relação a w_i ou escrita assim. Essas são apenas formas simplificadas e abreviadas dessa expressão aqui. Espero que isso lhe dê uma ideia do que são derivados. É só que se w subir um pouco, em Epsilon, quanto J de w muda como consequência. A seguir, vamos dar uma olhada em como você pode calcular derivados em uma rede neural. Para fazer isso, precisamos dar uma olhada em algo chamado gráfico de computação. Vamos dar uma olhada nisso no próximo vídeo.
: adicionado à seleção. Pressione [CTRL + S] para salvar como anotação
(Obrigatória)
pt-BR
​

