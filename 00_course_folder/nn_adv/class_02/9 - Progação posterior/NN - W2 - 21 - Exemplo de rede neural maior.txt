Neste vídeo final sobre intuição para backprop, vamos dar uma olhada em como o rascunho computacional funciona em um exemplo maior de rede neural. Aqui está a rede que usaremos com uma única camada oculta, com uma única unidade oculta que gera a1, que alimenta a camada de saída que gera a previsão final a2. Para tornar a matemática mais fácil de lidar, continuarei usando apenas um único exemplo de treinamento com entradas x = 1, y = 5. E esses serão os parâmetros da rede. E durante todo o processo, usaremos as funções de ativação do ReLU de g (z) = max (0, z). Então, para prop em sua rede, fica assim. Como de costume, a1 é igual a g (w1 vezes x + b1). E então acontece que w1x + b será positivo. Então, estamos no máximo (0, z) = z, partes dessa função de ativação. Então isso é exatamente igual a isso, que é 2 vezes 1, que é w1 é 2 vezes x1 + 0, isso é b1, que é igual a 2. E da mesma forma, a2 é igual a isso, g (w2a1 + b2) que é w2 vezes a1 + b. Novamente, porque estamos na parte positiva da função de ativação do ReLu, que é 3 x 2 + 1 = 7. Por fim, usaremos a função de custo de erro quadrático. Então j (w, b) é 1/2 (a2- y) ao quadrado = 1/2 (7-5) ao quadrado, que é 1/2 de 2 ao quadrado, que é exatamente igual a 2. Então, vamos pegar esse cálculo que acabamos de fazer e anotá-lo na forma de um gráfico de computação. Para realizar a computação passo a passo, a primeira coisa que precisamos fazer é pegar w1 e multiplicar isso por x. Então, temos w1 que alimenta o nó de computação que calcula w1 vezes x. E vou chamar isso de variável temporária t1. Em seguida, calculamos z1, que é esse termo aqui, que é t1 + b1. Então, também temos essa entrada b1 aqui. E, finalmente, a1 é igual a g (z1). Aplicamos a função de ativação e, portanto, acabamos com esse valor novamente aqui, 2. Em seguida, temos que calcular t2, que é w2 vezes a1. E então, com w2, isso nos dá esse valor que é 6. Então z2, que é essa quantidade, tivemos que b2 e isso nos dá 7. E, finalmente, aplique a função de ativação, g. Ainda acabamos com 7. E por último, j é 1/2 (a2- y) ao quadrado. E isso nos dá 2. Que era essa função de custo aqui. Então, é assim que você pega os cálculos passo a passo para uma rede neural maior e os escreve no gráfico de computação. Você já viu, no último vídeo, a mecânica de como realizar o backprop. Não vou passar pelos cálculos passo a passo aqui. Mas se você fosse executar o backprop, a primeira coisa a fazer é perguntar: qual é a derivada da função de custo j em relação a a2? E acontece que se você calcular isso, acaba sendo 2. Então, vamos preencher isso aqui. E a próxima etapa será perguntada: qual é a derivada do custo j em relação a z2. E usando essa derivada que calculamos anteriormente, você pode descobrir que isso acaba sendo 2. Porque se z aumentar em épsilon, você pode mostrar que, para a configuração atual de todos os parâmetros, a2 aumentará por épsilon. E, portanto, j aumentará 2 vezes o épsilon. Então, essa derivada é igual a 2 e assim por diante. Passo a passo. Podemos então descobrir que a derivada de j respectivo b2 também é igual a 2. A derivada em relação a t2 é igual a 2, e assim por diante. Até que finalmente você tenha calculado a derivada de j em relação a todos os parâmetros w1, b1, w2 e b2. E isso é backprop. E, novamente, não passei pelas etapas mecânicas de cada etapa do backprop. Mas é basicamente o processo que você viu no vídeo anterior. Deixe-me verificar um desses exemplos. Então vimos aqui que a derivada de j em relação a w1 é igual a 6. Então, o que isso está prevendo é que, se w1 aumenta em épsilon, j deve subir cerca de 6 vezes épsilon. Vamos percorrer o mapa e ver se isso realmente é verdade. Esses são os cálculos que fizemos, novamente. Então, se w, que era 2, fosse 2,001, sobe em épsilon, então a1 se torna, vamos ver, em vez de 2, isso também é 2,001. Então a1 em vez de 2 agora é 2,001. Então 3 x 2.001 + 1, isso nos dá 7.003. E se a2 é 7,003, então se torna 7,003- 5 ao quadrado. E então isso se torna 2,003 ao quadrado sobre 2, o que acaba sendo igual a 2,006005. Então, ignorando alguns dos dígitos extras, você vê neste pequeno cálculo que, se w1 aumenta em 0,001, j de w subiu de 2 para 2,006 aproximadamente. Então, 6 vezes mais. E então a derivada de j em relação a w1 é de fato igual a 6. Portanto, o procedimento de backprop oferece uma maneira muito eficiente de calcular todos esses derivados. Que você pode então inserir no algoritmo de gradiente descendente ou no algoritmo de otimização Adam , para então treinar os parâmetros da sua rede neural. E, novamente, a razão pela qual usamos o background para isso é que é uma maneira muito eficiente de calcular todas as derivadas de j em relação a w1, j em relação a b1, j em relação a w2 e j em relação a b2. Acabei de ilustrar como poderíamos aumentar um pouco o w1 e ver o quanto j muda. Mas esse foi um cálculo da esquerda para a direita. E então tivemos que fazer esse procedimento para cada parâmetro, um parâmetro por vez. Se tivéssemos que aumentar w em 0,001 para ver como isso muda j. Aumente b1 um pouco para ver como isso muda j e aumente cada parâmetro, um de cada vez, um pouco para ver como isso muda j. Então isso se torna um cálculo muito ineficiente. E se você tivesse N nós em seu gráfico de computação e parâmetros P, esse procedimento acabaria tomando N vezes P etapas, o que é muito ineficiente. Já obtivemos todas essas quatro derivadas N + P, em vez de N vezes P etapas. E isso faz uma grande diferença nas redes neurais práticas, nas quais o número de nós e o número de parâmetros podem ser muito grandes. Então, esse é o final do vídeo desta semana. Obrigado por ficar comigo até o final desses vídeos opcionais. E espero que agora você tenha uma intuição de quando usar estruturas de programa, como o tensorflow, para treinar uma rede neural. O que realmente está acontecendo nos bastidores e como é usar o gráfico de computação para calcular derivadas de forma eficiente para você. Muitos anos atrás, antes do surgimento de estruturas como tensorflow e pytorch, os pesquisadores costumavam usar o cálculo manualmente para calcular os derivados das redes neurais que eles queriam treinar. Portanto, em estruturas de programas modernas, você pode especificar o forwardprop e fazer com que ele cuide do backprop para você. Muitos anos atrás, os pesquisadores costumavam escrever a rede neural manualmente e usar o cálculo manualmente para calcular os derivados. E então a Neural implementa um monte de equações que eles extraíram laboriosamente no papel, para implementar o backprop. Graças ao gráfico de computação e a essas técnicas para realizar automaticamente cálculos de derivadas. Às vezes é chamado de autodiff, para diferenciação automática. Esse processo de pesquisadores usando manualmente o cálculo para obter derivadas não está mais realmente concluído. Pelo menos, eu não tenho que fazer isso sozinho há muitos anos, por causa do autodiff. Então, há muitos anos, para usar redes neurais, a barreira da quantidade de cálculo que você precisa saber na verdade costumava ser maior. Mas, devido aos algoritmos de diferenciação automática, geralmente baseados no gráfico de computação, agora você pode implementar uma rede neural e fazer com que os derivados sejam computados para você com mais facilidade do que antes. Então, talvez com o amadurecimento das redes neurais, a quantidade de cálculo que você precisa saber para fazer esses algoritmos funcionarem tenha realmente diminuído. E isso tem sido encorajador para muitas pessoas. E isso é tudo para os vídeos desta semana. Espero que você goste dos laboratórios e espero vê-lo na próxima semana.
