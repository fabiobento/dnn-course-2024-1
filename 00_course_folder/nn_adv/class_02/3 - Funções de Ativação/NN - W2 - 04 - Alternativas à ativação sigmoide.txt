Até agora, estamos usando a função de ativação sigmóide em todos os nós nas camadas ocultas e na camada de saída. E começamos dessa forma porque estávamos construindo redes neurais tomando a regressão logística e criando muitas unidades de regressão logística e juntando-as. Mas se você usar outras funções de ativação, sua rede neural pode se tornar muito mais poderosa. Vamos dar uma olhada em como fazer isso. Lembre-se do exemplo de previsão de demanda da semana passada, em que, considerando o preço , o custo de envio, o marketing e o material, você tentaria prever se algo é altamente acessível. Se houver boa consciência e alta qualidade percebida e, com base nisso, tente prever que foi um dos mais vendidos. Mas isso pressupõe que a consciência talvez seja binária, se as pessoas estão cientes ou não. Mas parece que o grau em que os possíveis compradores estão cientes da camiseta que você está vendendo pode não ser binário, eles podem estar um pouco atentos, um pouco atentos, extremamente atentos ou ela pode ter se tornado completamente viral. Então, em vez de modelar a consciência como um número binário 0, 1, você tenta estimar a probabilidade de consciência ou, em vez de modelar a consciência, é apenas um número entre 0 e 1. Talvez a consciência deva ser qualquer número não negativo, porque pode haver qualquer valor não negativo de consciência indo de 0 até números muito grandes.
Reproduza o vídeo começando em :1:31 e siga a transcrição1:31
Então, enquanto anteriormente tínhamos usado essa equação para calcular a ativação daquela segunda unidade oculta, estimando a consciência, onde g era a função sigmóide e fica apenas entre 0 e 1. Se você quiser permitir que a,1, 2 potencialmente assuma valores positivos muito maiores, podemos trocar por uma função de ativação diferente. Acontece que uma escolha muito comum de função de ativação em redes neurais é essa função. Parece assim. Vai se z for isso, então g (z) é 0 à esquerda e então há essa linha reta 45° à direita de 0. Então, quando z é maior ou igual a 0, g (z) é exatamente igual a z. Isso está na metade direita deste diagrama. E a equação matemática para isso é g (z) igual a max (0, z). Sinta-se à vontade para verificar por si mesmo que max (0, z) resulta nessa curva que desenhei aqui. E se um 1, 2 é g (z) para esse valor de z, então a, o valor de desativação não pode assumir 0 ou qualquer valor não negativo.
Reproduza o vídeo começando em :3:2 e siga a transcrição3:02
Essa função de ativação tem um nome. É conhecido pelo nome ReLu com essa capitalização engraçada e ReLu significa novamente, um termo um tanto misterioso, mas significa unidade linear retificada. Não se preocupe muito com o que significa retificado e o que significa unidade linear. Esse foi exatamente o nome que os autores deram a essa função de ativação específica quando a criaram. Mas a maioria das pessoas em aprendizado profundo apenas diz ReLU para se referir a esse g (z). De forma mais geral, você pode escolher o que usar para g (z) e, às vezes, usaremos uma opção diferente da função de ativação sigmóide. Aqui estão as funções de ativação mais usadas. Você viu a função de ativação sigmóide, g (z) é igual a essa função sigmóide. No último slide, acabamos de ver o ReLu ou unidade linear retificada g (z) igual a max (0, z). Há uma outra função de ativação que vale a pena mencionar, que é chamada de função de ativação linear, que é apenas g (z) igual a z. Às vezes, se você usar a função de ativação linear, as pessoas dirão que não estamos usando nenhuma função de ativação porque se a é g (z) onde g (z) é igual a z, então a é apenas igual a esse w.x mais b z. E então é como se não houvesse g lá em absoluto. Então, quando você está usando essa função de ativação linear g (z), às vezes as pessoas dizem, bem, não estamos usando nenhuma função de ativação. Embora nesta classe, eu me refira ao uso da função de ativação linear em vez de nenhuma função de ativação. Mas se você ouvir outra pessoa usar essa terminologia, é isso que ela quer dizer. Refere-se apenas à função de ativação linear. E essas três são provavelmente, de longe, as funções de ativação mais comumente usadas em redes neurais. Ainda esta semana, abordaremos a quarta chamada função de ativação softmax. Mas com essas funções de ativação, você poderá construir uma grande variedade de redes neurais poderosas. Então, ao construir uma rede neural para cada neurônio, você quer usar a função de ativação sigmóide ou a função de ativação ReLU? Ou uma função de ativação linear? Como você escolhe entre essas diferentes funções de ativação? Vamos dar uma olhada nisso no próximo vídeo.
