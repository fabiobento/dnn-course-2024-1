Vamos dar uma olhada em por que as redes neurais precisam de funções de ativação e por que elas simplesmente não funcionam se usássemos a função de ativação linear em cada neurônio da rede neural. Lembre-se desse exemplo de previsão de demanda. O que aconteceria se usássemos uma função de ativação linear para todos os nós dessa rede neural? Acontece que essa grande rede neural não será diferente da simples regressão linear. Portanto, isso anularia todo o propósito de usar uma rede neural, porque ela simplesmente não seria capaz de se encaixar em nada mais complexo do que o modelo de regressão linear que aprendemos no primeiro curso. Vamos ilustrar isso com um exemplo mais simples.
Reproduza o vídeo começando em ::53 e siga a transcrição0:53
Vejamos o exemplo de uma rede neural em que a entrada x é apenas um número e temos uma unidade oculta com os parâmetros w1 e b1 que gera a1, que é aqui, apenas um número, e então a segunda camada é a camada de saída e também tem apenas uma unidade de saída com os parâmetros w2 e b2 e, em seguida, a saída a2, que também é apenas um número, apenas um escalar, que é a saída da rede neural f de x. Vamos ver o que essa rede neural faria se usássemos a função de ativação linear g de z é igual a z em todos os lugares . Então, para calcular a1 em função de x, a rede neural usará a1 igual a g de w1 vezes x mais b1. Mas g de z é igual a z. Então isso é apenas w1 vezes x mais b1. Então a2 é igual a w2 vezes a1 mais b2, porque g de z é igual a z. Deixe-me pegar essa expressão por a1 e substituí-la aí. Então isso se torna w2 vezes w1 x mais b1 mais b2. Se simplificarmos, isso se torna w2, w1 vezes x mais w2, b1 mais b2.
Reproduza o vídeo começando em :2:37 e siga a transcrição2:37
Acontece que se eu definisse w igual a w2 vezes w1 e definisse b igual a essa quantidade aqui, o que acabamos de mostrar é que a2 é igual a w x mais b. Então a2 é apenas uma função linear da entrada x. Em vez de usar uma rede neural com uma camada oculta e uma camada de saída, poderíamos muito bem ter usado um modelo de regressão linear. Se você está familiarizado com álgebra linear, esse resultado vem do fato de que uma função linear de uma função linear é em si uma função linear. É por isso que ter várias camadas em uma rede neural não permite que a rede neural compute recursos mais complexos ou aprenda algo mais complexo do que apenas uma função linear. Então, no caso geral, se você tivesse uma rede neural com várias camadas como essa e dissesse que deveria usar uma função de ativação linear para todas as camadas ocultas e também usar uma função de ativação linear para a camada de saída , acontece que esse modelo computará uma saída que é completamente equivalente à regressão linear. A saída a4 pode ser expressa como uma função linear dos recursos de entrada x mais b. Ou, alternativamente, se ainda usássemos uma função de ativação linear para todas as camadas ocultas, para essas três camadas ocultas aqui, mas usássemos uma função de ativação logística para a camada de saída, então você pode mostrar que esse modelo se torna equivalente à regressão logística, e a4, nesse caso, pode ser expresso como 1 sobre 1 mais e para menos wx mais b para alguns valores de w e b. Então essa grande rede neural não faça qualquer coisa que você também não possa fazer com a regressão logística. É por isso que uma regra comum é não usar a função de ativação linear nas camadas ocultas da rede neural. Na verdade, eu recomendo que normalmente o uso da função de ativação ReLU funcione perfeitamente. É por isso que uma rede neural precisa de funções de ativação diferentes da função de ativação linear em todos os lugares. Até agora, você aprendeu a construir redes neurais para problemas de classificação binária em que y é zero ou um. Bem como para problemas de regressão em que y pode assumir valores negativos ou positivos, ou talvez apenas valores positivos e não negativos. No próximo vídeo, gostaria de compartilhar com você uma generalização do que você viu até agora para classificação. Em particular, quando y não assume apenas dois valores, mas pode assumir três, quatro, dez ou até mais valores categóricos. Vamos dar uma olhada em como você pode criar uma rede neural para esse tipo de problema de classificação.
(Obrigatória)
pt-BR
​

