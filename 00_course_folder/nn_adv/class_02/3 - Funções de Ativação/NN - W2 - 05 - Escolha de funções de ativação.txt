Vamos dar uma olhada em como você pode escolher a função de ativação para diferentes neurônios em sua rede neural. Começaremos com algumas orientações sobre como escolhê-lo para a camada de saída. Acontece que, dependendo de qual seja o rótulo alvo ou o rótulo verdadeiro fundamental y, haverá uma escolha bastante natural para a função de ativação da camada de saída e, em seguida, examinaremos a escolha da função de ativação também para as camadas ocultas de sua rede neural. Vamos dar uma olhada. Você pode escolher diferentes funções de ativação para diferentes neurônios em sua rede neural e, ao considerar a função de ativação para a camada de saída, verifica-se que geralmente haverá uma escolha bastante natural, dependendo de qual é o alvo ou o rótulo verdadeiro y. Especificamente, se você estiver trabalhando em um problema de classificação em que y é zero ou um,
Reproduza o vídeo começando em :1: e siga a transcrição1:00
então um problema de classificação binária, a função de ativação sigmóide quase sempre será a escolha mais natural, porque então a rede neural aprende a prever a probabilidade de que y seja igual a um, assim como fizemos com a regressão logística. Minha recomendação é que, se você estiver trabalhando em um problema de classificação binária, use sigmóide na camada de saída. Como alternativa, se você estiver resolvendo um problema de regressão , pode escolher uma função de ativação diferente. Por exemplo, se você está tentando prever como o preço das ações de amanhã mudará em comparação com o preço atual das ações. Bem, ele pode subir ou descer, então, neste caso, y seria um número que pode ser positivo ou negativo. Nesse caso, eu recomendaria que você usasse a função de ativação linear. Por que isso? Bem, isso porque então as saídas de sua rede neural, f de x, que é igual a a^3 no exemplo acima, seriam g aplicadas a z^3 e com a função de ativação linear, g de z pode assumir valores positivos ou negativos. Portanto, y pode ser positivo ou negativo, use uma função de ativação linear. Finalmente, se y só pode assumir valores não negativos, como se você estivesse prevendo o preço de uma casa, que nunca pode ser negativo, então a escolha mais natural será a função de ativação ReLu porque, como você pode ver aqui, essa função de ativação só assume valores não negativos, sejam valores zero ou positivos. Ao escolher a função de ativação a ser usada para sua camada de saída, geralmente dependendo de qual é o rótulo y que você está tentando prever, haverá uma escolha bastante natural. Na verdade, a orientação neste slide é como eu quase sempre escolho minha função de ativação também para a camada de saída de uma rede neural. E quanto às camadas ocultas de uma rede neural? Acontece que a função de ativação do ReLU é, de longe, a escolha mais comum na forma como as redes neurais são treinadas por muitos profissionais atualmente. Embora tenhamos descrito inicialmente as redes neurais usando a função de ativação sigmóide e, de fato, no início da história do desenvolvimento das redes neurais, as pessoas usassem funções de ativação sigmóide em muitos lugares, o campo evoluiu para usar o ReLU com muito mais frequência e os sigmóides quase nunca. Bem, a única exceção é que você usa uma função de ativação sigmóide na camada de saída se tiver um problema de classificação binária. Então, por que isso? Bem, existem alguns motivos.
Reproduza o vídeo começando em :3:43 e siga a transcrição3:43
Primeiro, se você comparar as funções de ativação do ReLU e do sigmóide, o ReLU é um pouco mais rápido de calcular porque requer apenas uma computação máxima de 0, z, enquanto o sigmóide requer uma exponenciação e depois uma inversa e assim por diante, e por isso é um pouco menos eficiente. Mas a segunda razão que se mostra ainda mais importante é que a função ReLu fica plana apenas em uma parte do gráfico; aqui à esquerda é completamente plana, enquanto a função de ativação sigmóide fica plana em dois lugares. Ela fica plana à esquerda do gráfico e plana à direita do gráfico. Se você estiver usando gradiente descendente para treinar uma rede neural, quando você tem uma função que é gorda em muitos lugares, as descidas de gradiente seriam muito lentas. Eu sei que o gradiente descendente otimiza a função de custo J de W, B em vez de otimizar a função de ativação, mas a função de ativação é uma parte do que entra na computação, e isso resulta em mais lugares na função de custo J de W, B que também são planos e com um pequeno gradiente, o que retarda o aprendizado. Eu sei que essa foi apenas uma explicação intuitiva, mas os pesquisadores descobriram que usar a função de ativação ReLU também pode fazer com que sua rede neural aprenda um pouco mais rápido, e é por isso que, para a maioria dos profissionais, se você está tentando decidir quais funções de ativação usar com a camada oculta, a função de ativação do ReLU se tornou agora, de longe, a escolha mais comum. Na verdade, como estou construindo uma rede neural, é assim que também escolho funções de ativação para as camadas ocultas. Resumindo, aqui está o que eu recomendo em termos de como você escolhe as funções de ativação para sua rede neural. Para a camada de saída, use um sigmóide, se você tiver um problema de classificação binária; linear, se y for um número que pode assumir valores positivos ou negativos, ou use ReLu se y puder assumir somente valores positivos ou zero valores positivos ou valores não negativos. Então, para as camadas ocultas, eu recomendaria apenas usar o ReLU como uma função de ativação padrão e, no TensorFlow, é assim que você o implementaria.
Reproduza o vídeo começando em :6:13 e siga a transcrição6:13
Em vez de dizer que a ativação é igual a sigmóide, como fizemos anteriormente, para as camadas ocultas, essa é a primeira camada oculta, a segunda camada oculta como TensorFlow para usar a função de ativação ReLu e, em seguida, para a camada de saída neste exemplo, pedi que ela usasse a função de ativação sigmóide, mas se você quisesse usar a função de ativação linear, é essa, essa é a sintaxe para ela, ou se você quisesse usar a Função de ativação do ReLU que mostra a sintaxe dele. Com esse conjunto mais rico de funções de ativação, você estará bem posicionado para construir redes neurais muito mais poderosas do que apenas uma vez usando apenas a função de ativação sigmóide. A propósito, se você olhar a literatura de pesquisa, às vezes ouve falar de autores usando até mesmo outras funções de ativação, como a função de ativação tan h ou a função de ativação LeakyRelu ou a função de ativação swish. A cada poucos anos, os pesquisadores às vezes criam outra função de ativação interessante e, às vezes, funcionam um pouco melhor. Por exemplo, usei a função de ativação do LeakyRelu algumas vezes em meu trabalho e, às vezes, ela funciona um pouco melhor do que a função de ativação do ReLU que você aprendeu neste vídeo. Mas acho que, na maioria das vezes e para a grande maioria dos aplicativos, o que você aprendeu neste vídeo seria bom o suficiente. Obviamente, se você quiser saber mais sobre outras funções de ativação, fique à vontade para procurar na Internet, e há apenas alguns casos em que essas outras funções de ativação também podem ser ainda mais poderosas. Com isso, espero que você também goste de praticar essas ideias, essas funções de ativação nos laboratórios opcionais e nos laboratórios práticos. Mas isso levanta outra questão. Por que precisamos mesmo de funções de ativação? Por que não usamos a função de ativação linear ou não usamos nenhuma função de ativação em nenhum lugar? Acontece que isso não funciona de jeito nenhum. No próximo vídeo, vamos ver por que esse é o caso e por que as funções de ativação são tão importantes para fazer suas redes neurais funcionarem.
