Para construir uma rede neural que possa realizar a classificação multiclasse, vamos pegar o modelo de regressão Softmax e colocá-lo essencialmente na camada de saída de uma rede neural. Vamos dar uma olhada em como fazer isso. Anteriormente, quando fazíamos reconhecimento de dígitos manuscritos com apenas duas classes. Usamos uma nova Rede Neural com essa arquitetura. Se agora você quiser fazer uma classificação manuscrita de dígitos com 10 classes, todos os dígitos de zero a nove, então vamos mudar essa Rede Neural para ter 10 unidades de saída como essas. E essa nova camada de saída será uma camada de saída Softmax. Então, às vezes, diremos que essa Rede Neural tem uma saída Softmax ou que essa camada superior é uma camada Softmax. E a forma como a propagação progressiva funciona nesta Rede Neural é dada uma entrada X A1 que é calculada exatamente da mesma forma que antes. E então A2, as ativações para a segunda camada oculta também são computadas exatamente da mesma forma que antes. E agora temos que calcular as ativações para essa camada de saída, que é a3. É assim que funciona. Se você tiver 10 classes de saída, calcularemos Z1, Z2 a Z10 usando essas expressões. Então, isso é realmente muito semelhante ao que tínhamos anteriormente para a fórmula que você está usando para calcular Z. Z1 é W1.produto com a2, as ativações da camada anterior mais b1 e assim por diante para Z1 a Z10. Então a1 é igual a e para o Z1 dividido por e para o Z1 mais até e para o Z10. E essa é a nossa estimativa da chance de y ser igual a 1. E da mesma forma para a2 e da mesma forma até a10. Então, isso lhe dá suas estimativas da chance de y ser igual a um, dois e assim por diante até o décimo rótulo possível para y. E só para completar, se você quiser indicar que essas são as quantidades associadas à camada três, tecnicamente, devo adicionar essas superfaixas três. Isso torna a notação um pouco mais confusa. Mas isso torna explícito que esse é, por exemplo, o valor Z (3), 1 e esses são os parâmetros associados à primeira unidade da camada três dessa Rede Neural. E com isso, sua camada aberta Softmax agora fornece estimativas da chance de y ser qualquer um desses 10 rótulos de saída possíveis. Quero mencionar que a camada Softmax às vezes também é chamada de função de ativação Softmax. É um pouco incomum em um aspecto em comparação com as outras funções de ativação que vimos até agora, como sigma, radial e linear, ou seja, quando analisamos funções de ativação sigmóide ou de valor ou linear, a1 era uma função de Z1 e a2 era uma função de Z2 e somente Z2. Em outras palavras, para obter os valores de ativação, poderíamos aplicar a função de ativação g, seja sigmóide ou raramente, ou algo mais elementar, a Z1 e Z2 e assim por diante para obter a1 e a2 e a3 e a4. Mas com a função de ativação da Softmax, observe que a1 é uma função de Z1 e Z2 e Z3 até Z10. Portanto, cada um desses valores de ativação depende de todos os valores de Z. E essa é uma propriedade um pouco exclusiva da saída da Softmax ou da função de ativação da Softmax ou declarada de forma diferente se você quiser calcular a1 a a10, que é uma função de Z1 até Z 10 simultaneamente. E isso é diferente das outras funções de ativação que vimos até agora. Finalmente, vamos ver como você implementaria isso no tensorflow. Se você quiser implementar a rede neural que mostrei aqui neste slide, este é o código para fazer isso. Da mesma forma que antes, há três etapas para especificar e treinar o modelo. A primeira etapa é fazer com que o tensorflow junte três camadas sequencialmente. Primeira camada, são essas 25 unidades com função de ativação do trilho. Segunda camada, 15 unidades de função de ativação de rally. E então, na terceira camada, porque agora existem 10 unidades de saída, você deseja produzir de a1 a a10, então elas agora são 10 unidades de saída. E diremos ao tensorflow que use a função de ativação da Softmax. E a função de custo que você viu no último vídeo, tensorflow a chama de função SparseCategoricalCrossEntropy.
Reproduza o vídeo começando em :5:22 e siga a transcrição5:22
Então eu sei que esse nome é um pouco exagerado, enquanto que para regressão logística tínhamos a função BinaryCrossEntropy, aqui estamos usando a função SparseCategoricalCrossEntropy. E o que categórico esparso se refere é que você ainda está classificado por y em categorias. Portanto, é categórico. Isso assume valores de 1 a 10. E sparse se refere a que y só pode assumir um desses 10 valores. Portanto, cada imagem é 0 ou 1 ou 2 ou assim por diante até 9. Você não verá uma imagem que seja simultaneamente o número dois e o número sete, tão esparsa se refere a que cada dígito é apenas uma dessas categorias. Mas é por isso que a função de perda que você viu no último vídeo é chamada de intensiva, embora seja a função de perda de entropia cruzada categórica esparsa. E então o código para treinar o modelo é exatamente o mesmo de antes. E se você usar esse código, poderá treinar uma rede neural em um problema de classificação multiclasse. Apenas uma observação importante: se você usar esse código exatamente como escrevi aqui, ele funcionará, mas na verdade não use esse código porque acontece que, no tensorflow, há uma versão melhor do código que faz com que o tensorflow funcione melhor. Portanto, mesmo que o código mostrado neste slide funcione. Não use esse código da maneira que eu o escrevi aqui, porque em um vídeo posterior desta semana, você verá uma versão diferente que, na verdade, é a versão recomendada para implementá-lo, que funcionará melhor. Mas vamos dar uma olhada nisso em um vídeo posterior. Agora, você sabe como treinar uma rede neural com uma camada de saída softmax com uma ressalva. Há uma versão diferente do código que tornará o tensorflow capaz de calcular essas probabilidades com muito mais precisão. Vamos dar uma olhada nisso: no próximo vídeo, também devemos mostrar o código real que eu recomendo que você use se estiver treinando uma rede neural Softmax. Vamos para o próximo vídeo.
(Obrigatória)
pt-BR
​


