A implementação que você viu no último vídeo de uma rede neural com uma camada softmax funcionará bem. Mas há uma maneira ainda melhor de implementá-lo. Vamos dar uma olhada no que pode dar errado com essa implementação e também como melhorá-la. Deixe-me mostrar duas maneiras diferentes de calcular a mesma quantidade em um computador. Opção 1, podemos definir x igual a 2/10.000. Opção 2, podemos definir x igual a 1 mais 1/10.000 menos 1 menos 1/10.000, e você primeiro calcula isso, depois calcula isso e, em seguida, calcula isso e obtém a diferença. Se você simplificar essa expressão, isso acabará sendo igual a 2/10.000. Deixe-me ilustrar isso neste caderno. Primeiro, vamos definir x igual a 2/10.000 e imprimir o resultado com vários pontos decimais de precisão. Isso parece muito bom. Segundo, deixe-me definir x igual, vou insistir em calcular 1/1 mais 10.000 e depois subtrair desse 1 menos 1/10.000. Vamos imprimir isso. Parece um pouco estranho, como se houvesse algum erro de arredondamento.
Reproduza o vídeo começando em :1:22 e siga a transcrição1:22
Como o computador tem apenas uma quantidade finita de memória para armazenar cada número, chamada de número de ponto flutuante nesse caso, dependendo de como você decide calcular o valor 2/10.000, o resultado pode ter um erro de arredondamento numérico maior ou menor. Acontece que, embora a forma como calculamos a função de custo do softmax esteja correta, há uma maneira diferente de formulá-la que reduz esses erros de arredondamento numérico, levando a cálculos mais precisos no TensorFlow. Deixe-me primeiro explicar isso com um pouco mais de detalhes usando regressão logística. Em seguida, mostraremos como essas ideias se aplicam para melhorar nossa implementação do softmax. Primeiro, deixe-me ilustrar essas ideias usando regressão logística. Em seguida, mostraremos também como melhorar sua implementação do softmax.
Reproduza o vídeo começando em :2:20 e siga a transcrição2:20
Lembre-se de que, para regressão logística, se você quiser calcular a função de perda para um determinado exemplo, primeiro calcule essa ativação de saída a, que é g de z ou 1/1 mais e elevado a z negativo. Em seguida, você calculará a perda usando essa expressão aqui. Na verdade, essa é a aparência dos códigos para uma camada de saída logística com essa perda de entropia cruzada binária. Para regressão logística, isso funciona bem e, geralmente, os erros de arredondamento numérico não são tão ruins. Mas acontece que, se você permitir que o TensorFlow não precise calcular a como um termo intermediário. Mas, em vez disso, se você disser ao TensorFlow que perdeu essa expressão aqui embaixo. Tudo o que fiz foi pegar um e expandi-lo para essa expressão aqui embaixo. Então, o TensorFlow pode reorganizar os termos nessa expressão e criar uma maneira numericamente mais precisa de calcular essa função de perda. Já o procedimento original era como insistir em calcular como um valor intermediário, 1 mais 1/10.000 e outro valor intermediário, 1 menos 1/10.000, e então manipular esses dois para obter 2/10.000. Essa implementação parcial insistia em calcular explicitamente a como uma quantidade intermediária. Mas, em vez disso, ao especificar essa expressão na parte inferior diretamente como a função de perda, ela dá ao TensorFlow mais flexibilidade em termos de como computá-la e se deseja ou não computá-la explicitamente. O código que você pode usar para fazer isso é mostrado aqui e o que isso faz é definir a camada de saída para usar apenas uma função de ativação linear e colocar a função de ativação, 1/1 mais em z negativo, bem como essa perda cruzada de entropia na especificação da função de perda aqui. É isso que esse argumento de logits equals true faz com que o TensorFlow faça.
Reproduza o vídeo começando em :4:44 e siga a transcrição4:44
Caso você esteja se perguntando quais são os logits, é basicamente esse número z. O TensorFlow calculará z como um valor intermediário, mas pode reorganizar os termos para que isso seja calculado com mais precisão. Uma desvantagem desse código é que ele se torna um pouco menos legível. Mas isso faz com que o TensorFlow tenha um pouco menos de erro de arredondamento numérico. Agora, no caso da regressão logística, qualquer uma dessas implementações realmente funciona bem, mas os erros de arredondamento numérico podem piorar quando se trata do softmax. Agora, vamos pegar essa ideia e aplicar à regressão softmax. Lembre-se do que você viu no último vídeo ao calcular as ativações da seguinte forma.
Reproduza o vídeo começando em :5:29 e siga a transcrição5:29
As ativações são g de z_1, até z_10 onde a_1, por exemplo, é e para z_1 dividido pela soma de e com z_j's, e então a perda foi essa dependendo de qual é o valor real de y é log negativo de aj para um dos aj's e então esse foi o código que tivemos para fazer esse cálculo em duas etapas separadas. Mas, mais uma vez, se você especificar que a perda é se y for igual a 1, será um log negativo dessa fórmula e assim por diante. Se y for igual a 10 nessa fórmula, isso dará ao TensorFlow a capacidade de reorganizar os termos e calcular essa forma integral numericamente precisa. Só para dar uma ideia de por que o TensorFlow pode querer fazer isso, acontece que se um dos z é muito pequeno do que e para um número menor negativo se torna muito, muito pequeno ou se um dos z é um número muito grande, então e para z pode se tornar um número muito grande e, ao reorganizar os termos, o TensorFlow pode evitar alguns desses números muito pequenos ou muito grandes e, portanto, criar mais cálculos de atriz para a função de perda.
Reproduza o vídeo começando em :6:50 e siga a transcrição6:50
O código para fazer isso é mostrado aqui na camada de saída, agora estamos apenas usando uma função de ativação linear, então a camada de saída apenas calcula z_1 a z_10 e todo esse cálculo da perda é então capturado na função de perda aqui, onde novamente temos o parâmetro from_logists equals true. Mais uma vez, esses dois trechos de código fazem praticamente a mesma coisa, exceto que a versão recomendada é mais numericamente precisa, embora, infelizmente, também seja um pouco mais difícil de ler. Se você está lendo o código de outra pessoa e vê isso e está se perguntando, o que está acontecendo é realmente equivalente à implementação original, pelo menos em conceito, exceto que é mais numericamente preciso. Os erros de arredondamento numérico da regressão_logist não são tão ruins, mas é recomendável que você use essa implementação até o final e, conceitualmente, esse código faz a mesma coisa que a primeira versão que você tinha anteriormente, exceto que é um pouco mais numericamente precisa. Embora a desvantagem também seja um pouco mais difícil de interpretar. Agora, há apenas mais um detalhe, que é que agora mudamos a rede neural para usar uma função de ativação linear em vez de uma função de ativação softmax. A camada final da rede neural não gera mais essas probabilidades de A_1 a A_10. É em vez de colocar z_1 até z_10.
Reproduza o vídeo começando em :8:30 e siga a transcrição8:30
Eu não falei sobre isso no caso da regressão logística, mas se você estivesse combinando a função logística da saída com a função de perda, então, para regressões logísticas, você também teria que alterar o código dessa forma para pegar o valor da saída e mapeá-lo através da função logística para realmente obter a probabilidade. Agora você sabe como fazer a classificação multiclasse com uma camada de saída softmax e também como fazer isso de uma forma numericamente estável. Antes de encerrar a classificação multiclasse, quero compartilhar com vocês um outro tipo de problema de classificação chamado problema de classificação com vários rótulos. Vamos falar sobre isso no próximo vídeo.
