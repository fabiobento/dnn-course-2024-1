{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3889ed6",
   "metadata": {},
   "source": [
    "[adaptado de [Programa de cursos integrados Aprendizado de máquina](https://www.coursera.org/specializations/machine-learning-introduction) de [Andrew Ng](https://www.coursera.org/instructor/andrewng)  ([Stanford University](http://online.stanford.edu/), [DeepLearning.AI](https://www.deeplearning.ai/) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar arquivos adicionais para o laboratório\n",
    "!wget https://github.com/fabiobento/dnn-course-2024-1/raw/main/00_course_folder/nn_adv/class_02/Laborat%C3%B3rios/lab_utils_ml_adv_week_2.zip\n",
    "      \n",
    "!unzip -n -q lab_utils_ml_adv_week_2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19897f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar se estamos no Google Colab\n",
    "# Necessário para ativar widgets\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ativação ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from autils import plt_act_trio\n",
    "from lab_utils_relu import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357a5c1",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Introdução a ativação ReLU\n",
    "Nessa aula, foi apresentada uma nova ativação, a Unidade Linear Retificada (_Rectified Linear Unit_ - ReLU). \n",
    "$$ a = max(0,z) \\quad\\quad\\text{\\# função ReLU} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e57fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_act_trio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"./images/C2_W2_ReLu.png\"     style=\" width:380px; padding: 10px 20px; \" >\n",
    "O exemplo da aula à direita mostra uma aplicação do ReLU. Nesse exemplo, o recurso derivado de \"_awareness_\" não é binário, mas tem uma faixa contínua de valores. O sigmoide é melhor para situações de ligado/desligado ou binárias. O ReLU fornece uma relação linear contínua. Além disso, ele tem um intervalo \"desligado\" em que a saída é zero.     \n",
    "O recurso \"off\" torna o ReLU uma ativação não linear. Por que isso é necessário? Vamos examinar isso a seguir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que ativações não lineares?\n",
    "<img align=\"left\" src=\"./images/C2_W2_ReLU_Graph.png\"     style=\" width:250px; padding: 10px 20px; \" > \n",
    "\n",
    "A função mostrada é composta de partes lineares (_piecewise linear_). A inclinação é consistente durante a parte linear e, em seguida, muda abruptamente nos pontos de transição. Nos pontos de transição, é adicionada uma nova função linear que, quando adicionada à função existente, produzirá a nova inclinação. A nova função é adicionada no ponto de transição, mas não contribui para o resultado antes desse ponto. A função de ativação não linear é responsável por desativar a entrada antes e, às vezes, depois dos pontos de transição. O exercício a seguir fornece um exemplo mais tangível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exercício usará a rede abaixo em um problema de regressão em que você deve modelar um alvo linear por partes:\n",
    "<img align=\"center\" src=\"./images/C2_W2_ReLU_Network.png\"     style=\" width:650px; padding: 10px 20px; \">  \n",
    "A rede tem 3 unidades na primeira camada. Cada uma delas é necessária para formar o alvo. A unidade 0 é pré-programada e fixa para mapear o primeiro segmento. Você modificará os pesos e as polarizações nas unidades 1 e 2 para modelar o segundo e o terceiro segmentos. A unidade de saída também é fixa e simplesmente soma as saídas da primeira camada.  \n",
    "\n",
    "Usando os controles deslizantes abaixo, modifique os pesos e a polarização para que correspondam ao objetivo. \n",
    "\n",
    ">Dicas: Comece com `w1` e `b1` e deixe `w2` e `b2` zerados até que você corresponda ao segundo segmento. Clicar em vez de deslizar é mais rápido.  Se tiver problemas, não se preocupe, o texto abaixo descreverá isso com mais detalhes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt_relu_ex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "O objetivo deste exercício é avaliar como o comportamento não linear da ReLU proporciona a capacidade necessária de desativar funções até que elas sejam necessárias. Vamos ver como isso funcionou neste exemplo.\n",
    "<img align=\"right\" src=\"./images/C2_W2_ReLU_Plot.png\"     style=\" width:600px; padding: 10px 20px; \"> \n",
    "Os gráficos à direita contêm a saída das unidades na primeira camada.   \n",
    "Começando no topo, a unidade 0 é responsável pelo primeiro segmento marcado com um 1. São mostradas a função linear $z$ e a função que segue a ReLU $a$. Você pode ver que a ReLU corta a função após o intervalo [0,1]. Isso é importante, pois evita que a Unidade 0 interfira no segmento seguinte. \n",
    "\n",
    "A unidade 1 é responsável pelo segundo segmento. Aqui, o ReLU manteve essa unidade em \"silêncio\" até que x seja 1. Como a primeira unidade não está contribuindo, a inclinação da unidade 1, $w^{[1]}_1$, é apenas a inclinação da linha de destino. A polarização deve ser ajustada para manter a saída negativa até que x atinja 1. Observe como a contribuição da unidade 1 também se estende ao terceiro segmento.\n",
    "\n",
    "A Unidade 2 é responsável pelo terceiro segmento. A ReLU zera novamente a saída até que x atinja o valor correto. A inclinação da unidade, $w^{[1]}_2$, deve ser definida de modo que a soma das unidades 1 e 2 tenha a inclinação desejada. A polarização é novamente ajustada para manter a saída negativa até que x atinja 2. \n",
    "\n",
    "O recurso \"off\" ou desativado da ativação do ReLU permite que os modelos unam segmentos lineares para modelar funções não lineares complexas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parabéns!\n",
    "Agora você está mais familiarizado com a ReLU e a importância de seu comportamento não linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
