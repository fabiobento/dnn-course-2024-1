{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[adaptado de [Programa de cursos integrados Aprendizado de máquina](https://www.coursera.org/specializations/machine-learning-introduction) de [Andrew Ng](https://www.coursera.org/instructor/andrewng)  ([Stanford University](http://online.stanford.edu/), [DeepLearning.AI](https://www.deeplearning.ai/) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar arquivos adicionais para o laboratório\n",
    "!wget https://github.com/fabiobento/dnn-course-2024-1/raw/main/00_course_folder/nn_adv/class_02/Laborat%C3%B3rios/lab_utils_ml_adv_week_2.zip\n",
    "      \n",
    "!unzip -n -q lab_utils_ml_adv_week_2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar se estamos no Google Colab\n",
    "# Necessário para ativar widgets\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Função Softmax\n",
    "Neste laboratório, exploraremos a função softmax. Essa função é usada na regressão softmax e em redes neurais para resolver problemas de classificação multiclasse. \n",
    "\n",
    "<center>  <img  src=\"./images/C2_W2_Softmax_Header.PNG\" width=\"600\" />  <center/>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from lab_utils_softmax import plt_softmax\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Normalmente, neste curso, os notebooks usam a convenção de iniciar as contagens com 0 e terminar com N-1, $\\sum_{i=0}^{N-1}$, enquanto as aulas teóricas começam com 1 e terminam com N, $\\sum_{i=1}^{N}$. Isso se deve ao fato de que o código normalmente inicia a iteração com 0, enquanto na aula, contar de 1 a N resulta em equações mais limpas e sucintas. Este notebook tem mais equações do que o normal em um laboratório e, portanto, romperá com a convenção e contará de 1 a N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introdução a Função Softmax\n",
    "Tanto na regressão softmax quanto nas redes neurais com saídas Softmax, são geradas N saídas e uma saída é selecionada como a categoria prevista. Em ambos os casos, um vetor $\\mathbf{z}$ é gerado por uma função linear que é aplicada a uma função softmax. A função softmax converte $\\mathbf{z}$ em uma distribuição de probabilidade, conforme descrito abaixo. Depois de aplicar a softmax, cada saída estará entre 0 e 1 e as soma das saídas será igual a1, de modo que possam ser interpretadas como probabilidades. As entradas maiores corresponderão a saída com maiores probabilidades.\n",
    "\n",
    "\n",
    "\n",
    "<center>  <img  src=\"./images/C2_W2_SoftmaxReg_NN.png\" width=\"600\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função softmax pode ser escrita como:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
    "\n",
    "O resultado $\\mathbf{a}$ é um vetor de comprimento N, portanto, para a regressão softmax, você também poderia escrever:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que mostra que a saída é um vetor de probabilidades. A primeira entrada é a probabilidade de a entrada ser a primeira categoria dada a entrada $\\mathbf{x}$ e os parâmetros $\\mathbf{w}$ e $\\mathbf{b}$.  \n",
    "Vamos criar uma implementação do NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):\n",
    "    ez = np.exp(z)   #exponencial elemento-a-elemento\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, varie os valores das entradas `z` usando os controles deslizantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt_softmax(my_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À medida que você está variando os valores dos z's acima, há alguns aspectos a serem observados:\n",
    "* o exponencial no numerador do softmax amplia pequenas diferenças nos valores \n",
    "* Os valores de saída somam um\n",
    "* o softmax abrange todos as saídas outputs. Uma alteração em `z0`, por exemplo, alterará os valores de `a0`-`a3`. Compare isso com outras ativações, como ReLU ou Sigmoid, que têm uma única entrada e uma única saída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Custo\n",
    "<center> <img  src=\"./images/C2_W2_SoftMaxCost.png\" width=\"400\" />    <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de perda associada ao Softmax, a perda de entropia cruzada, é:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Onde y é a categoria de alve para este exemplo e $\\mathbf{a}$ é a saída de uma função softmax. Em particular, os valores em $\\mathbf{a}$ são probabilidades que somam um.\n",
    ">**Lembre-se: neste curso, a perda é para um exemplo, enquanto o custo abrange todos os exemplos. \n",
    " \n",
    " \n",
    "Observe que em (3) acima, somente a linha que corresponde ao alvo contribui para a perda, as outras linhas são zero. Para escrever a equação de custo, precisamos de uma \"função indicadora\" que será 1 quando o índice corresponder ao alvo e zero caso contrário. \n",
    "\n",
    "  $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "  1, & \\text{if $y==n$}.\\\\\n",
    "  0, & \\text{caso contrário}.\n",
    "  \\end{cases}$$\n",
    "  \n",
    "Agora o custo é definido como:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Onde $m$ é o número de exemplos, $N$ é o número de saídas. Essa é a média de todas as perdas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "Este laboratório discutirá duas maneiras de implementar a perda de entropia cruzada softmax no Tensorflow: o método \"óbvio\" e o método \"recomendado\". O primeiro é o mais simples, enquanto o segundo é mais estável numericamente.\n",
    "\n",
    "Vamos começar criando um conjunto de dados para treinar um modelo de classificação multiclasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criar um conjunto de dados de exemplo\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A organização *Óbvia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo abaixo é implementado com o softmax como ativação na camada `Dense` final.\n",
    "A função de perda é especificada separadamente na diretiva `compile`. \n",
    "\n",
    "A função de perda é `SparseCategoricalCrossentropy`. Essa perda é descrita em (3) acima. Nesse modelo, o softmax ocorre na última camada. A função de perda recebe a saída do softmax, que é um vetor de probabilidades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'softmax')    # < ativação softmax aqui\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o softmax é integrado à camada de saída, a saída é um vetor de probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nonpreferred = model.predict(X_train)\n",
    "print(p_nonpreferred [:2])\n",
    "print(\"maior valor\", np.max(p_nonpreferred), \"menor valor\", np.min(p_nonpreferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Recomendado <img align=\"Right\" src=\"./images/C2_W2_softmax_accurate.png\"  style=\" width:400px; padding: 10px 20px ; \">\n",
    "\n",
    "Relembrando a aula, resultados mais estáveis e precisos podem ser obtidos se o softmax e a perda forem combinados durante o treinamento.   Isso é possibilitado pela organização \"recomendada\" mostrada aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na organização recomendada, a camada final tem uma ativação linear. Por razões históricas, os outputs nesse formato são chamados de *logits*. A função de perda tem um argumento adicional: `from_logits = True`. Isso informa à função de perda que a operação softmax deve ser incluída no cálculo da perda. Isso permite uma implementação otimizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "preferred_model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'linear')   #<-- observe\n",
    "    ]\n",
    ")\n",
    "preferred_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),   #<-- observe\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "preferred_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manuseio de resultados\n",
    "Observe que, no modelo recomendado, os resultados não são probabilidades, mas podem variar de grandes números negativos a grandes números positivos.A saída deve ser enviada por meio de um softmax ao realizar uma previsão que espera uma probabilidade. \n",
    "Vamos dar uma olhada nos resultados do modelo preferencial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_preferred = preferred_model.predict(X_train)\n",
    "print(f\"dois exemplos de vetores de saída:\\n {p_preferred[:2]}\")\n",
    "print(\"maior valor\", np.max(p_preferred), \"menor valor\", np.min(p_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previsões de saída não são probabilidades!\n",
    "Se a saída desejada for uma probabilidade, ela deverá ser processada por um [softmax] (https://www.tensorflow.org/api_docs/python/tf/nn/softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
    "print(f\"dois exemplos de vetores de saída:\\n {sm_preferred[:2]}\")\n",
    "print(\"maior valor\", np.max(sm_preferred), \"menor valor\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para selecionar a categoria mais provável, o softmax não é necessário. É possível encontrar o índice da maior saída usando [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseCategorialCrossentropy ou CategoricalCrossEntropy\n",
    "O Tensorflow tem dois formatos possíveis para valores de alvo e a seleção da perda define o que é esperado.\n",
    "- `SparseCategorialCrossentropy`: espera que o alvo seja um número inteiro correspondente ao índice. Por exemplo, se houver 10 valores-alvo em potencial, y estaria entre 0 e 9. \n",
    "- `CategoricalCrossEntropy`: Espera que o valor-alvo de um exemplo seja codificado _one-hot encoded_, em que o valor no índice-alvo é 1, enquanto as outras N-1 entradas são zero. Um exemplo com 10 valores-alvo potenciais, em que o alvo é 2, seria [0,0,1,0,0,0,0,0,0,0,0,0,0].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parabéns!\n",
    "Neste laboratório, você \n",
    "- Familiarizou-se com a função softmax e seu uso na regressão softmax e nas ativações softmax em redes neurais. \n",
    "- Aprendeu a construção do modelo recomendado no Tensorflow:\n",
    "    - Nenhuma ativação na camada final (o mesmo que ativação linear)\n",
    "    - Função de perda SparseCategoricalCrossentropy\n",
    "    - use from_logits=True\n",
    "- Reconheceu que, diferentemente do ReLU e do Sigmoid, o softmax abrange várias saídas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
