Para um aplicativo em que você não tem muitos dados, o aprendizado por transferência é uma técnica maravilhosa que permite usar dados de uma tarefa diferente para ajudar no seu aplicativo. Essa é uma daquelas técnicas que eu uso com muita frequência. Vamos dar uma olhada em como o aprendizado por transferência funciona. Veja como o aprendizado por transferência funciona. Digamos que você queira reconhecer os dígitos manuscritos de zero a nove, mas não tenha muitos dados rotulados desses dígitos manuscritos. Veja o que você pode fazer. Digamos que você encontre um conjunto de dados muito grande de um milhão de imagens de gatos, cachorros, carros, pessoas e assim por diante, mil classes. Você pode então começar treinando uma rede neural nesse grande conjunto de dados de um milhão de imagens com mil classes diferentes e treinar o algoritmo para tomar como entrada uma imagem X e aprender a reconhecer qualquer uma dessas 1.000 classes diferentes. Nesse processo, você acaba aprendendo parâmetros para a primeira camada da rede neural W^1, b^1, para a segunda camada W^2, b^2 e assim por diante, W^3, b^3, W^4, b^4 e W^5, b^5 para a camada de saída.
Reproduza o vídeo começando em :1:17 e siga a transcrição1:17
Para aplicar o aprendizado por transferência, o que você faz é então fazer uma cópia dessa rede neural onde você registraria os parâmetros W^1, b^1, W^2, b^2, W^3, b^3 e W^4, b^4. Mas para a última camada, você eliminaria a camada de saída e a substituiria por uma camada de saída muito menor, com apenas 10 em vez de 1.000 unidades de saída. Essas 10 unidades de saída corresponderão às classes de zero , um a nove que você deseja que sua rede neural reconheça.
Reproduza o vídeo começando em :1:53 e siga a transcrição1:53
Observe que os parâmetros W^5, b^5 não podem ser copiados porque a dimensão dessa camada mudou, então você precisa criar novos parâmetros W^5, b^5 que você precisa treinar do zero em vez de simplesmente copiá-los da rede neural anterior. No aprendizado por transferência, o que você pode fazer é usar os parâmetros das primeiras quatro camadas, na verdade, todas as camadas, exceto a camada de saída final, como ponto de partida para os parâmetros e, em seguida, executar um algoritmo de otimização, como gradiente descendente ou o algoritmo de otimização Adam, com os parâmetros inicializados usando os valores dessa rede neural no topo. Em detalhes, há duas opções de como você pode treinar esses parâmetros de redes neurais. A opção 1 é treinar apenas os parâmetros das camadas de saída.
Reproduza o vídeo começando em :2:50 e siga a transcrição2:50
Você tomaria os parâmetros W^1, b^1, W^2, b^2 a W^4, b^4 como valores de cima para baixo e os manteria fixos sem se preocupar em alterá-los, e usaria um algoritmo como a descida do gradiente estocástico ou o algoritmo de otimização Adam para atualizar apenas W^5, b^5 para reduzir a função de custo usual que você usa para aprender a reconhecer esses dígitos a zero nove de um pequeno conjunto de treinamento desses dígitos de zero a nove, então essa é a Opção 1. A opção 2 seria treinar todos os parâmetros na rede, incluindo W^1, b^1, W^2, b^2 até W^5, b^5, mas os primeiros parâmetros das quatro camadas seriam inicializados usando os valores que você treinou na parte superior. Se você tiver um conjunto de treinamento muito pequeno, a Opção 1 pode funcionar um pouco melhor, mas se você tiver um conjunto de treinamento um pouco maior, a Opção 2 pode funcionar um pouco melhor. Esse algoritmo é chamado de aprendizado por transferência porque a intuição consiste em aprender a reconhecer gatos, cachorros, vacas, pessoas e assim por diante. Espero que tenha aprendido alguns conjuntos plausíveis de parâmetros para as camadas anteriores de processamento de entradas de imagem. Então, ao transferir esses parâmetros para a nova rede neural, a nova rede neural começa com os parâmetros em um lugar muito melhor, para que tenhamos um pouco mais de aprendizado. Espero que possa acabar em um modelo muito bom. Essas duas etapas de primeiro treinamento em um grande conjunto de dados e, em seguida, de ajuste adicional dos parâmetros em um conjunto de dados menor, são chamadas de pré-treinamento supervisionado para esta etapa superior. É quando você treina a rede neural em um conjunto de dados muito grande de, digamos, um milhão de imagens de uma tarefa não relacionada. Em seguida, a segunda etapa é chamada de ajuste fino, em que você pega os parâmetros inicializados ou obtidos do pré-treinamento supervisionado e, em seguida, executa a descida do gradiente para ajustar os pesos de acordo com a aplicação específica de reconhecimento de dígitos manuscritos que você possa ter. Se você tiver um conjunto de dados pequeno, até mesmo dezenas, centenas ou milhares ou apenas dezenas de milhares de imagens dos dígitos manuscritos, aprender com esses milhões de imagens de uma tarefa não totalmente relacionada pode realmente ajudar muito no desempenho do seu algoritmo de aprendizado. Uma coisa boa sobre o aprendizado por transferência também é que talvez você não precise realizar o pré-treinamento supervisionado. Para muitas redes neurais, já haverá pesquisadores que já treinaram uma rede neural em uma imagem grande e publicaram uma rede neural treinada na Internet, licenciada gratuitamente para qualquer pessoa baixar e usar. Isso significa que, em vez de realizar a primeira etapa sozinho, você pode simplesmente baixar a rede neural que outra pessoa pode ter passado semanas treinando e, em seguida, substituir a camada de saída por sua própria camada de saída e executar a Opção 1 ou a Opção 2 para ajustar uma rede neural
Reproduza o vídeo começando em :5:53 e siga a transcrição5:53
na qual outra pessoa já realizou um pré-treinamento supervisionado, e apenas fazer um pequeno ajuste fino para obter rapidamente uma rede neural que funcione bem em sua tarefa. Baixar um modelo pré-treinado que outra pessoa treinou e forneceu gratuitamente é uma dessas técnicas em que, aproveitando o trabalho de cada um na comunidade de aprendizado de máquina, todos podemos obter resultados muito melhores. Pela generosidade de outros pesquisadores que pré-treinaram e publicaram suas redes neurais on-line. Mas por que o aprendizado por transferência ainda funciona? Como você pode pegar parâmetros obtidos reconhecendo gatos, cães, carros e pessoas e usá-los para ajudá-lo a reconhecer algo tão diferente quanto dígitos manuscritos? Aqui está uma intuição por trás disso. Se você estiver treinando uma rede neural para detectar, digamos, objetos diferentes de imagens , a primeira camada de uma rede neural pode aprender a detectar bordas na imagem. Pensamos nisso como características de baixo nível na imagem, que servem para detectar bordas. Cada um desses quadrados é uma visualização do que um único neurônio aprendeu a detectar ao aprender a agrupar pixels para encontrar bordas em uma imagem. A próxima camada da rede neural então aprende a agrupar as bordas para detectar cantos. Cada uma delas é uma visualização do que um neurônio pode ter aprendido a detectar, deve aprender com formas técnicas e simples, como formas semelhantes a cantos como essa. A próxima camada da rede neural pode ter aprendido a detectar que algumas são mais complexas, elas armazenam formas genéricas, como curvas básicas, ou formas menores como essas. É por isso que, ao aprender a detectar muitas imagens diferentes, você está ensinando a rede neural a detectar bordas , cantos e formas básicas. É por isso que, ao treinar uma rede neural para detectar coisas tão diversas como gatos, cachorros, carros e pessoas, você a ajuda a aprender a detectar essas características bastante genéricas das imagens e a encontrar bordas, cantos, curvas e formas básicas. Isso é útil para muitas outras tarefas de visão computacional, como reconhecer dígitos manuscritos. Porém, uma restrição do pré-treinamento é que o tipo de imagem x deve ser o mesmo para as etapas de pré-treinamento e ajuste fino. Se a tarefa final que você deseja resolver for uma tarefa de visão computacional, a etapa de pré-treinamento também foi uma rede neural treinada no mesmo tipo de entrada, ou seja, uma imagem das dimensões desejadas. Por outro lado, se seu objetivo é criar um sistema de reconhecimento de fala para processar áudio , uma rede neural pré-treinada em imagens provavelmente não funcionará muito bem em áudio. Em vez disso, você quer uma rede neural pré-treinada em dados de áudio, onde você pode ajustar seu próprio conjunto de dados de áudio e o mesmo para outros tipos de aplicativos. Você pode pré-treinar uma rede neural em dados de texto e, se seu aplicativo tiver um recurso de salvamento inserindo x de dados de texto, você poderá ajustar essa rede neural com seus próprios dados. Resumindo, essas são as duas etapas para o aprendizado por transferência. A etapa 1 é baixar a rede neural com parâmetros que foram pré-treinados em um grande conjunto de dados com o mesmo tipo de entrada do seu aplicativo. Esse tipo de entrada pode ser imagens, áudio, textos ou qualquer outra coisa, ou se você não quiser baixar a rede neural, talvez você possa treinar a sua própria. Mas na prática, se você estiver usando imagens, por exemplo, é muito mais comum baixar a rede neural pré-treinada de outra pessoa. Em seguida, treine ou ajuste ainda mais a rede com seus próprios dados. Descobri que, se você pode obter uma rede neural pré-treinada em um grande conjunto de dados, digamos , um milhão de imagens, às vezes você pode usar um conjunto de dados muito menor, talvez mil imagens, talvez até menores, para ajustar a rede neural em seus próprios dados e obter resultados muito bons. Às vezes, eu treinava redes neurais em apenas 50 imagens que funcionavam muito bem usando essa técnica, quando ela já havia sido pré-treinada em um conjunto de dados muito maior. Essa técnica não é uma panaceia. , você não pode fazer com que todos os aplicativos funcionem com apenas 50 imagens, mas isso ajuda muito quando o conjunto de dados que você tem para seu aplicativo não é tão grande.
Reproduza o vídeo começando em :10:16 e siga a transcrição10:16
A propósito, se você já ouviu falar de técnicas avançadas nas notícias, como GPT-3 ou BERTs ou redes neurais pré-treinadas no ImageNet, esses são, na verdade, exemplos de redes neurais pré-treinadas por outra pessoa em conjuntos de dados de imagem ou de texto muito grandes, elas podem então ser ajustadas em outros aplicativos. Se você ainda não ouviu falar de GPT-3, BERTs ou ImageNet, não se preocupe, se você já ouviu falar. Essas foram aplicações bem-sucedidas de pré-treinamento na literatura de aprendizado de máquina. Uma das coisas que eu gosto no aprendizado por transferência é apenas uma das maneiras pelas quais a comunidade de aprendizado de máquina compartilhou ideias , códigos e até parâmetros entre si, graças aos pesquisadores que pré-treinaram grandes redes neurais e publicaram os parâmetros na Internet gratuitamente para que qualquer outra pessoa pudesse baixar e usar. Isso permite que qualquer pessoa use modelos, pré-treinados, para ajustar um conjunto de dados potencialmente muito menor. No aprendizado de máquina, todos nós geralmente acabamos aproveitando o trabalho uns dos outros e esse compartilhamento aberto de ideias, códigos e parâmetros treinados é uma das maneiras pelas quais a comunidade de aprendizado de máquina, todos nós, coletivamente, conseguimos fazer um trabalho muito melhor do que qualquer pessoa sozinha. Espero que você, se juntando à comunidade de aprendizado de máquina, algum dia, encontre uma maneira de contribuir também com essa comunidade. Isso é tudo para o pré-treinamento. Espero que você ache essa técnica útil. No próximo vídeo, gostaria de compartilhar com vocês algumas ideias sobre o ciclo completo de um projeto de aprendizado de máquina. Ao construir um sistema de aprendizado de máquina, sejam todas as etapas nas quais vale a pena pensar. Vamos dar uma olhada nisso no próximo vídeo.
(Obrigatória)
pt-BR
​

