No último vídeo, você viu como usar o conjunto de testes para avaliar o desempenho de um modelo. Vamos refinar ainda mais essa ideia neste vídeo, que permite que você use a técnica para escolher automaticamente um bom modelo para seu algoritmo de aprendizado de máquina. Uma coisa que vimos é que, uma vez que os parâmetros w e b do modelo foram ajustados ao conjunto de treinamento. O erro de treinamento pode não ser um bom indicador de quão bem o algoritmo funcionará ou de quão bem ele será generalizado para novos exemplos que não estavam no conjunto de treinamento e, em particular, neste exemplo, o erro de treinamento será praticamente zero. Isso provavelmente é muito menor do que o erro de generalização real e, com isso, quero dizer o erro médio em novos exemplos que não estavam no conjunto de treinamento. O que você viu no último vídeo é que eu testei o desempenho do algoritmo em exemplos, sem treinamento, isso será um indicador melhor de quão bem o modelo provavelmente se sairá com novos dados. Com isso, quero dizer outros dados que não estão no conjunto de treinamento. Vamos dar uma olhada em como isso afeta, como podemos usar um conjunto de testes para escolher um modelo para um determinado aplicativo de aprendizado de máquina. Se for um ajuste de uma função para prever os preços da habitação ou algum outro problema de regressão, um modelo que você pode considerar é ajustar um modelo linear como esse.
Reproduza o vídeo começando em :1:28 e siga a transcrição1:28
Este é um polinômio de primeira ordem e vamos usar d igual a 1 neste slide para indicar o ajuste de um polinômio de primeira ordem ou de primeira ordem. Se você ajustasse um modelo como esse ao seu conjunto de treinamento, obteria alguns parâmetros, w e b, e poderá então calcular os testes J para estimar o quão bem isso se generaliza para novos dados?
Reproduza o vídeo começando em :1:51 e siga a transcrição1:51
Neste slide, vou usar w^1, b^1 para indicar que esses são os parâmetros que você obtém se ajustar um polinômio de primeira ordem, um grau um, d é igual a 1 polinômio.
Reproduza o vídeo começando em :2:5 e siga a transcrição2:05
Agora, você também pode considerar ajustar um modelo polinomial ou quadrático de segunda ordem, então esse é o modelo.
Reproduza o vídeo começando em :2:14 e siga a transcrição2:14
Se você ajustasse isso ao seu conjunto de treinamento, obteria alguns parâmetros, w ^ 2, b ^ 2, e você poderia então avaliar de forma semelhante esses parâmetros em seu conjunto de teste e obter o teste J w ^ 2, b ^ 2, e isso lhe dará uma ideia de quão bem o polinômio de segunda ordem se sai. Você pode tentar que d é igual a 3, que é um polinômio de terceira ordem ou grau três que se parece com este, ajustar os parâmetros e, da mesma forma, obter o teste J. Você pode continuar fazendo isso até, digamos, tentar até um polinômio de 10ª ordem e terminar com o teste J de w ^ 10, b ^ 10. Isso dá uma ideia do desempenho do polinômio de 10ª ordem. Um procedimento que você pode tentar, isso acaba não sendo o melhor procedimento, mas uma coisa que você pode tentar é dar uma olhada em todos esses testes J e ver qual deles fornece o valor mais baixo. Digamos que você descubra que o teste J para o polinômio de quinta ordem para w ^ 5, b ^ 5 acaba sendo o mais baixo. Se for esse o caso, você pode decidir que o polinômio de quinta ordem d igual a 5 funciona melhor e escolher esse modelo para sua aplicação. Se você quiser estimar o desempenho desse modelo, uma coisa que você pode fazer, mas isso acaba sendo um procedimento um pouco falho, é relatar o erro do conjunto de testes, teste J w ^ 5, b ^ 5. A razão pela qual esse procedimento é falho é que o teste J de w ^ 5, b ^ 5 provavelmente é uma estimativa otimista do erro de generalização. Em outras palavras, é provável que seja menor do que o erro de generalização real, e o motivo é que, no procedimento sobre o qual falamos neste slide com ajustes básicos, um parâmetro extra, que é d, o grau de polinômio, e escolhemos esse parâmetro usando o conjunto de teste. No slide anterior, vimos que, se você ajustasse w, b aos dados de treinamento , os dados de treinamento seriam uma estimativa excessivamente otimista do erro de generalização. Acontece também que, se você quiser escolher o parâmetro d usando o conjunto de teste , o teste J do conjunto de teste agora é excessivamente otimista, que é menor do que a estimativa real do erro de generalização. O procedimento neste slide em particular é falho e eu não recomendo usá-lo. Em vez disso, se você quiser escolher automaticamente um modelo, como decidir qual polinômio de grau usar. Veja como você modifica o procedimento de treinamento e teste para realizar a seleção do modelo. Por isso, seleção de modelos, quero dizer escolher entre diferentes modelos, como esses 10 modelos diferentes que você pode considerar usar em seu aplicativo de aprendizado de máquina. A forma como modificaremos o procedimento é que, em vez de dividir seus dados em apenas dois subconjuntos, o conjunto de treinamento e o conjunto de teste, dividiremos seus dados em três subconjuntos diferentes, que chamaremos de conjunto de treinamento, conjunto de validação cruzada e, em seguida, também conjunto de teste. Usando nosso exemplo anterior a esses 10 exemplos de treinamento, podemos dividi-lo colocando 60% dos dados no conjunto de treinamento e, portanto, a notação que usaremos para a parte do conjunto de treinamento será a mesma de antes, exceto que agora o trem M, o número de exemplos de treinamento será seis e poderemos colocar 20% dos dados no conjunto de validação cruzada e uma notação que vou usar é x_cv de um, y_cv de um para o primeiro exemplo de validação cruzada. Então cv significa validação cruzada, até x_cv de m_cv e y_cv de m_cv. Onde aqui, m_cv é igual a 2 neste exemplo, é o número de exemplos de validação cruzada. Então, finalmente, temos o conjunto de testes igual ao anterior, então testes de x1 a x m e y1 a y m, onde m testes são iguais a 2. Esse é o número de exemplos de testes. No próximo slide, veremos como usar o conjunto de validação cruzada. A forma como modificaremos o procedimento é que você já viu o conjunto de treinamento e o conjunto de testes e vamos introduzir um novo subconjunto dos dados chamado conjunto de validação cruzada. O nome validação cruzada se refere ao fato de que esse é um conjunto de dados extra que usaremos para verificar ou verificar com confiança a validade ou realmente a precisão de diferentes modelos. Não acho que seja um bom nome, mas é assim que as pessoas do aprendizado de máquina chamam esse conjunto de dados extra. Você também pode ouvir as pessoas chamarem isso de conjunto de validação, abreviadamente, são apenas menos sílabas do que validação cruzada ou, em alguns aplicativos, as pessoas também chamam isso de conjunto de desenvolvimento. Significa basicamente a mesma coisa ou para abreviar. Às vezes, você ouve as pessoas chamarem isso de conjunto de desenvolvimento, mas todos esses termos significam a mesma coisa que conjunto de validação cruzada. Pessoalmente, uso o termo dev set com mais frequência porque é a maneira mais curta e rápida de dizer isso, mas a validação cruzada é muito usada com um pouco mais de frequência pelos profissionais de aprendizado de máquina. Nesses três subconjuntos do conjunto de treinamento de dados, conjunto de validação cruzada e conjunto de teste, você pode então calcular o erro de treinamento, o erro de validação cruzada e o erro de teste usando essas três fórmulas.
Reproduza o vídeo começando em :8:4 e siga a transcrição8:04
Normalmente, nenhum desses termos inclui o termo de regularização incluído no objetivo do treinamento, e esse novo termo no meio, o erro de validação cruzada é apenas a média sobre seus exemplos de validação cruzada m_cv da média, digamos, erro quadrático. Esse termo, além de ser chamado de erro de validação cruzada, também é comumente chamado de erro de validação, abreviadamente, ou mesmo erro do conjunto de desenvolvimento ou erro do desenvolvedor. Munido dessas três medidas de desempenho do algoritmo de aprendizado, é assim que você pode realizar a seleção do modelo. Você pode, com os 10 modelos, iguais aos anteriores neste slide, com d igual a 1, d igual a 2, até um polinômio de 10º grau ou 10ª ordem, você pode então ajustar os parâmetros w_1, b_1.
Reproduza o vídeo começando em :8:59 e siga a transcrição8:59
Mas, em vez de avaliar isso em seu conjunto de testes, você avaliará esses parâmetros em seus conjuntos de validação cruzada e calculará J_CV de w1, b1 e, da mesma forma, para o segundo modelo, obteremos J_cv de w2, v2 e até J_cv de w10, b10.
Reproduza o vídeo começando em :9:22 e siga a transcrição9:22
Então, para escolher um modelo, você verá qual modelo tem o menor erro de validação cruzada e, concretamente, digamos que J_cv de w4, b4 é tão baixo quanto, então o que isso significa é que você escolhe esse polinômio de quarta ordem como o modelo que você usará para esta aplicação. Finalmente, se você quiser relatar uma estimativa do erro de generalização de quão bem esse modelo se sairá com novos dados. Você fará isso usando esse terceiro subconjunto de seus dados, o conjunto de testes, e reportará o Jtest de w4, b4. Você percebe que, durante todo esse procedimento, você ajustou esses parâmetros usando o conjunto de treinamento. Você então escolheu o parâmetro d ou escolheu o grau de polinômio usando o conjunto de validação cruzada e, até este ponto, você não ajustou nenhum parâmetro, seja w ou b ou d, ao conjunto de teste e é por isso que Jtest neste exemplo será uma estimativa justa do erro de generalização deste modelo, portanto, parâmetros w4, b4. Isso fornece um procedimento melhor para a seleção do modelo e permite que você tome automaticamente uma decisão, como qual ordem polinomial escolher para seu modelo de regressão linear. Esse procedimento de seleção de modelos também funciona para escolher entre outros tipos de modelos. Por exemplo, escolher uma arquitetura de rede neural. Se você está ajustando um modelo para reconhecimento de dígitos manuscrito, você pode considerar três modelos como este, talvez até um conjunto maior de modelos do que eu, mas aqui estão algumas redes neurais diferentes de pequenas, um pouco maiores e ainda maiores. Para ajudá-lo a decidir quantas camadas a rede neural tem e quantas unidades ocultas por camada você deve ter, você pode treinar todos esses três modelos e obter os parâmetros w1, b1 para o primeiro modelo, w2, b2 para o segundo modelo e w3, b3 para o terceiro modelo.
Reproduza o vídeo começando em :11:35 e siga a transcrição11:35
Você pode então avaliar o desempenho das redes neurais usando Jcv, usando seu conjunto de validação cruzada. Como esse é um problema de classificação, Jcv, a escolha mais comum seria computar isso como a fração dos exemplos de validação cruzada que o algoritmo classificou incorretamente. Você calcularia isso usando todos os três modelos e, em seguida, escolheria o modelo com o menor erro de validação cruzada. Se neste exemplo, este tiver o menor erro de validação cruzada, você escolherá a segunda rede neural e usará parâmetros treinados nesse modelo e, finalmente, se quiser relatar uma estimativa do erro de generalização, use o conjunto de teste para estimar o desempenho da rede neural que você acabou de escolher.
Reproduza o vídeo começando em :12:30 e siga a transcrição12:30
É considerada a melhor prática em aprendizado de máquina que, se você precisar tomar decisões sobre seu modelo, como ajustar parâmetros ou escolher a arquitetura do modelo, como arquitetura de rede neural ou grau de polinômio, se estiver ajustando uma regressão linear, tome todas essas decisões usando apenas seu conjunto de treinamento e seu conjunto de validação cruzada e não examine o conjunto de testes enquanto ainda estiver tomando decisões sobre seu algoritmo de aprendizado. É somente depois de criar um modelo como modelo final para avaliá-lo no conjunto de teste e porque você não tomou nenhuma decisão usando o conjunto de teste, que garante que seu conjunto de teste seja uma estimativa justa e não excessivamente otimista de quão bem seu modelo se generalizará para novos dados. Isso é seleção de modelos e, na verdade, é um procedimento muito usado. Eu uso isso o tempo todo para escolher automaticamente qual modelo usar para um determinado aplicativo de aprendizado de máquina. No início desta semana, mencionei a execução de diagnósticos para decidir como melhorar o desempenho de um algoritmo de aprendizado. Agora que você tem uma maneira de avaliar algoritmos de aprendizado e até mesmo escolher automaticamente um modelo, vamos nos aprofundar em exemplos de alguns diagnósticos. O diagnóstico mais poderoso que conheço e que usei para muitos aplicativos de aprendizado de máquina é chamado de viés e variância. Vamos dar uma olhada no que isso significa no próximo vídeo.
