Foi observado que um alto viés ou uma alta variância são ruins no sentido de que prejudicam o desempenho do seu algoritmo. Uma das razões pelas quais as redes neurais têm sido tão bem-sucedidas é porque suas redes, juntamente com a ideia de big data ou, esperançosamente, de grandes conjuntos de dados. Isso nos deu uma nova maneira de lidar com o alto viés e a alta variância. Vamos dar uma olhada. Você viu que se você está ajustando um polinômio de ordem diferente a um conjunto de dados, então se você ajustasse um modelo linear como este à esquerda. Você tem um modelo bem simples que pode ter um alto viés, enquanto você se ajusta a um modelo complexo, então você pode sofrer de alta variância. E há essa troca entre viés e variância, e em nosso exemplo, foi escolher um polinômio de segunda ordem que ajuda você a fazer uma troca e escolher um modelo com o menor erro de validação cruzada possível. Portanto, antes dos dias das redes neurais, os engenheiros de aprendizado de máquina falavam muito sobre essa compensação de viés e variância, na qual você precisa equilibrar a complexidade que é o grau de polinômio. Ou o parâmetro de regularização é mais longo para fazer com que compradores e variantes não sejam muito altos. E se você ouvir engenheiros de aprendizado de máquina falarem sobre a compensação de viés e variância. É a isso que eles se referem: se você tem um modelo muito simples, você tem um viés alto, um modelo muito complexo, uma alta variância. E você precisa encontrar uma compensação entre essas duas coisas ruins para encontrar provavelmente o melhor resultado possível. Mas acontece que suas redes nos oferecem todo esse dilema de ter que compensar preconceitos e variações com algumas ressalvas. E acontece que grandes redes neurais, quando treinadas em conjuntos de dados de tamanho moderado de pequeno prazo, são máquinas de baixo viés. E o que quero dizer com isso é que, se você tornar sua rede neural grande o suficiente, quase sempre poderá ajustar bem seu conjunto de treinamento. Desde que seu conjunto de treinamento não seja enorme. E o que isso significa é que isso nos dá uma nova receita para tentar reduzir o viés ou reduzir a variância conforme necessário, sem precisar realmente trocar entre os dois. Então, deixe-me compartilhar com vocês uma receita simples que nem sempre é aplicável. Mas se ele fornece, pode ser muito poderoso para obter um modelo preciso usando uma rede neural, que primeiro treina seu algoritmo em seu conjunto de treinamento e depois pergunta se ele funciona bem no conjunto de treinamento. Então, meça o Jtrain e veja se ele está alto e alto, quero dizer, por exemplo, em relação ao desempenho de nível humano ou a algum nível básico de desempenho e, se não estiver indo bem, você tem um problema de alto viés, erro de trens altos. E uma maneira de reduzir o viés é usar apenas uma rede neural maior e, por uma rede neural maior, quero dizer mais camadas ocultas ou mais unidades ocultas por camada. E você pode então continuar percorrendo esse ciclo e tornar sua rede neural cada vez maior até que ela se saia bem no conjunto de treinamento. O que significa que atinge o nível de erro em seu conjunto de treinamento que é aproximadamente comparável ao nível de erro alvo que você espera atingir, que pode ser o desempenho em nível humano. Depois de cair no conjunto de treinamento, a resposta para essa pergunta é sim. Você então pergunta se não funciona bem no conjunto de validação trans. Em outras palavras, ele tem alta variância e se a resposta for não, então você pode concluir que o algoritmo tem alta variância porque ele não quer treinar, o conjunto não funciona no conjunto de validação cruzada. Portanto, essa grande lacuna entre Jcv e Jtrain indica que você provavelmente tem um problema de alta variância e, se você tiver um problema de alta variância , uma maneira de tentar corrigi-lo é obter mais dados. Para obter mais dados, voltar e treinar novamente o modelo e verificar novamente, você quer apenas o conjunto de treinamento? Caso contrário, tenha uma rede maior ou ela verá se funciona quando a base cruzada é estabelecida e, se não, obtém mais dados. E se você puder continuar dando voltas e voltas nesse ciclo até que, eventualmente, ele se saia bem no conjunto de validação cruzada. Então você provavelmente terminou, porque agora você tem um modelo que funciona bem no conjunto de validação cruzada e, com sorte, também generalizará para novos exemplos. Agora, é claro que existem limitações na aplicação dessa receita. Treinar uma rede neural maior não reduz o viés, mas em algum momento ela se torna computacionalmente cara. É por isso que o surgimento das redes neurais foi realmente auxiliado pelo surgimento de computadores muito rápidos, incluindo especialmente GPUs ou unidades de processamento gráfico. O hardware é tradicionalmente usado para acelerar a computação gráfica, mas acontece que também tem sido muito útil para acelerar em redes neurais. Mas mesmo com aceleradores de hardware além de um certo ponto, as redes neurais são tão grandes que demoram tanto para serem treinadas que se tornam inviáveis. E, claro, a outra limitação são mais dados. Às vezes, você só consegue obter uma quantidade limitada de dados e, além de um certo ponto, é difícil obter muito mais dados. Mas acho que essa receita explica muito do aumento do aprendizado profundo nos últimos anos, que é para aplicativos nos quais você tem acesso a muitos dados. Então, ser capaz de treinar grandes redes neurais permite que você eventualmente obtenha um desempenho muito bom em muitos aplicativos. Uma coisa implícita neste slide que pode não ter sido óbvia é que, ao desenvolver um algoritmo de aprendizado, às vezes você descobre que tem um alto viés. Nesse caso, você faz coisas como aumentar sua rede neural. Mas depois de aumentar sua rede neural, você pode descobrir que tem uma alta variação. Nesse caso, você pode fazer outras coisas, como coletar mais dados. E durante as horas, dias ou semanas, você desenvolve um algoritmo de aprendizado de máquina em pontos diferentes. Você pode ter grandes compras ou alta variação. E isso pode mudar, mas depende de seu algoritmo ter alto viés ou alta variância naquele momento. Então, isso pode ajudar a orientar o que você deve tentar a seguir. Quando você treina sua rede neural, uma coisa que as pessoas me perguntaram antes é, ei Andrew, e se minha rede neural for muito grande? Isso criará um problema de alta variância? Acontece que uma grande rede neural com regularização bem escolhida geralmente funciona tão bem ou melhor do que uma menor. Então, por exemplo, se você tem uma pequena rede neural como essa e mudasse para uma rede neural muito maior como essa, você pensaria que o risco de sobreajuste aumenta significativamente. Mas acontece que, se você regularizar essa rede neural maior de forma adequada, essa rede neural maior geralmente funcionará pelo menos tão bem ou melhor do que a menor. Desde que a regularização tenha sido escolhida adequadamente.
Reproduza o vídeo começando em :7:22 e siga a transcrição7:22
Então, outra maneira de dizer isso é que quase nunca é demais lançar uma rede neural, desde que você se regularize adequadamente com uma ressalva, que é que quando você treina uma rede neural maior, ela se torna mais computacional e cara. Portanto, a principal forma de doer é desacelerar seu treinamento e seu processo de inferência e, brevemente, regularizar uma rede neural. Isso é o que você faz se a função de custo de sua rede neural for a perda média e, portanto, o último ano pode ser um erro quadrático ou uma perda logística. Então, o termo de regularização para uma rede neural parece que o que você esperaria é maior que dois m vezes a soma de w ao quadrado, onde isso é uma música sobre sempre W na rede neural e semelhante à regularização para regressão linear e regressão logística, geralmente não regularizamos os parâmetros da rede neural, embora na prática faça muito pouca diferença se você faz isso ou não. E a maneira como você implementaria a regularização no tensorflow é lembrar que esse era o código para implementar um modelo de classificação de dígitos manuscrito Rising não regulamentado. Criamos três camadas dessa forma com a ativação de várias unidades de ajuste e, em seguida, criamos um modelo sequencial com as três camadas.
Reproduza o vídeo começando em :8:52 e siga a transcrição8:52
Se você quiser adicionar regularização, basta adicionar esse termo extra coronel regularizar A é igual a l. dois e depois 0,01, onde esse é o valor de mais longo em termos de, mas, na verdade, permite escolher valores diferentes de lambda para camadas diferentes, embora, para simplificar, você possa escolher o mesmo valor de lambda para todos os pesos e todas as camadas diferentes da seguinte forma. E então isso permitirá que você implemente a regularização em sua rede neural.
Reproduza o vídeo começando em :9:18 e siga a transcrição9:18
Então, para resumir duas conclusões, espero que você tenha aprendido neste vídeo. Quase nunca é demais ter uma rede neural maior, desde que você se regularize adequadamente. uma ressalva é que ter uma rede neural maior pode desacelerar seu algoritmo. Então, talvez essa seja a única maneira de doer, mas não deve prejudicar o desempenho do seu álbum na maior parte e, na verdade, pode até ajudar significativamente. E segundo, desde que seu conjunto de treinamento não seja muito grande. Então, uma nova rede, especialmente uma rede neural grande, costuma ser uma máquina de baixa polarização. Ele se encaixa muito bem em funções muito complicadas, e é por isso que, quando estou treinando redes neurais, acho que geralmente estou enfrentando vários problemas em vez de problemas de preconceito, pelo menos se a rede neural for grande o suficiente. Portanto, a ascensão do aprendizado profundo realmente mudou a forma como os profissionais de aprendizado de máquina pensam sobre preconceitos e variações. Dito isso, mesmo quando você está treinando uma rede neural medindo preconceitos e variâncias e usando isso para Deus, o que você faz a seguir geralmente é uma coisa muito útil.
Reproduza o vídeo começando em :10:23 e siga a transcrição10:23
Então, isso é tudo para preconceitos e variâncias. Vamos para o próximo vídeo. Vamos pegar todas as ideias que aprendemos e ver como elas se encaixam no processo de desenvolvimento de sistemas de aprendizado de máquina. E espero que juntemos muitas dessas peças para dar conselhos práticos sobre como avançar rapidamente no desenvolvimento de seus sistemas de aprendizado de máquina.
