Se você estiver trabalhando em um aplicativo de aprendizado de máquina em que a proporção de exemplos positivos e negativos é muito distorcida, muito longe de 50 a 50 , acontece que as métricas de erro usuais, como precisão, não funcionam muito bem. Vamos começar com um exemplo. Digamos que você esteja treinando um classificador binário para detectar uma doença rara em pacientes com base em testes de laboratório ou com base em outros dados dos pacientes. Y é igual a 1 se a doença estiver presente e y é igual a 0 caso contrário. Suponha que você descubra que obteve um por cento de erro no conjunto de testes e, portanto, tem um diagnóstico correto de 99%. Parece um ótimo resultado. Mas acontece que, se essa é uma doença rara, então y é igual a 1, muito raramente, isso pode não ser tão impressionante quanto parece. Especificamente, se é uma doença rara e se apenas 0,5% dos pacientes em sua população têm a doença, então se você escreveu o programa, dito isso, imprimir y é igual a 0. Ele prevê que y é igual a 0 o tempo todo. Esse algoritmo muito simples, mesmo que não aprenda, porque diz apenas que y é igual a 0 o tempo todo, na verdade terá 99,5% de precisão ou 0,5% de erro. Esse algoritmo realmente idiota supera seu algoritmo de aprendizado, que teve um por cento de erro, muito pior do que um erro de 0,5 por cento. Mas acho que um software que apenas imprime y igual a 0 não é uma ferramenta de diagnóstico muito útil. O que isso realmente significa é que você não sabe se obter um por cento de erro é realmente um resultado bom ou ruim.
Reproduza o vídeo começando em :1:55 e siga a transcrição1:55
Em particular, se você tiver um algoritmo que atinge 99,5% de precisão, outro que atinge 99,2% de precisão, outro que atinge 99,6% de precisão. É difícil saber qual deles é realmente o melhor algoritmo. Porque se você tem um algoritmo que atinge 0,5% de erro e outro que atinge um por cento de erro e outro que atinge 1,2% de erro, é difícil saber qual deles é o melhor algoritmo. Como aquela com o menor erro pode não ser uma previsão particularmente útil como essa, que sempre prevê que y é igual a 0 e nunca diagnostica nenhum paciente como portador dessa doença. Muito possivelmente, um algoritmo que tenha um por cento de erro, mas que pelo menos diagnostique alguns pacientes como portadores da doença pode ser mais útil do que simplesmente imprimir y igual a 0 o tempo todo. Ao trabalhar com problemas com conjuntos de dados distorcidos, geralmente usamos uma métrica de erro diferente em vez de apenas um erro de classificação para descobrir o desempenho do seu algoritmo de aprendizado. Em particular, um par comum de métricas de erro é precisão e recuperação, que definiremos no slide. Neste exemplo, y igual a um será a classe rara, como a doença rara que talvez queiramos detectar. Em particular, avaliar o desempenho de um algoritmo de aprendizado com uma classe rara de útil para construir o que é chamado de matriz de confusão, que é uma matriz de dois por dois ou uma tabela de dois por dois que se parece com isso. No eixo superior, vou escrever a classe real, que pode ser um ou zero. No eixo vertical, vou escrever a classe prevista, que é o que seu algoritmo de aprendizado prevê em um determinado exemplo, um ou zero? Para avaliar o desempenho do seu algoritmo no conjunto de validação cruzada ou no conjunto de testes, digamos, contaremos quantos exemplos? A classe real era 1 e a classe prevista 1? Talvez você tenha 100 exemplos de validação cruzada e, em 15 deles, o algoritmo de aprendizado tenha previsto um e o rótulo real também tenha sido um. Aqui, você contaria o número de exemplos em C ou conjunto de validação cruzada em que a classe real era zero e seu algoritmo previu uma. Talvez você tenha cinco exemplos aqui e aqui previu a Classe 0, a Classe 1 real. Você tem 10 exemplos e, digamos, 70 exemplos com Classe 0 prevista e Classe 0 real. Neste exemplo, a inclinação não é tão extrema quanto a que eu tinha no slide anterior. Porque nesses 100 exemplos em seu conjunto de validação cruzada, temos um total de 25 exemplos em que a classe real era uma e 75 em que a classe real era zero, somando esses números verticalmente. Você também notará que estou usando cores diferentes para indicar essas quatro células na tabela. Na verdade, vou dar nomes a essas quatro células. Quando a classe real é uma e a classe prevista é uma, vamos chamar isso de verdadeiro positivo porque você previu positivo e era verdade, há um exemplo positivo. Nesta célula no canto inferior direito, onde a classe real é zero e a classe prevista é zero, chamaremos isso de verdadeiro negativo porque você previu negativo e era verdadeiro. Realmente foi um exemplo negativo. Essa célula no canto superior direito é chamada de falso positivo porque o algoritmo previu positivo, mas era falso. Na verdade, não é positivo, então isso é chamado de falso positivo. Essa célula é chamada de número de falsos negativos porque o algoritmo previu zero, mas era falsa. Na verdade, não foi negativo. A aula real era uma. Depois de dividir as classificações nessas quatro células, duas métricas comuns que você pode calcular são a precisão e a recuperação. Aqui está o que eles significam. A precisão do algoritmo de aprendizado calcula todos os pacientes em que previmos que y é igual a 1, qual fração realmente tem a doença rara. Em outras palavras, a precisão é definida como o número de positivos verdadeiros dividido pelo número classificado como positivo. Em outras palavras, de todos os exemplos que você previu como positivos, qual fração nós realmente acertamos.
Reproduza o vídeo começando em :6:51 e siga a transcrição6:51
Outra forma de escrever essa fórmula seria os verdadeiros positivos divididos por verdadeiros positivos mais falsos positivos, porque é somando essa célula e essa célula que você acaba com o número total previsto como positivo. Neste exemplo, o numerador, verdadeiros positivos, seria 15 e dividido por 15 mais 5, ou seja, 15 sobre 20 ou três quartos, 0,75. Então, dizemos que esse algoritmo tem uma precisão de 75 por cento por causa de todas as coisas que ele previu como positivas, de todos os pacientes que ele achava que tinham essa doença rara, estava certo 75 por cento das vezes. A segunda métrica útil para calcular é a recuperação. E lembre-se de perguntar: De todos os pacientes que realmente têm a doença rara, qual fração detectamos corretamente como portadora dela? O recall é definido como o número de positivos verdadeiros dividido pelo número de positivos reais. Como alternativa, podemos escrever isso como o número de positivos verdadeiros dividido pelo número de positivos reais. Bem, é essa célula mais essa célula. Então, na verdade, é o número de verdadeiros positivos mais o número de falsos negativos, porque é somando essa célula superior esquerda e essa célula inferior esquerda que você obtém o número de exemplos positivos reais. Em nosso exemplo, isso seria 15 dividido por 15 mais 10, que é 15 sobre 25, que é 0,6 ou 60 por cento. Esse algoritmo de aprendizado teria 0,75 de precisão e 0,60 de recall. Você percebe que isso o ajudará a detectar se o algoritmo de aprendizado está apenas imprimindo y igual a 0 o tempo todo. Porque se ele prediz zero o tempo todo , o numerador de ambas as quantidades seria zero. Não tem pontos positivos verdadeiros. A métrica de recall, em particular, ajuda a detectar se o algoritmo de aprendizado está prevendo zero o tempo todo. Porque se seu algoritmo de aprendizado apenas imprimir y igual a 0 , o número de positivos verdadeiros será zero porque ele nunca prediz positivos e, portanto, o recall será igual a zero dividido pelo número de positivos reais, que é igual a zero. Em geral, um algoritmo de aprendizado com precisão zero ou evocação zero não é um algoritmo útil. Mas, como uma observação lateral, se um algoritmo realmente prevê zero o tempo todo, a precisão na verdade se torna indefinida porque, na verdade, está zero acima. zero. Mas, na prática, se um algoritmo não prevê nem mesmo um único positivo, dizemos apenas que a precisão também é igual a zero. Mas descobriremos que calcular a precisão e a recordação facilita identificar se um algoritmo é razoavelmente preciso, pois, quando diz que um paciente tem uma doença, há uma boa chance de o paciente ter uma doença, como 0,75 de chance neste exemplo, e também garantindo que, de todos os pacientes que têm a doença, está ajudando a diagnosticar uma fração razoável deles, como aqui está encontrando 60% deles. Quando você tem uma aula rara, analisando a precisão e a recordação e certificando-se de que os dois números sejam decentemente altos, espero que isso ajude a garantir que seu algoritmo de aprendizado é realmente útil. O termo recordação foi motivado pela observação de que, se você tiver um grupo de pacientes ou uma população de pacientes, lembre-se de medir, de todos os pacientes que têm a doença, quantos você teria diagnosticado com precisão como portadora da doença. Portanto, quando você tem classes distorcidas ou uma classe rara que deseja detectar, a precisão e a recuperação ajudam você a saber se seu algoritmo de aprendizado está fazendo boas previsões ou previsões úteis. Agora que temos essas métricas para mostrar o desempenho do seu algoritmo de aprendizado, no próximo vídeo, vamos dar uma olhada em como trocar precisão e recordação para tentar otimizar o desempenho do seu algoritmo de aprendizado.
