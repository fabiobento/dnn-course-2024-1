O processo de construção de uma árvore de decisão com base em um conjunto de treinamento tem algumas etapas. Neste vídeo, vamos dar uma olhada no processo geral do que você precisa fazer para criar uma árvore de decisão. Recebi um conjunto de treinamento de 10 exemplos de cães e gatos, como você viu no último vídeo. A primeira etapa do aprendizado da árvore de decisão é decidir qual recurso usar no nó raiz. Esse é o primeiro nó no topo da árvore de decisão. Por meio de um algoritmo sobre o qual falaremos nos próximos vídeos. Digamos que decidimos escolher como característica e nó raiz, a característica do formato da orelha. O que isso significa é que decidiremos examinar todos os nossos exemplos de treinamento, todos os exemplos de tangentes mostrados aqui. Eu os divido de acordo com o valor do recurso de formato da orelha. Em particular, vamos escolher os cinco exemplos com orelhas pontudas e movê-los para a esquerda. Vamos escolher os cinco exemplos com orelhas flexíveis e movê-los para a direita. A segunda etapa é focar apenas na parte esquerda ou, às vezes, chamada de ramo esquerdo da árvore de decisão, para decidir quais nós colocar ali. Em particular, em qual recurso queremos dividir ou qual recurso queremos usar em seguida. Por meio de um algoritmo sobre o qual, novamente, falaremos no final desta semana. Digamos que você decida usar o recurso de formato de rosto lá. O que faremos agora é pegar esses cinco exemplos e dividi-los em dois subconjuntos com base no valor do formato do rosto. Vamos pegar os quatro exemplos desses cinco com formato de rosto redondo e movê-los para a esquerda. O único exemplo com um formato de rosto não redondo e mova-o para baixo para a direita. Finalmente, notamos que esses quatro exemplos são todos gatos, quatro deles são gatos. Em vez de nos dividirmos ainda mais, criamos um nódulo foliar que faz uma previsão de que coisas que se resumem a nada de outros gatos. Aqui, notamos que nenhum dos exemplos, zero de um exemplo, são gatos ou, alternativamente, 100% dos exemplos aqui são cães. Podemos criar um nódulo foliar aqui que faz uma previsão de que não é um gato. Depois de fazer isso na parte esquerda do ramo esquerdo dessa árvore de decisão, agora repetimos um processo semelhante na parte direita ou na ramificação direita dessa árvore de decisão. Concentre a atenção apenas nesses cinco exemplos, que contêm um capitão para cães. Teríamos que escolher algum recurso aqui para dividir ainda mais esses cinco exemplos. Se acabássemos escolhendo o recurso de bigodes, dividiríamos esses cinco exemplos com base em onde os bigodes estão presentes ou ausentes, assim. Você percebe que um em cada um dos exemplos à esquerda para gatos e zeros em cada quatro são gatos. Cada um desses nós é completamente puro, ou seja, todos gatos ou não gatos e não há mais uma mistura de cães e gatos. Podemos criar esses nós foliares, fazendo uma previsão do gato à esquerda e uma previsão da touca noturna aqui à direita. Esse é um processo de construção de uma árvore de decisão. Por meio desse processo,
Reproduza o vídeo começando em :3:39 e siga a transcrição3:39
tivemos que tomar algumas decisões importantes em várias etapas do algoritmo. Vamos falar sobre quais foram essas decisões-chave e acompanharemos os detalhes de como tomar essas decisões nos próximos vídeos. A primeira decisão importante foi: como escolher quais recursos usar para dividir em cada nó? No nó raiz, bem como no galho esquerdo e direito da árvore de decisão, tivemos que decidir se havia alguns exemplos naquele nó que incluíam uma mistura de cães e gatos. Você quer dividir a característica em forma de orelha, a característica facial ou a característica dos bigodes? Veremos no próximo vídeo que as árvores de decisão escolherão em qual recurso dividir para tentar maximizar a pureza. Por pureza, quero dizer, você quer chegar a quais subconjuntos, que são o mais próximos possível de todos os gatos ou cães. Por exemplo, se tivéssemos uma característica que dissesse se esse animal tem DNA de gato, na verdade não temos essa característica. Mas se o fizéssemos, poderíamos ter dividido essa característica no nó raiz, o que resultaria em cinco em cada cinco gatos no galho esquerdo e zero dos cinco gatos no galho direito. Esses subconjuntos de dados esquerdo e direito são completamente puros, o que significa que há apenas uma classe, apenas gatos ou não gatos apenas em ambas as sub-ramificações esquerda e direita, e é por isso que o recurso de DNA do gato, se tivéssemos esse recurso, teria sido um ótimo recurso para usar. Mas com as características que realmente temos, tivemos que decidir
Reproduza o vídeo começando em :5:18 e siga a transcrição5:18
qual é a divisão no formato do ano, que resulta em quatro dos cinco exemplos à esquerda sendo gatos, e um dos cinco exemplos à direita sendo gatos ou formato de rosto, onde resultou em quatro dos sete à esquerda e um dos três à direita, ou bigodes, o que resultou em três dos quatro exemplos sendo lançados à esquerda e dois em cada seis não sendo gatos à direita. O algoritmo de aprendizado da árvore de decisão precisa escolher entre formato de orelha, formato de rosto e bigodes. Quais dessas características resultam na maior pureza dos rótulos nos subramos esquerdo e direito? Porque, se você puder obter um subconjunto altamente puro de exemplos, poderá prever gatos ou não gatos e acertar na maioria das vezes. No próximo vídeo sobre entropia, falaremos sobre como estimar a impureza e como minimizar a impureza. A primeira decisão que precisamos tomar ao aprender uma árvore de decisão é como escolher qual recurso, o salão e cada nó. A segunda decisão importante que você precisa tomar ao criar uma árvore de decisão é decidir quando parar de dividir. O critério que usamos agora era até eu saber que existem 100 por cento, todos gatos ou 100 por cento de cães e não gatos. Porque nesse ponto parece natural construir um nó foliar que apenas faça uma previsão de classificação. Como alternativa, você também pode decidir interromper a divisão ao dividir e não fazer com que a árvore exceda a profundidade máxima. A profundidade máxima que você permite que a árvore atinja é um parâmetro que você poderia simplesmente dizer. Na árvore de decisão, a profundidade de um nó é definida como o número de saltos necessários para ir do nó raiz, indicado no topo, até aquele nó específico. Portanto, o nódulo raiz não dá saltos, sobe sozinho e está na profundidade 0. As notas abaixo estão na profundidade um e nas abaixo estariam na profundidade 2. Se você tivesse decidido que a profundidade máxima da árvore de decisão é, digamos, dois, então você decidiria não dividir nenhum nó abaixo desse nível para que a árvore nunca chegue à profundidade 3. Um dos motivos
Reproduza o vídeo começando em :7:55 e siga a transcrição7:55
pelos quais você pode querer limitar a profundidade da árvore de decisão é garantir que a árvore não fique muito grande e pesada e, segundo, ao mantê-la pequena, ela fica menos propensa a se encaixar demais. Outro critério que você pode usar para decidir parar de dividir pode ser se as melhorias na pontuação de prioridade, que você vê em um vídeo posterior, estão abaixo de um determinado limite. Se a divisão de um nó resultar em melhorias mínimas na pureza ou, você verá mais tarde, na verdade, haverá uma diminuição na impureza. Mas se os ganhos forem muito pequenos, eles podem não se preocupar. Novamente, tanto para manter as árvores menores quanto para reduzir o risco de sobremontagem. Finalmente, se o número de exemplos de um nó estiver abaixo de um determinado limite, você também poderá decidir interromper a divisão. Por exemplo, se no nó raiz dividimos a característica do formato do rosto , o ramo direito terá apenas três exemplos de treinamento com um gato e dois cães e, em vez de dividi-lo em subconjuntos ainda menores, se você decidir não dividir mais exemplos com apenas três de seus exemplos, você apenas criará um nó de decisão e, como talvez haja cães para outros três adultos aqui, esse seria um nó e isso faz uma previsão de não gato. Novamente, um dos motivos pelos quais você pode decidir que não vale a pena dividir é manter a árvore menor e evitar o encaixe excessivo. Quando eu mesmo vejo a árvore de decisão aprendendo tarefas, às vezes sinto que , cara, há muitas partes diferentes e muitas coisas diferentes acontecendo nesse algoritmo. Parte do motivo pelo qual isso pode parecer está na evolução das árvores de decisão. Houve um pesquisador que propôs uma versão básica das árvores de decisão e, em seguida, outro pesquisador disse , oh, podemos modificar essa coisa dessa forma, como seu novo critério de divisão. Então, outro pesquisador surge com uma coisa diferente, como, oh, talvez devêssemos parar de suar quando ela atinge uma certa profundidade máxima. Com o passar dos anos, diferentes pesquisadores criaram diferentes refinamentos no algoritmo. Como resultado disso, funciona muito bem, mas analisamos todos os detalhes de como implementar uma decisão G. Há muitas partes diferentes, como por que existem tantas maneiras diferentes de decidir quando parar de dividir. Se parece uma média um pouco complicada e confusa para você, também parece para mim. Mas essas diferentes partes se encaixam em um algoritmo de aprendizado muito eficaz e o que você aprende neste curso é a principal
Reproduza o vídeo começando em :10:34 e siga a transcrição10:34
e mais importante ideia de como fazê-lo funcionar bem. No final desta semana, também compartilharei com vocês algumas orientações, algumas sugestões de como usar pacotes de código aberto para que você não precise complicar o procedimento para tomar todas essas decisões. Por exemplo, como eu decido parar de me dividir? Você realmente faz com que esses átomos funcionem bem para si mesmo. Mas quero garantir que, se esse algoritmo parece complicado e confuso, francamente também funciona para mim, mas funciona bem. Agora, a próxima decisão-chave na qual quero me aprofundar é como você decide como dividir um nó. No próximo vídeo, vamos dar uma olhada nessa definição de entropia, que seria uma forma de medir a pureza, ou mais precisamente, a impureza em um nó. Vamos para o próximo vídeo.
e : adicionado à seleção. Pressione [CTRL + S] para salvar como anotação
(Obrigatória)
pt-BR
​


