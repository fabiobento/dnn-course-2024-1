Agora que temos uma maneira de usar algo com substituição para criar novos conjuntos de treinamento que são um pouco semelhantes, mas também bem diferentes do conjunto de treinamento original. Estamos prontos para criar nosso primeiro algoritmo de conjunto de árvores. Em particular, neste vídeo, falaremos sobre o algoritmo de floresta aleatória, que é um algoritmo poderoso de árvore em amostra que funciona muito melhor do que usar uma única árvore de decisão. Veja como podemos gerar um conjunto de árvores. Se você receber um conjunto de treinamento de tamanho M, então para B é igual a 1 com b maiúsculo, então fazemos isso maiúsculo B vezes.
Reproduza o vídeo começando em ::40 e siga a transcrição0:40
Você pode usar algo com substituição para criar um novo conjunto de treinamento de tamanho M. Então, se você tiver 10 exemplos de treinamento, colocará os 10 exemplos de treinamento nessa bolsa virtual e uma amostra de substituição 10 vezes para gerar um novo conjunto de treinamento com também 10 exemplos, e então você treinaria uma árvore de decisão nesse conjunto de dados. Então, aqui está o conjunto de dados que eu gerei usando algo com substituição. Se você observar com atenção, poderá notar que alguns dos exemplos de treinamento se repetem e está tudo bem. E se você treinar a decisão com base nesses dados, você acaba com essa árvore de decisão. E tendo feito isso uma vez, repetiríamos isso uma segunda vez. Use algo com substituição para gerar outro conjunto de treinamento de M ou 10 exemplos de treinamento. Novamente, isso se parece um pouco com o conjunto de treinamento original , mas também é um pouco diferente. Em seguida, você treina a árvore de decisão nesse novo conjunto de dados e acaba com uma árvore de decisão um pouco diferente. E assim por diante. E você pode fazer isso um total de B vezes maiúsculas. Escolha típica de maiúscula B: o número dessas árvores que você construiu pode ser de cerca de 100 pessoas, recomendando qualquer valor a partir de, digamos, 64, 228. E tendo construído um conjunto de, digamos, 100 árvores diferentes, você obteria então, ao tentar fazer uma previsão, obter todas as votos dessas árvores na previsão final correta. Acontece que definir o capital B como maior nunca prejudica o desempenho, mas, além de um certo ponto, você acaba com retornos decrescentes e, na verdade, não fica muito melhor quando B é muito maior do que, digamos, 100 ou mais. E é por isso que eu nunca uso, digamos, 1000 árvores que apenas diminuem significativamente a computação sem aumentar significativamente o desempenho do algoritmo geral.
Reproduza o vídeo começando em :2:35 e siga a transcrição2:35
Só para dar um nome a esse álbum em particular.
Reproduza o vídeo começando em :2:43 e siga a transcrição2:43
Essa instância específica de criação de conjunto de árvores às vezes também é chamada de árvore de decisão ensacada. E isso se refere a colocar seus exemplos de treinamento nessa bolsa virtual. E é por isso que também usamos o letus minúsculo B e o B maiúsculo aqui, porque isso significa bolsa. Há uma modificação neste álbum que realmente fará com que ele funcione ainda melhor e que transforma esse algoritmo, a árvore de decisão reversa, no algoritmo de floresta aleatória. A ideia principal é que, mesmo com esse procedimento de amostragem com substituição, às vezes você acaba sempre usando a mesma divisão no nó raiz e divisões muito semelhantes perto da nota raiz. Isso não aconteceu neste exemplo específico em que uma pequena mudança nos treinamentos resultou em uma divisão diferente na nota raiz. Mas para outros conjuntos de treinamento, não é incomum que, para muitos ou mesmo todos os conjuntos de treinamento B maiúsculo, você acabe com a mesma opção de recurso no nó raiz e em algumas notas próximas à nota raiz. Portanto, há uma modificação no algoritmo para tentar randomizar ainda mais a escolha do recurso em cada nota, o que pode fazer com que o conjunto de árvores e você aprenda a se tornarem mais diferentes umas das outras. Então, quando você vota neles, você acaba com uma previsão ainda mais precisa. Normalmente, isso é feito em todas as notas ao escolher um recurso a ser usado para dividir se os recursos finais estiverem disponíveis. Portanto, em nosso exemplo, tínhamos três recursos disponíveis em vez de escolher entre todos os recursos finais. Em vez disso, escolheremos um subconjunto aleatório de K menos de N recursos. E permita que o algoritmo escolha somente esse subconjunto de recursos K. Então, em outras palavras, você escolheria K recursos como os recursos permitidos e , dentre esses K recursos, escolheria aquele com o maior ganho de informação como a escolha do recurso para usar a divisão. Quando N é grande, digamos que n seja dezenas ou 10 ou até centenas. Uma escolha típica para o valor de K seria escolher a raiz quadrada de N. Em nosso exemplo, temos apenas três recursos e essa técnica tende a ser usada mais para problemas maiores com um número maior de recursos. E só mudará ainda mais o algoritmo, você acabará com o algoritmo aleatório Forest, que normalmente funcionará muito melhor e se tornará muito mais robusto do que apenas uma única árvore de decisão. Uma maneira de pensar por que isso é mais robusto do que uma única árvore de decisão é que o procedimento de substituição de algo faz com que o algoritmo já explore muitas pequenas alterações nos dados, treine diferentes árvores de decisão e calcule a média de todas essas alterações nos dados que o procedimento de substituição causa. Isso significa que qualquer pequena alteração adicional no conjunto de treinamento torna menos provável que tenha um grande impacto na produção geral do algoritmo geral de floresta aleatória. Porque ele já foi explorado e tem uma média de pequenas mudanças no conjunto de treinamento. Antes de encerrar este vídeo, há apenas mais uma ideia que eu gostaria de compartilhar. Ou seja, onde um engenheiro de aprendizado de máquina acampa? Em uma floresta aleatória. Tudo bem. Vá e conte essa piada para seus amigos. Espero que você goste. A floresta aleatória é uma sala eficaz e espero que você a use melhor em seu trabalho. Além da floresta aleatória, acontece que há um outro algoritmo que funciona ainda melhor. Que é uma árvore de decisão aprimorada. No próximo vídeo, vamos falar sobre um algoritmo de árvore de decisão aprimorado chamado X G boost.
