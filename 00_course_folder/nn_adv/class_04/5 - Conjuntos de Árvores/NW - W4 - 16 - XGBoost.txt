Ao longo dos anos, os pesquisadores de aprendizado de máquina criaram várias maneiras diferentes de criar árvores de decisão e árvores de decisão em amostras. Hoje, de longe, a forma ou implementação mais comumente usada de conjuntos de árvores de decisão ou árvores de decisão é um álbum chamado XGBoost. Ele é executado rapidamente, as implementações de código aberto são facilmente usadas e também foram usadas com muito sucesso para vencer muitas competições de aprendizado de máquina, bem como em muitos aplicativos comerciais. Vamos dar uma olhada em como o XGBoost funciona. Há uma modificação no algoritmo da árvore de decisão reversa que vimos no último vídeo que pode fazer com que ele funcione muito melhor. Aqui, novamente, está o álbum que escrevemos anteriormente. Dado o conjunto de treinamento para dimensioná-los, você repete B vezes, usa algo com substituição para criar um novo conjunto de treinamento de tamanho M e , em seguida, treina a árvore de decisão no novo conjunto de dados. Então, na primeira vez nesse ciclo, podemos criar um conjunto de treinamento como esse e treinar uma árvore de decisão como essa. Mas é aqui que vamos mudar o algoritmo, que passa sempre por esse loop, exceto na primeira vez, ou seja, na segunda vez, na terceira vez e assim por diante. Ao amostrar, em vez de escolher entre todos os m exemplos de probabilidade igual com probabilidade de um sobre m, vamos aumentar a probabilidade de escolhermos exemplos mal classificados nos quais as árvores previamente treinadas se saem mal. Em treinamento e educação, existe uma ideia chamada prática deliberada. Por exemplo, se você está aprendendo a tocar piano e está tentando dominar uma peça no piano em vez de praticar a peça inteira, digamos, cinco minutos repetidamente, o que consome muito tempo. Se, em vez disso, você tocar a peça e depois focar sua atenção apenas nas partes da peça que ainda não está tocando muito bem, pratique essas partes menores repetidamente. Então, essa é uma maneira mais eficiente de aprender a tocar piano bem. Portanto, essa ideia de impulsionar é semelhante. Vamos examinar as árvores de decisão que treinamos até agora e ver o que ainda não estamos fazendo bem. E então, ao criar a próxima árvore de decisão, vamos focar mais atenção nos exemplos de que ainda não estamos fazendo bem. Portanto, em vez de analisar todos os exemplos de treinamento, focamos mais atenção no subconjunto de exemplos em que ainda não está indo bem e obtemos a nova árvore de decisão, o próximo conjunto de relatórios de árvores de decisão, para tentar nos sair bem com eles. E essa é a ideia por trás do impulsionamento , que acaba ajudando o algoritmo de aprendizado a aprender a melhorar mais rapidamente. Então, detalhadamente, examinaremos essa árvore que acabamos de construir e voltaremos ao conjunto de treinamento original. Observe que esse é o conjunto de treinamento original, não gerado por meio de uma substituição. E examinaremos todos os dez exemplos e veremos o que essa árvore de decisão aprendida prevê em todos os dez exemplos. Portanto, esta quarta coluna contém suas previsões e coloca uma marca de seleção ao lado de cada exemplo, dependendo se a classificação das árvores estava correta ou incorreta.
Reproduza o vídeo começando em :3:9 e siga a transcrição3:09
Então, o que faremos na segunda vez nesse ciclo é usar algo com substituição para gerar outro conjunto de treinamento de dez exemplos. Mas toda vez que escolhermos um exemplo desses dez, teremos uma chance maior de escolher um desses três exemplos que ainda estavam sendo classificados incorretamente. Então, isso concentra a atenção das árvores de segunda decisão por meio de um processo como a prática deliberada, nos exemplos de que o álbum ainda não está indo muito bem. E o procedimento de reforço fará isso por um total de B vezes, onde, em cada iteração, você observa em que o conjunto de árvores para as árvores de 1, 2 até (b- 1) ainda não está indo muito bem. E ao construir a árvore número b, você terá uma probabilidade maior de escolher exemplos nos quais o conjunto das árvores de amostra anterior ainda não está indo bem. Os detalhes matemáticos de exatamente quanto aumentar a probabilidade de escolher esse exemplo em comparação com aquele exemplo são bastante complexos, mas você não precisa se preocupar com eles para usar implementações de árvore avermelhada. E das diferentes formas de implementar o impulsionamento, a mais usada atualmente é o XGBoost, que significa aumento de gradiente extremo, que é uma implementação de código aberto de árvores impulsionadas que é muito rápida e eficiente. O XGBoost também tem uma boa escolha de critérios de divisão padrão e critérios para quando parar de dividir. E uma das inovações do XGBoost é que ele também incorporou uma regularização para evitar ajustes excessivos e em competições de aprendizado de máquina, como faz um site de competição amplamente usado chamado Kaggle. O XGBoost costuma ser um algoritmo altamente competitivo. Na verdade, o XGBoost e os algoritmos de aprendizado profundo parecem ser os dois tipos de algoritmos que vencem muitas dessas competições. E uma nota técnica, em vez de fazer algo com o substituto do XGBoost, na verdade, atribui maneiras diferentes a diferentes exemplos de treinamento. Portanto, na verdade, ele não precisa gerar muitos conjuntos de treinamento escolhidos aleatoriamente e isso o torna um pouco mais eficiente do que usar uma amostragem com procedimento de substituição. Mas a intuição que você viu no slide anterior ainda está correta em termos de como o XGBoost está escolhendo exemplos para se concentrar. Os detalhes do XGBoost são bastante complexos de implementar, e é por isso que muitos profissionais usarão as bibliotecas de código aberto que implementam o XGBoost. Isso é tudo o que você precisa fazer para usar o XGBoost. Você importará a biblioteca XGBoost da seguinte maneira e inicializará um modelo como um classificador XGBoost. Modelo adicional e, finalmente, isso permite que você faça previsões usando esse algoritmo de árvores de decisão aprimoradas. Espero que você ache esse algoritmo útil para muitos aplicativos que você possa criar no futuro. Ou, alternativamente, se você quiser usar o XGBoost para regressão em vez de para classificação, essa linha aqui se torna XGBRegressor e o resto do código funciona de forma semelhante. Então é isso para o algoritmo XGBoost. Temos apenas um último vídeo para esta semana e para este curso, onde encerraremos e também falaremos sobre quando você deve usar uma árvore de decisão em vez de talvez usar a rede neural. Vamos ao último e último vídeo desta semana.
