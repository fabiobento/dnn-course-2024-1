Um dos pontos fracos de usar uma única árvore de decisão é que essa árvore de decisão pode ser altamente sensível a pequenas mudanças nos dados. Uma solução para tornar a flecha menos sensível ou mais robusta é construir não uma árvore de decisão, mas construir muitas árvores de decisão, e chamamos isso de conjunto de árvores. Vamos dar uma olhada. Com o exemplo que estamos usando, o melhor recurso a ser dividido no nó raiz acabou sendo o formato de orelha, resultando nesses dois subconjuntos de dados e, em seguida, criando outras subárvores nesses dois subconjuntos dos dados.
Reproduza o vídeo começando em ::39 e siga a transcrição0:39
Mas acontece que se você pegasse apenas um dos dez exemplos e o mudasse para um gato diferente para que, em vez de ter orelhas pontudas, rosto redondo, bigodes ausentes, esse novo gato tivesse orelhas caídas, rosto redondo e bigodes presentes, com apenas a mudança de um único exemplo de treinamento, o recurso de maior ganho de informação a ser dividido se tornaria o recurso de bigodes em vez do recurso de formato de orelha. Como resultado disso, os subconjuntos de dados que você obtém nas subárvores esquerda e direita se tornam totalmente diferentes e, à medida que você continua executando o algoritmo de aprendizado da árvore de decisão recursivamente, você cria subárvores totalmente diferentes à esquerda e à direita. O fato de alterar apenas um exemplo de treinamento fazer com que o algoritmo crie uma divisão diferente na raiz e , portanto, uma árvore totalmente diferente, faz com que esse algoritmo não seja tão robusto. É por isso que, quando você usa árvores de decisão, geralmente obtém um resultado muito melhor, ou seja, obtém previsões mais precisas se treinar não apenas uma única árvore de decisão , mas várias árvores de decisão diferentes. Isso é o que chamamos de conjunto de árvores, o que significa apenas uma coleção de várias árvores. Veremos, nos próximos vídeos, como construir esse conjunto de árvores. Mas se você tivesse esse conjunto de três árvores, cada uma delas talvez fosse uma forma plausível de classificar gato versus não gato. Se você tivesse um novo exemplo de teste que quisesse classificar , o que você faria seria executar todas essas três árvores em seu novo exemplo e fazer com que elas votassem se é a previsão final. Este exemplo de teste tem orelhas pontudas, um formato de rosto não redondo e bigodes estão presentes, então a primeira árvore faria inferências como essa e prediria que é um gato. A inferência da segunda árvore seguiria esse caminho através da árvore e, portanto, prediria que não é gato. A terceira árvore seguiria esse caminho e , portanto, prediria que é um gato. Essas três árvores fizeram previsões diferentes e, portanto, o que faremos é realmente fazer com que elas votem. A maioria dos votos das previsões entre essas três árvores é, gato. A previsão final desse conjunto de árvores é que se trata de um gato, o que por acaso é a previsão correta. O motivo pelo qual usamos um conjunto de árvores é porque temos muitas árvores de decisão e fazer com que elas votem, isso torna seu algoritmo geral menos sensível ao que qualquer árvore pode estar fazendo, porque obtém apenas um voto em três ou um voto em muitos, muitos votos diferentes e torna seu algoritmo geral mais robusto. Mas como você cria todas essas diferentes árvores de decisão plausíveis, mas talvez ligeiramente diferentes, para que eles votem? No próximo vídeo, falaremos sobre uma técnica estatística chamada amostragem com substituição e essa se tornará uma técnica fundamental que usaremos no vídeo a seguir para construir esse conjunto de árvores. Vamos ao próximo vídeo para falar sobre amostragem com substituição.
(Obrigatória)
pt-BR
​


