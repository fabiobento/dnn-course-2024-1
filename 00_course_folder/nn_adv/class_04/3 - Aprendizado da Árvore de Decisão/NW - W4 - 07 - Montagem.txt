Os critérios de ganho de informações permitem que você decida como escolher um recurso para dividir um único nó. Vamos pegar isso e usá-lo em vários lugares por meio de uma árvore de decisão para descobrir como criar uma grande árvore de decisão com vários nós. Aqui está o processo geral de construção de uma árvore de decisão. Comece com todos os exemplos de treinamento no nó raiz da árvore e calcule o ganho de informações para todos os recursos possíveis e escolha o recurso a ser dividido, que oferece o maior ganho de informações. Depois de escolher esse recurso, você dividiria o conjunto de dados em dois subconjuntos de acordo com o recurso selecionado, criaria ramificações esquerda e direita da árvore e enviaria os exemplos de treinamento para a ramificação esquerda ou direita, dependendo do valor desse recurso para esse exemplo. Isso permite que você tenha feito uma divisão no nó raiz. Depois disso, você continuará repetindo o processo de divisão no galho esquerdo da árvore, no galho direito da árvore e assim por diante. Continue fazendo isso até que os critérios de parada sejam atendidos. Onde o critério de interrupção pode ser: quando um nó é 100% uma única cláusula, alguém atingiu a entropia zero, ou quando uma divisão adicional de um nó fará com que a árvore exceda a profundidade máxima que você definiu, ou se as informações obtidas de divisões adicionais forem menores que o limite, ou se o número de exemplos em um nó estiver abaixo de um limite. Você continuará repetindo o processo de divisão até que os critérios de parada que você escolheu, que podem ser um ou mais desses critérios, sejam atendidos. Vejamos uma ilustração de como esse processo funcionará. Começamos todos os exemplos nos nós raiz e, com base no ganho de informações de computação para todos os três recursos, decidimos que o formato de orelha é o melhor recurso para dividir. Com base nisso, criamos uma sub-ramificação esquerda e direita e enviamos os subconjuntos dos dados com orelha pontiaguda versus orelha flexível para as sub-ramificações esquerda e direita. Deixe-me abordar o nó raiz e o sub-ramo direito e focar apenas no sub-ramo esquerdo, onde temos esses cinco exemplos. Vamos ver se o critério de divisão é continuar dividindo até que tudo no nó pertença a uma única classe, portanto, todos cancelam todos os nós. Examinaremos esse nó e veremos se ele atende aos critérios de divisão, e isso não acontece porque há uma mistura de cães e gatos aqui. A próxima etapa é escolher um recurso para dividir. Em seguida, examinamos os recursos um de cada vez e calculamos o ganho de informações de cada um desses recursos como se esse nó fosse o novo nó raiz de uma árvore de decisão que foi treinada usando apenas cinco exemplos de treinamento mostrados aqui. Calcularíamos o ganho de informação na divisão no recurso bigodes, o ganho de informação na divisão no recurso em forma de V. Acontece que o ganho de informação para dividir o formato da orelha será zero porque todas elas têm o mesmo formato de orelha pontual. Entre os bigodes e a forma de V, a forma de V acaba tendo o maior ganho de informação. Vamos nos dividir em forma de V e isso nos permite construir sub-ramificações esquerda e direita da seguinte maneira. Para o sub-ramo esquerdo, verificamos os critérios para saber se devemos ou não parar de nos dividir e temos todos os gatos aqui. Os critérios de parada são atendidos e criamos um nó foliar que faz uma previsão do gato. Para o sub-ramo direito, descobrimos que são todos cães. Também pararemos de nos dividir, pois atendemos aos critérios de divisão e colocamos um nó foliar lá, que prediz que não é gato. Tendo construído essa subárvore esquerda, agora podemos voltar nossa atenção para a construção da subárvore direita. Deixe-me agora cobrir novamente o nódulo raiz e toda a subárvore esquerda. Para criar a subárvore certa, temos esses cinco exemplos aqui. Novamente, a primeira coisa que fazemos é verificar se os critérios para interromper a divisão foram atendidos, se seus critérios foram atendidos ou não. Todos os exemplos são uma única cláusula, não atendemos a esses critérios. Também decidiremos continuar nos dividindo neste sub-ramo direito. Na verdade, o procedimento para criar a sub-ramificação correta será muito parecido com se você estivesse treinando um algoritmo de aprendizado de árvore de decisão do zero, em que o conjunto de dados que você tem inclui apenas esses cinco exemplos de treinamento. Novamente, calculando o ganho de informações para todos os recursos possíveis de divisão, você descobre que o recurso bigodes usa o maior ganho de informação. Divida esse conjunto de cinco exemplos de acordo com a presença ou ausência de bigodes. Verifique se os critérios para interromper a divisão são atendidos nas sub-ramificações esquerda e direita aqui e decida se sim. Você acaba com nós de folhas que predizem gatos e cachorros. Esse é o processo geral de construção da árvore de decisão. Observe que há aspectos interessantes do que fizemos, ou seja, depois de decidirmos o que dividir no nó raiz, a forma como construímos a subárvore esquerda foi construindo uma árvore de decisão com base em um subconjunto de cinco exemplos. A forma como construímos a subárvore correta foi, novamente, construindo uma árvore de decisão com base em um subconjunto de cinco exemplos. Em ciência da computação, esse é um exemplo de algoritmo recursivo. Tudo isso significa que a maneira de construir uma árvore de decisão na raiz é construindo outras árvores de decisão menores nas sub-ramificações esquerda e direita. A recursão na ciência da computação se refere à escrita de código que chama a si mesmo. A forma como isso surge na construção de uma árvore de decisão é que você constrói a árvore de decisão geral construindo árvores de subdecisão menores e , em seguida, juntando-as todas. É por isso que, se você observar implementações de software de árvores de decisão, às vezes verá referências a um algoritmo recursivo. Mas se você acha que não entendeu completamente esse conceito de algoritmos recursivos, não se preocupe com isso. Você ainda poderá concluir totalmente as tarefas desta semana, bem como usar bibliotecas para fazer com que as árvores de decisão funcionem por si mesmo. Mas se você está implementando um algoritmo de árvore de decisão do zero , um algoritmo recursivo acaba sendo uma das etapas que você teria que implementar. A propósito, você pode estar se perguntando como escolher o parâmetro de profundidade máxima. Há muitas opções possíveis, mas algumas das bibliotecas de código aberto terão boas opções padrão que você pode usar. Uma intuição é que quanto maior a profundidade máxima, maior a árvore de decisão que você está disposto a construir. Isso é um pouco como ajustar um polinômio de maior grau ou treinar uma rede neural maior. Isso permite que a árvore de decisão aprenda um modelo mais complexo, mas também aumenta o risco de sobreajuste se isso ajustar uma função muito complexa aos seus dados. Em teoria, você poderia usar a validação cruzada para escolher parâmetros como a profundidade máxima, onde você experimenta valores diferentes da profundidade máxima e escolhe o que funciona melhor no conjunto de validação cruzada. Embora, na prática, as bibliotecas de código aberto tenham maneiras ainda melhores de escolher esse parâmetro para você. Ou outro critério que você pode usar para decidir quando parar de dividir é se as informações obtidas de uma fenda adicional forem menores que um determinado limite. Se algum recurso for isolado e atingir apenas uma pequena redução na entropia ou um ganho de informação muito pequeno, você também pode decidir não se preocupar. Finalmente, você também pode decidir começar a dividir quando o número de exemplos no nó estiver abaixo de um determinado limite. Esse é o processo de construção de uma árvore de decisão. Agora que você aprendeu a árvore de decisão, se quiser fazer uma previsão, siga o procedimento que viu no primeiro vídeo desta semana, onde você pega um novo exemplo, digamos, um exemplo de teste, e inicia uma rota e continua seguindo as decisões até chegar à nota, que então faz a previsão. Agora que você conhece o algoritmo básico de aprendizado da árvore de decisão, nos próximos vídeos, eu gostaria de abordar mais alguns refinamentos desse algoritmo. Até agora, usamos apenas recursos para assumir dois valores possíveis. Mas às vezes você tem um recurso que assume valores categóricos ou discretos, mas talvez mais de dois valores. Vamos dar uma olhada no próximo vídeo sobre como lidar com esse caso.
(Obrigatória)
pt-BR
​

