Até agora, falamos apenas sobre árvores de decisão como algoritmos de classificação. Neste vídeo opcional, generalizaremos as árvores de decisão para serem algoritmos de regressão para que possamos prever um número. Vamos dar uma olhada. O exemplo que vou usar para este vídeo será usar os três recursos que tínhamos anteriormente, ou seja, esses recursos X, para prever o peso do animal, Y. Então, só para ficar claro, o peso aqui, ao contrário do vídeo anterior, não é mais um recurso de entrada. Em vez disso, essa é a saída alvo, Y, que queremos prever em vez de tentar prever se um animal é ou não um gato. Esse é um problema de regressão porque queremos prever um número, Y. Vamos ver como será a aparência de uma árvore de regressão. Aqui, eu já construí uma árvore para esse problema de regressão em que o nó raiz se divide no formato da orelha e, em seguida, a subárvore esquerda e direita se divide no formato da face e também no formato da face aqui à direita. E não há nada de errado com uma árvore de decisão que opta por se dividir na mesma característica nos ramos esquerdo e direito. Tudo bem se o algoritmo de divisão optar por fazer isso. Se durante o treinamento, você tivesse decidido por essas divisões, esse nó aqui embaixo teria esses quatro animais com pesos 7,2, 7,6 e 10,2. Este nodo teria um animal com peso 9,2 e assim por diante para os dois nós restantes.
Reproduza o vídeo começando em :1:43 e siga a transcrição1:43
Então, a última coisa que precisamos preencher para essa árvore de decisão é se houver um exemplo de teste que se resume a esse nó, quais são os pesos que devemos prever para um animal com orelhas pontudas e formato de rosto redondo? A árvore de decisão fará uma previsão com base na média dos pesos nos exemplos de treinamento aqui embaixo. E calculando a média desses quatro números, você obtém 8,35.
Reproduza o vídeo começando em :2:14 e siga a transcrição2:14
Se, por outro lado, um animal tiver orelhas pontudas e um formato de rosto não redondo, ele preverá 9,2 ou 9,2 libras, porque esse é o peso desse animal aqui embaixo. E da mesma forma, serão 17,70 e 9,90. Então, o que esse modelo fará é dar um novo exemplo de teste, seguir os nós de decisão como de costume até chegar a um nó foliar e, em seguida, prever esse valor no nó foliar que eu acabei de calcular tomando uma média dos pesos dos animais que durante o treinamento chegaram ao mesmo nó foliar.
Reproduza o vídeo começando em :2:56 e siga a transcrição2:56
Então, se você estivesse construindo uma árvore de decisão do zero usando esse conjunto de dados para prever o peso. A principal decisão, como você viu no início desta semana, será: como escolher em qual recurso dividir? Deixe-me ilustrar como tomar essa decisão com um exemplo. No nó da raiz, uma coisa que você pode fazer é dividir o formato da orelha e, se fizer isso, você acabará com os galhos esquerdo e direito da árvore com cinco animais à esquerda e à direita com os seguintes pesos.
Reproduza o vídeo começando em :3:32 e siga a transcrição3:32
Se você escolher o cuspo no formato do rosto, acabará com esses animais à esquerda e à direita com os pesos correspondentes que estão escritos abaixo. E se você optar por dividir os bigodes por estarem presentes ou ausentes, você acabará com isso. Então, a questão é: dadas essas três características possíveis de dividir no nó raiz, qual delas você deseja escolher que dê as melhores previsões para o peso do animal? Ao construir uma árvore de regressão, em vez de tentar reduzir a entropia, que era a medida de impureza que tínhamos para um problema de classificação, tentamos reduzir a variância do peso dos valores Y em cada um desses subconjuntos dos dados. Então, se você viu a noção de variantes em outros contextos, isso é ótimo. Essa é a noção matemática estatística das variantes que usaremos em um minuto. Mas se você ainda não viu como calcular a variância de um conjunto de números, não se preocupe. Tudo o que você precisa saber para este slide é que as variantes computam informalmente a variação de um conjunto de números. Então, para esse conjunto de números 7,2, 9,2 e assim por diante, até 10,2, acontece que a variância é 1,47, então não varia muito. Já aqui 8,8, 15, 11, 18 e 20, esses números vão de 8,8 até 20. E então a variância é muito maior, resulta na variância de 21,87. Então, a forma como avaliaremos a qualidade da divisão é computar a mesma de antes, W à esquerda e W à direita como a fração dos exemplos que foram para os ramos esquerdo e direito. E a variação média após a divisão será 5/10, que é W à esquerda vezes 1,47, que é a variância à esquerda e mais 5/10 vezes a variância à direita, que é 21,87.
Reproduza o vídeo começando em :5:48 e siga a transcrição5:48
Portanto, essa variação da média ponderada desempenha um papel muito semelhante à entropia média ponderada que usamos ao decidir qual divisão usar para um problema de classificação. E podemos então repetir esse cálculo para as outras opções possíveis de recursos a serem divididos. Aqui na árvore no meio, a variação desses números aqui acaba sendo 27,80. A variação aqui é 1,37. Então, com W à esquerda igual a sete décimos e W à direita como três décimos, e com esses valores, você pode calcular a variância ponderada da seguinte forma. Finalmente, no último exemplo, se você dividisse no recurso bigodes, essa é a variação à esquerda e à direita, há W à esquerda e W à direita. Então, o peso da variância é esse. Uma boa maneira de escolher uma divisão seria simplesmente escolher o valor da variância ponderada mais baixa. Da mesma forma que quando estamos calculando o ganho de informação, vou fazer apenas mais uma modificação nessa equação. Assim como no problema de classificação, não medimos apenas a entropia média ponderada , medimos a redução na entropia e isso foi ganho de informação. Para uma árvore de regressão, também mediremos de forma semelhante a redução na variância. Acontece que, se você observar todos os exemplos no conjunto de treinamento, todos os dez exemplos e calcular a variância de todos eles, a variância de todos os exemplos será 20,51. E esse é o mesmo valor para o nó raiz em todos eles, é claro, porque são os mesmos dez exemplos no nó raiz. Então, o que realmente calcularemos é a variância do nó raiz, que é 20,51 menos essa expressão aqui embaixo, que acaba sendo igual a 8,84. Então, no nó da raiz, a variância foi de 20,51 e, após a divisão no formato da orelha, a variação média ponderada nesses dois nós é 8,84 menor. Então, a redução na variância é 8,84. Da mesma forma, se você computar a expressão para redução na variância para este exemplo no meio, é 20,51 menos essa expressão que tínhamos antes, que acaba sendo igual a 0,64. Então, essa é uma redução muito pequena na variância. E para o recurso de bigodes, você acaba com isso, que é 6,22. Então, entre todos esses três exemplos, 8,84 oferece a maior redução na variância. Então, assim como anteriormente escolheríamos o recurso que oferece o maior ganho de informação para uma árvore de regressão, você escolherá o recurso que oferece a maior redução na variância, e é por isso que você escolhe o formato da orelha como o recurso a ser dividido. Depois de escolher as feições em forma de ano para cuspir, agora você tem dois subconjuntos de cinco exemplos nos ramos esquerdo e direito e, novamente, dizemos recursivamente, pegando esses cinco exemplos e criando uma nova árvore de decisão focando apenas nesses cinco exemplos, novamente, avaliando diferentes opções de recursos para dividir e escolhendo aquela que oferece a maior redução de variância. E da mesma forma à direita. E você continua se dividindo até atender aos critérios para não se dividir mais. E então é isso. Com essa técnica, você pode tomar a decisão de não apenas realizar problemas de classificação, mas também problemas de regressão. Até agora, falamos sobre como treinar uma única árvore de decisão. Acontece que se você treinar muitas árvores de decisão, chamamos isso de conjunto de árvores de decisão, você pode obter um resultado muito melhor. Vamos ver por que e como fazer isso no próximo vídeo.
(Obrigatória)
pt-BR
​


