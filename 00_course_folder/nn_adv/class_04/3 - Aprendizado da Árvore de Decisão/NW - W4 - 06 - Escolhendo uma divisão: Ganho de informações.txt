Ao criar uma árvore de decisão, a forma como decidiremos em qual recurso dividir em um nó será baseada na escolha de recurso que mais reduz a entropia. Reduz a entropia ou reduz a impureza ou maximiza a pureza. No aprendizado da árvore de decisão, a redução da entropia é chamada de ganho de informação. Vamos dar uma olhada, neste vídeo, em como calcular o ganho de informações e, portanto, escolher quais recursos usar para dividir em cada nó em uma árvore de decisão. Vamos usar o exemplo de decidir qual recurso usar no nó raiz da árvore de decisão que estávamos construindo agora para reconhecer gatos versus não gatos. Se tivéssemos nos dividido usando o recurso de formato de orelha no nó raiz, isso é o que teríamos obtido, cinco exemplos à esquerda e cinco à direita. À esquerda, teríamos quatro em cada cinco gatos, então P1 seria igual a 4/5 ou 0,8. À direita, um em cada cinco são gatos, então P1 é igual a 1/5 ou 0,2. Se você aplicar a fórmula de entropia do último vídeo a esse subconjunto esquerdo de dados e a esse subconjunto direito de dados, descobriremos que o grau de impureza à esquerda é entropia de 0,8, que é cerca de 0,72, e à direita, a entropia de 0,2 também é 0,72. Essa seria a entropia nos subramos esquerdo e direito se fôssemos dividir a característica do formato da orelha. Uma outra opção seria dividir o recurso de formato de rosto. Se tivéssemos feito isso, à esquerda, quatro dos sete exemplos seriam gatos, então P1 é 4/7 e à direita, 1/3 são gatos, então P1 à direita é 1/3. A entropia de 4/7 e a entropia de 1/3 são 0,99 e 0,92. Portanto, o grau de impureza nos nós esquerdo e direito parece muito maior, 0,99 e 0,92 em comparação com 0,72 e 0,72. Finalmente, a terceira opção possível de recurso a ser usada no nó raiz seria o recurso bigodes. Nesse caso, você divide com base na presença ou ausência dos bigodes. Nesse caso, P1 à esquerda é 3/4, P1 à direita é 2/6 e os valores de entropia são os seguintes. A principal pergunta que precisamos responder é, dadas essas três opções de um recurso para usar no nó raiz, qual delas achamos que funciona melhor? Acontece que, em vez de analisar esses números de entropia e compará-los, seria útil fazer uma média ponderada deles, e aqui está o que quero dizer. Se houver um nó com muitos exemplos com alta entropia, isso parece pior do que se houvesse um nó com apenas alguns exemplos com alta entropia. Porque a entropia, como medida de impureza, é pior se você tiver um conjunto de dados muito grande e impuro em comparação com apenas alguns exemplos e um galho da árvore que é muito impuro. A principal decisão é: dessas três opções possíveis de recursos para usar no nó raiz, qual delas queremos usar? Associados a cada uma dessas divisões estão dois números, a entropia no sub-ramo esquerdo e a entropia no sub-ramo direito. Para escolher entre eles, gostamos de combinar esses dois números em um único número. Então você pode escolher entre essas três opções, qual delas funciona melhor? A maneira como vamos combinar esses dois números é fazendo uma média ponderada. Porque a importância de ter baixa entropia, digamos, no sub-ramo esquerdo ou direito também depende de quantos exemplos foram para o sub-ramo esquerdo ou direito. Porque se houver muitos exemplos, digamos, na sub-ramificação esquerda, parece mais importante garantir que o valor de entropia da sub-ramificação esquerda seja baixo.
Reproduza o vídeo começando em :4:25 e siga a transcrição4:25
Neste exemplo que temos, cinco dos 10 exemplos foram para o sub-ramo esquerdo, então podemos calcular a média ponderada como 5/10 vezes a entropia de 0,8 e, em seguida, adicionar a isso 5/10 exemplos também foram para o sub-ramo direito, mais 5/10 vezes a entropia de 0,2. Agora, para este exemplo no meio, o sub-ramo esquerdo recebeu sete dos 10 exemplos. e então vamos calcular 7/10 vezes a entropia de 0,57 mais, o sub-ramo direito tinha três dos 10 exemplos, então mais 3/10 vezes a entropia de 0,3 de 1/3. Finalmente, à direita, calcularemos 4/10 vezes a entropia de 0,75 mais 6/10 vezes a entropia de 0,33. A maneira como escolheremos uma divisão é computando esses três números e escolhendo o que for mais baixo, porque isso nos dá as sub-ramificações esquerda e direita com a menor entropia média ponderada. Da forma como as árvores de decisão são construídas, na verdade faremos mais uma alteração nessas fórmulas para manter a convenção na construção da árvore de decisão, mas isso não mudará o resultado. Ou seja, em vez de calcular essa entropia média ponderada, vamos calcular a redução na entropia em comparação com se não tivéssemos nos dividido. Se formos para o nó raiz, lembre-se de que começamos com todos os 10 exemplos no nó raiz com cinco cães e gatos e, portanto, no nó raiz, tínhamos p_1 igual a 5/10 ou 0,5. A entropia dos nódulos radiculares, entropia de 0,5, era na verdade igual a 1. Era o máximo em pureza porque eram cinco gatos e cinco cães.
Reproduza o vídeo começando em :6:31 e siga a transcrição6:31
A fórmula que realmente usaremos para escolher uma divisão não é essa forma de entropia nas sub-ramificações esquerda e direita, mas sim a entropia no nó raiz, que é entropia de 0,5 e, em seguida, menos essa fórmula. Neste exemplo, se você calcular a matemática, o resultado será 0,28. Para o exemplo do formato da face, podemos calcular a entropia do nó raiz, entropia de 0,5 menos isso, que resulta em 0,03, e para bigodes, calcular isso, que resulta em 0,12. Esses números que acabamos de calcular, 0,28, 0,03 e 0,12, são chamados de ganho de informação, e o que ele mede é a redução na entropia que você obtém em sua árvore resultante de uma divisão. Como a entropia era originalmente uma no nó raiz e ao fazer a divisão, você acaba com um valor menor de entropia e a diferença entre esses dois valores é uma redução na entropia, que é 0,28 no caso de divisão no formato da orelha. Por que nos preocupamos em calcular a redução na entropia em vez de apenas a entropia nas sub-ramificações esquerda e direita? Acontece que um dos principais critérios para decidir quando não se preocupar em se dividir ainda mais é se a redução na entropia for muito pequena.
Reproduza o vídeo começando em :8:2 e siga a transcrição8:02
Nesse caso, você pode decidir que está apenas aumentando o tamanho da árvore desnecessariamente e correndo o risco de se sobrepor ao se dividir, e simplesmente decidir não se preocupar se a redução na entropia for muito pequena ou abaixo de um limite. Neste outro exemplo, cuspir no formato da orelha resulta na maior redução na entropia, 0,28 é maior que 0,03 ou 0,12 e, portanto, optaríamos por dividir a característica do formato da orelha no nó raiz. No próximo slide, vamos dar uma definição mais formal do ganho de informação. A propósito, uma notação adicional que apresentaremos também no próximo slide são esses números, 5/10 e 5/10. Vou chamar isso de w^left porque essa é a fração dos exemplos que foram para a ramificação esquerda, e vou chamar isso de w^right porque essa é a fração dos exemplos que foram para a ramificação direita. Já para este outro exemplo, w^left seria 7/10 e w^right seria 3/10. Vamos agora anotar a fórmula geral de como calcular o ganho de informação.
Reproduza o vídeo começando em :9:11 e siga a transcrição9:11
Usando o exemplo de divisão no recurso de formato de orelha, deixe-me definir p_1^left como igual à fração de exemplos na subárvore esquerda que têm um rótulo positivo, que são gatos. Neste exemplo, p_1^left será igual a 4/5. Além disso, deixe-me definir w^left como a fração de exemplos de todos os exemplos do nó raiz que foi para a sub-ramificação esquerda e, portanto, neste exemplo, w^left seria 5/10. Da mesma forma, vamos definir p_1^right como um de todos os exemplos na ramificação direita. A fração que são exemplos positivos e , portanto, um dos cinco exemplos sendo gatos, será 1/5 e, da mesma forma, w^right é 5/10 a fração dos exemplos que foram para o sub-ramo direito. Também vamos definir p_1^root como a fração de exemplos que são positivos no nó raiz. Nesse caso, isso seria 5/10 ou 0,5.
Reproduza o vídeo começando em :10:24 e siga a transcrição10:24
O ganho de informação é então definido como a entropia de p_1^root, então qual é a entropia no nó raiz, menos o cálculo de entropia ponderada que tínhamos no slide anterior, menos w^left, essas eram 5/10 no exemplo, vezes a entropia aplicada a p_1^left, que é entropia na sub-ramificação esquerda, mais w^right a fração de exemplos que foram para o ramo direito, multiplicado pela entropia de p_1^right. Com essa definição de entropia, você pode calcular o ganho de informação associado à escolha de qualquer recurso específico para dividir no nó. Então, dentre todos os futuros possíveis, você pode optar por dividir e escolher aquele que oferece o maior ganho de informação. Isso resultará, esperançosamente, no aumento da pureza dos subconjuntos de dados que você obtém nas sub-ramificações esquerda e direita da sua árvore de decisão e resultará na escolha de um recurso para dividir que aumente a pureza de seus subconjuntos de dados nas
Reproduza o vídeo começando em :11:30 e siga a transcrição11:30
sub-ramificações esquerda e direita da sua árvore de decisão. Agora que você sabe como calcular o ganho ou a redução de informações na entropia, você sabe como escolher um recurso para dividir em outro nó. Vamos reunir todas as coisas sobre as quais falamos no algoritmo geral para criar uma árvore de decisão com um conjunto de treinamento. Vamos ver isso no próximo vídeo.
