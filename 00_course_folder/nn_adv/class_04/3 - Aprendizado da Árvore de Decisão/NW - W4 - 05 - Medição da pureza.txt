Neste vídeo, veremos como medir a pureza de um conjunto de exemplos. Se os exemplos são todos gatos de uma única classe, então isso é muito puro, se nem todos são gatos que também são muito puros, mas se estiver em algum lugar intermediário, como você quantifica o quão puro é o conjunto de exemplos? Vamos dar uma olhada na definição de entropia, que é uma medida da impureza de um conjunto de dados. Dado um conjunto de seis exemplos como esse, temos três gatos e três cachorros, vamos definir p_1 como a fração dos exemplos que são gatos, ou seja, a fração dos exemplos com o rótulo um, é o que o subscrito indica. p_1 neste exemplo é igual a 3/6. Vamos medir a impureza de um conjunto de exemplos usando uma função chamada entropia, que se parece com isso.
Reproduza o vídeo começando em :1:3 e siga a transcrição1:03
A função de entropia é convencionalmente denotada como H maiúsculo desse número p_1 e a função se parece com essa curva aqui, onde o eixo horizontal é p_1, a fração de gatos na amostra e o eixo vertical é o valor da entropia. Neste exemplo, onde p_1 é 3/6 ou 0,5, o valor da entropia de p_1 seria igual a um. Você percebe que essa curva é mais alta quando seu conjunto de exemplos é 50-50, então ela é mais impura como uma impureza de um ou com uma entropia de um quando seu conjunto de exemplos é 50-50, enquanto que, em contraste, se seu conjunto de exemplos era só gatos ou não gatos, a entropia é zero. Vamos ver mais alguns exemplos para obter mais intuição sobre a entropia e como ela funciona. Aqui está um conjunto diferente de exemplos com cinco gatos e um cachorro, então p_1 é a fração dos exemplos positivos, uma fração dos exemplos rotulados como um é 5/6 e, portanto, p_1 é cerca de 0,83. Se você ler esse valor em cerca de 0,83, descobriremos que a entropia de p_1 é de cerca de 0,65. E aqui estou escrevendo apenas para dois dígitos significativos. Aqui está mais um exemplo. Esta amostra de seis imagens tem todos os gatos, então p_1 é seis de seis porque todos os seis são gatos e a entropia de p_1 é esse ponto aqui que é zero. Vemos que, à medida que você passa de 3/6 para seis em cada seis gatos, a impureza diminui de um para zero ou, em outras palavras, a pureza aumenta à medida que você passa de uma mistura de 50-50 de cães e gatos para todos os gatos. Vamos ver mais alguns exemplos. Aqui está outra amostra com dois gatos e quatro cães, então p_1 aqui é 2/6, que é 1/3, e se você ler a entropia em 0,33, resulta em cerca de 0,92. Na verdade, isso é bastante impuro e, em particular, esse conjunto é mais impuro do que esse conjunto porque está mais próximo de uma mistura de 50-50, e é por isso que a impureza aqui é 0,92 em vez de 0,65. Finalmente, um último exemplo, se tivermos um conjunto de todos os seis cães, então p_1 é igual a 0 e a entropia de p_1 é apenas esse número aqui embaixo que é igual a 0, então não há nenhuma impureza ou isso seria um conjunto completamente puro de todos, não gatos ou todos os cães. Agora, vamos ver a equação real da função de entropia H (p_1). Lembre-se de que p_1 é a fração de exemplos que são iguais a gatos, portanto, se você tiver uma amostra de 2/3 gatos, essa amostra deve ter 1/3 que não sejam gatos. Deixe-me definir p_0 como igual à fração de exemplos que não são gatos como sendo apenas igual a 1 menos p_1.
Reproduza o vídeo começando em :4:40 e siga a transcrição4:40
A função de entropia é então definida como negativa p_1log_2 (p_1) e, por convenção, ao calcular a entropia, levamos os logs para a base dois em vez de para a base e e, em seguida, menos p_0log_2 (p_0). Como alternativa, isso também é igual a menos p_1log_2 (p_1) menos 1 menos p_1 log_2 (1 menos p_1). Se você plotar essa função em um computador, descobrirá que será exatamente essa função à esquerda. Tomamos log_2 apenas para tornar o pico dessa curva igual a um, se tomarmos log_e ou a base dos logaritmos naturais, isso apenas dimensiona verticalmente essa função, e ainda funcionará, mas os números se tornam um pouco difíceis de interpretar porque o pico da função não é mais um bom número redondo como um.
Reproduza o vídeo começando em :5:49 e siga a transcrição5:49
Uma observação sobre o cálculo dessa função: se p_1 ou p_0 for igual a 0, uma expressão como essa se parecerá com 0log (0), e log (0) é tecnicamente indefinido, na verdade é infinito negativo. Mas, por convenção, para fins de computação da entropia, consideraremos 0log (0) igual a 0 e isso calculará corretamente a entropia como zero ou como um igual a zero. Se você está pensando que essa definição de entropia se parece um pouco com a definição de perda logística que aprendemos no último curso, na verdade existe uma justificativa matemática para explicar por que essas duas fórmulas parecem tão semelhantes. Mas você não precisa se preocupar com isso e não vamos falar sobre isso nesta aula. Mas aplicar essa fórmula para entropia deve funcionar muito bem quando você está construindo uma árvore de decisão. Resumindo, a função de entropia é uma medida da impureza de um conjunto de dados. Começa do zero, sobe até um e depois volta para zero em função da fração de exemplos positivos em sua amostra. Existem outras funções que se parecem com isso, elas vão de zero para um e depois voltam para baixo. Por exemplo, se você procurar em pacotes de código aberto, também poderá ouvir sobre algo chamado critério de Gini, que é outra função que se parece muito com a função de entropia e que também funcionará bem para criar árvores de decisão. Mas, por uma questão de simplicidade, nesses vídeos, vou me concentrar em usar os critérios de entropia, que geralmente funcionam bem para a maioria dos aplicativos. Agora que temos essa definição de entropia, no próximo vídeo vamos dar uma olhada em como você pode realmente usá-la para tomar decisões sobre qual recurso dividir nos nós de uma árvore de decisão.
(Obrigatória)
pt-BR
​

