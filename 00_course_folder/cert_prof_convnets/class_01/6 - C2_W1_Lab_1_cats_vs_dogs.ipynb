{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/cert_prof_convnets/class_01/6%20-%20C2_W1_Lab_1_cats_vs_dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adaptado de [Certificado Profissional Desenvolvedor do TensorFlow](https://www.coursera.org/professional-certificates/tensorflow-in-practice) de [Laurence Moroney](https://laurencemoroney.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHK6DyunSbs4"
   },
   "source": [
    "# Uso de imagens mais sofisticadas com redes neurais convolucionais\n",
    "\n",
    "Você viu na seção anterior como usar uma CNN para tornar mais eficiente o reconhecimento de imagens de cavalos e humanos geradas por computador.\n",
    "\n",
    "Nesta lição, você levará isso para o próximo nível: criar um modelo para classificar imagens reais de gatos e cachorros.\n",
    "\n",
    "Assim como o conjunto de dados de cavalos e humanos, as imagens do mundo real também têm diferentes formas, proporções, etc., e você precisará levar isso em consideração ao preparar os dados.\n",
    "\n",
    "Neste laboratório, você primeiro reverá como criar CNNs, preparará os dados com o `ImageDataGenerator` e examinará os resultados. Você seguirá estas etapas:\n",
    "\n",
    "1.   Explore os dados de exemplo de `Dogs vs. Cats`\n",
    "2.   Crie e treine uma rede neural para classificar os dois animais de estimação\n",
    "3.   Avalie a acurácia do treinamento e da validação\n",
    "\n",
    "Você usará os resultados obtidos aqui nos próximos laboratórios para aprimorá-los, principalmente para evitar o overfitting. Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY6KJV6z6l7_"
   },
   "source": [
    "## Baixar e inspecionar o conjunto de dados\n",
    "\n",
    "Você começará fazendo o download do conjunto de dados.\n",
    "\n",
    "Trata-se de um `.zip` com 2.000 imagens JPG de cães e gatos.\n",
    "\n",
    "É um subconjunto do conjunto de dados [\"Dogs vs. Cats\"] (https://www.kaggle.com/c/dogs-vs-cats/data) disponível no Kaggle, que contém 25.000 imagens.\n",
    "\n",
    "Você usará apenas 2.000 do conjunto de dados completo para reduzir o tempo de treinamento para fins educacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXZT2UsyIVe_"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9brUxyTpYZHy"
   },
   "source": [
    "Em seguida, você o extrairá para o diretório atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLy3pthUS0D2"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Descompacte o arquivo\n",
    "local_zip = './cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall()\n",
    "\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-qUPyfO7Qr8"
   },
   "source": [
    "O conteúdo do .zip é extraído para o diretório base `./cats_and_dogs_filtered`, que contém os subdiretórios `train` e `validation` para os conjuntos de dados de treinamento e validação (você pode ignorar o `vectorize.py` na saída da próxima célula). \n",
    "\n",
    "Lembre-se\n",
    "* **conjunto de treinamento** são os dados usados para informar ao modelo de rede neural que \"esta é a aparência de um gato\" e \"esta é a aparência de um cachorro\".\n",
    "*  **conjunto de validação** são imagens de gatos e cachorros que a rede neural não verá como parte do treinamento. Você pode usá-lo para testar se ele se sai bem ou mal ao avaliar se uma imagem contém um gato ou um cachorro. (Consulte o [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/validation/check-your-intuition) se quiser relembrar os conjuntos de treinamento, validação e teste).\n",
    "\n",
    "Esses subdiretórios, por sua vez, contêm subdiretórios `cats` e `dogs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp39PPeAETY8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'cats_and_dogs_filtered'\n",
    "\n",
    "print(\"Conteúdo do diretório base:\")\n",
    "print(os.listdir(base_dir))\n",
    "\n",
    "print(\"\\nConteúdo do diretório de treino:\")\n",
    "print(os.listdir(f'{base_dir}/train'))\n",
    "\n",
    "print(\"\\nConteúdo do diretório validação:\")\n",
    "print(os.listdir(f'{base_dir}/validation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOUQFqi9Gz-Q"
   },
   "source": [
    "Você pode atribuir cada um desses diretórios a uma variável para poder usá-la posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLZKVtE0dSfk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with training cat/dog pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Diretório com imagens de gatos/cães para validação\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuBYtA_Zd8_T"
   },
   "source": [
    "Agora veja como são os nomes dos arquivos nos diretórios `cats` e `dogs` `train` (as convenções de nomenclatura de arquivos são as mesmas no diretório `validation`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PIP1rkmeAYS"
   },
   "outputs": [],
   "source": [
    "train_cat_fnames = os.listdir( train_cats_dir )\n",
    "train_dog_fnames = os.listdir( train_dogs_dir )\n",
    "\n",
    "print(train_cat_fnames[:10])\n",
    "print(train_dog_fnames[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlqN5KbafhLI"
   },
   "source": [
    "Vamos descobrir o número total de imagens de cães e gatos nos diretórios `train` e `validation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4XHh2xSfgie"
   },
   "outputs": [],
   "source": [
    "print('total de imagens de gatos para treinamento :', len(os.listdir(      train_cats_dir ) ))\n",
    "print('total de imagens de cães para treinamento :', len(os.listdir(      train_dogs_dir ) ))\n",
    "\n",
    "print('total de imagens de gatos para validação :', len(os.listdir( validation_cats_dir ) ))\n",
    "print('total de imagens de cães para validação :', len(os.listdir( validation_dogs_dir ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3WZABE9eX-8"
   },
   "source": [
    "Para cães e gatos, você tem 1.000 imagens de treinamento e 500 imagens de validação.\n",
    "\n",
    "Agora, dê uma olhada em algumas imagens para ter uma noção melhor de como são os conjuntos de dados de cães e gatos. Primeiro, configure os parâmetros do `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2_Q0-_5UAv-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parâmetros para nosso gráfico; produziremos imagens em uma configuração 4x4\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "pic_index = 0 # Índice para iteração de imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTvHzGCxXkqp"
   },
   "source": [
    "Agora, exiba um lote de 8 imagens de gatos e 8 de cachorros. Você pode executar a célula novamente para ver um novo lote a cada vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wpr8GxjOU8in"
   },
   "outputs": [],
   "source": [
    "# Configure o matplotlib fig e dimensione-o para caber em fotos 4x4\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols*4, nrows*4)\n",
    "\n",
    "pic_index+=8\n",
    "\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[ pic_index-8:pic_index] \n",
    "               ]\n",
    "\n",
    "next_dog_pix = [os.path.join(train_dogs_dir, fname) \n",
    "                for fname in train_dog_fnames[ pic_index-8:pic_index]\n",
    "               ]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
    "  # Configure o subplot; os índices do subplot começam em 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Não mostre os eixos (ou linhas de grade)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQhDdYPEZvJt"
   },
   "source": [
    "Talvez não seja óbvio observar as imagens nessa grade, mas uma observação importante é que essas imagens têm todas as formas e tamanhos (assim como o conjunto de dados \"cavalos ou humanos\").\n",
    "\n",
    "Portanto, antes de treinar uma rede neural com elas, você precisará ajustar as imagens. Você verá isso nas próximas seções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oqBkNBJmtUv"
   },
   "source": [
    "## Criando um modelo pequeno a partir do zero para chegar a ~72% de acurácia\n",
    "\n",
    "Para treinar uma rede neural para lidar com as imagens, você precisará que elas tenham um tamanho uniforme. Você escolherá 150x150 pixels para isso, e verá o código que pré-processa as imagens para esse formato em breve. \n",
    "\n",
    "Você pode definir o modelo importando o Tensorflow e usando a API do Keras. Aqui está o código completo primeiro e a discussão vem depois. Isso é muito semelhante aos modelos que você criou no Curso 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJ0E-Y7VN3gW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Observe que a forma de entrada é o tamanho desejado da imagem 150x150 com 3 bytes de cor\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Achatar os resultados para alimentar um DNN\n",
    "    tf.keras.layers.Flatten(), \n",
    "    # Camada oculta de 512 neurônios\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    # Apenas 1 neurônio de saída. Ele conterá um valor de 0 a 1, sendo 0 para uma classe (\"gatos\") e 1 para a outra (\"cães\")\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FiCc93hN6xe"
   },
   "source": [
    "Você definiu uma camada `Sequential` como antes, adicionando algumas camadas convolucionais primeiro.\n",
    "\n",
    "Observe o parâmetro `input_shape` desta vez. Aqui é onde você coloca o tamanho `150x150` e `3` para a profundidade de cor, pois você tem imagens coloridas.\n",
    "\n",
    "Em seguida, você adiciona algumas camadas convolucionais e achata o resultado final para alimentar as camadas densamente conectadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gokG5HKpdtzm"
   },
   "source": [
    "Observe que, como você está enfrentando um problema de classificação de duas classes, ou seja, um *problema de classificação binária*, você terminará a rede com uma ativação [*sigmoid*](https://wikipedia.org/wiki/Sigmoid_function).\n",
    "\n",
    "A saída da rede será um único escalar entre `0` e `1`, codificando a probabilidade de que a imagem atual seja da classe `1` (em oposição à classe `0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9EaFDP5srBa"
   },
   "source": [
    "Você pode analisar a arquitetura da rede com o método `model.summary()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZKj8392nbgP"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmtkTn06pKxF"
   },
   "source": [
    "A coluna `output_shape` mostra como o tamanho do seu mapa de recursos evolui em cada camada sucessiva.\n",
    "\n",
    "A operação de convolução remove os pixels mais externos das dimensões originais, e cada camada de agrupamento os reduz pela metade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEkKSpZlvJXA"
   },
   "source": [
    "Em seguida, você configurará as especificações para o treinamento do modelo.\n",
    "\n",
    "Você treinará nosso modelo com a perda `binary_crossentropy`, porque se trata de um problema de classificação binária e sua ativação final é um sigmoide.\n",
    "\n",
    "Usaremos o otimizador `rmsprop` com uma taxa de aprendizado de `0,001`.\n",
    "\n",
    "Durante o treinamento, você deverá monitorar a acurácia da classificação.\n",
    "\n",
    "**NOTA**: Nesse caso, o uso do [algoritmo de otimização RMSprop](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) é preferível ao [stochastic gradient descent](https://developers.google.com/machine-learning/glossary/#SGD) (SGD), porque o RMSprop automatiza o ajuste da taxa de aprendizado para nós. (Outros otimizadores, como [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) e [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), também adaptam automaticamente a taxa de aprendizado durante o treinamento e funcionariam igualmente bem aqui)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DHWhFP_uhq3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn9m9D3UimHM"
   },
   "source": [
    "### Pré-processamento de dados\n",
    "\n",
    "A próxima etapa é configurar os geradores de dados que lerão as imagens nas pastas de origem, convertê-las em tensores `float32` e alimentá-las (com seus rótulos) ao modelo. Você terá um gerador para as imagens de treinamento e outro para as imagens de validação. Esses geradores produzirão lotes de imagens de tamanho 150x150 e seus rótulos (binários).\n",
    "\n",
    "Como você já deve saber, os dados que entram nas redes neurais geralmente devem ser normalizados de alguma forma para torná-los mais fáceis de serem processados pela rede (ou seja, não é comum alimentar uma ConvNet com pixels brutos). Nesse caso, você pré-processará as imagens normalizando os valores de pixel para que fiquem no intervalo `[0, 1]` (originalmente todos os valores estão no intervalo `[0, 255]`).\n",
    "\n",
    "No Keras, isso pode ser feito por meio da classe `keras.preprocessing.image.ImageDataGenerator` usando o parâmetro `rescale`. Essa classe `ImageDataGenerator` permite que você instancie geradores de lotes de imagens aumentadas (e seus rótulos) por meio de `.flow(data, labels)` ou `.flow_from_directory(directory)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClebU9NJg99G"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Todas as imagens serão redimensionadas em 1,/255.\n",
    "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "# --------------------\n",
    "# Imagens de treinamento de fluxo em lotes de 20 usando o gerador train_datagen\n",
    "# --------------------\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(150, 150))     \n",
    "# --------------------\n",
    "# Imagens de validação de fluxo em lotes de 20 usando o gerador test_datagen\n",
    "# --------------------\n",
    "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=20,\n",
    "                                                         class_mode  = 'binary',\n",
    "                                                         target_size = (150, 150))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu3Jdwkjwax4"
   },
   "source": [
    "### Treinamento\n",
    "Agora você treinará em todas as 2.000 imagens disponíveis, por 15 épocas, e monitorará a acurácia também nas 1.000 imagens do conjunto de validação.\n",
    "\n",
    "Observe os valores por época.\n",
    "\n",
    "Você verá 4 valores por época: perda, acurácia, perda de validação e acurácia de validação. \n",
    "\n",
    "A \"perda\" e a \"acurácia\" são ótimos indicadores do progresso no treinamento. A `perda` mede a previsão do modelo atual em relação aos rótulos conhecidos, calculando o resultado. A \"acurácia\", por outro lado, é a porção de suposições corretas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb1_lgobv81m"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=15,\n",
    "            validation_data=validation_generator,\n",
    "            verbose=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6vSHzPR2ghH"
   },
   "source": [
    "### Previsão do modelo\n",
    "\n",
    "Agora, dê uma olhada na execução de uma previsão usando o modelo. Esse código permitirá que você escolha um ou mais arquivos do seu sistema de arquivos, carregue-os e execute-os por meio do modelo, fornecendo uma indicação de que o objeto é um gato ou um cachorro.\n",
    "\n",
    "Observação:** Versões antigas do navegador Safari podem ter problemas de compatibilidade com o bloco de código abaixo. Se você receber um erro após selecionar as imagens a serem carregadas, considere a possibilidade de atualizar seu navegador para a versão mais recente. Se não for possível, comente ou ignore o bloco de código abaixo, descomente o próximo bloco de código e execute-o._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoWp43WxJDNT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from google.colab import files\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "uploaded=files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  # Previsão de imagens\n",
    "  path='/content/' + fn\n",
    "  img=load_img(path, target_size=(150, 150))\n",
    "  \n",
    "  x=img_to_array(img)\n",
    "  x /= 255\n",
    "  x=np.expand_dims(x, axis=0)\n",
    "  images = np.vstack([x])\n",
    "  \n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  \n",
    "  print(classes[0])\n",
    "  \n",
    "  if classes[0]>0.5:\n",
    "    print(fn + \" é um cachorro\")\n",
    "  else:\n",
    "    print(fn + \" é um gato\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8EHQyWGDvWz"
   },
   "source": [
    "### Visualizando representações intermediárias\n",
    "\n",
    "Para ter uma ideia do tipo de recursos que sua CNN aprendeu, uma coisa divertida a se fazer é visualizar como uma entrada é transformada à medida que passa pelo modelo.\n",
    "\n",
    "Você pode escolher uma imagem aleatória do conjunto de treinamento e, em seguida, gerar uma figura em que cada linha é a saída de uma camada e cada imagem na linha é um filtro específico nesse mapa de recursos de saída. Execute novamente essa célula para gerar representações intermediárias para uma variedade de imagens de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5tES8rXFjux"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# Definir um novo modelo que receberá uma imagem como entrada e produzirá\n",
    "# representações intermediárias para todas as camadas do modelo anterior\n",
    "successive_outputs = [layer.output for layer in model.layers]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Prepare uma imagem de entrada aleatória do conjunto de treinamento.\n",
    "cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\n",
    "dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n",
    "img_path = random.choice(cat_img_files + dog_img_files)\n",
    "img = load_img(img_path, target_size=(150, 150))  # esta é uma imagem PIL\n",
    "x   = img_to_array(img)                           # Matriz Numpy com formato (150, 150, 3)\n",
    "x   = x.reshape((1,) + x.shape)                   # Matriz Numpy com formato (1, 150, 150, 3)\n",
    "\n",
    "# Escala de 1/255\n",
    "x /= 255.0\n",
    "\n",
    "# Execute a imagem na rede, obtendo assim todas as\n",
    "# representações intermediárias para essa imagem.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# Esses são os nomes das camadas, para que você possa tê-las como parte de nosso gráfico\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "# Exibir as representações\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  \n",
    "  if len(feature_map.shape) == 4:\n",
    "    \n",
    "    #-------------------------------------------\n",
    "    # Faça isso apenas para as camadas conv / maxpool, não para as camadas totalmente conectadas\n",
    "    #-------------------------------------------\n",
    "    n_features = feature_map.shape[-1]  # Número de recursos no mapa de recursos\n",
    "    size       = feature_map.shape[ 1]  # Forma do mapa de recursos (1, tamanho, tamanho, n_recursos)\n",
    "    \n",
    "    # Colocar as imagens em mosaico nessa matriz\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    \n",
    "    #-------------------------------------------------\n",
    "    # Pós-processar o recurso para que fique visualmente agradável\n",
    "    #-------------------------------------------------\n",
    "    for i in range(n_features):\n",
    "      x  = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std ()\n",
    "      x *=  64\n",
    "      x += 128\n",
    "      x  = np.clip(x, 0, 255).astype('uint8')\n",
    "      display_grid[:, i * size : (i + 1) * size] = x # Coloque cada filtro em uma grade horizontal\n",
    "\n",
    "    #-----------------\n",
    "    # Exibir a grade\n",
    "    #-----------------\n",
    "    scale = 20. / n_features\n",
    "    plt.figure( figsize=(scale * n_features, scale) )\n",
    "    plt.title ( layer_name )\n",
    "    plt.grid  ( False )\n",
    "    plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuqK2arJL0wo"
   },
   "source": [
    "Você pode ver acima como os pixels destacados se transformam em representações cada vez mais abstratas e compactas, especialmente na grade inferior. \n",
    "\n",
    "As representações a jusante começam a destacar aquilo a que a rede presta atenção e mostram cada vez menos recursos sendo \"ativados\"; a maioria é definida como zero. Isso é chamado de _esparsidade de representação_ e é um recurso fundamental da aprendizagem profunda. Essas representações carregam cada vez menos informações sobre os pixels originais da imagem, mas informações cada vez mais refinadas sobre a classe da imagem. Você pode pensar em uma convnet (ou em uma rede profunda em geral) como um pipeline de destilação de informações em que cada camada filtra os recursos mais úteis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5Vulban4ZrD"
   },
   "source": [
    "### Avaliando a acurácia e a perda do modelo\n",
    "\n",
    "Você traçará a acurácia e a perda do treinamento/validação conforme coletadas durante o treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oj0gTIy4k60"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Recupere uma lista de resultados de lista em dados de treinamento e teste\n",
    "# conjuntos para cada época de treinamento\n",
    "#-----------------------------------------------------------\n",
    "acc      = history.history[     'accuracy' ]\n",
    "val_acc  = history.history[ 'val_accuracy' ]\n",
    "loss     = history.history[    'loss' ]\n",
    "val_loss = history.history['val_loss' ]\n",
    "\n",
    "epochs   = range(len(acc)) # Obter o número de épocas\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plotar a acurácia do treinamento e da validação por época\n",
    "#------------------------------------------------\n",
    "plt.plot  ( epochs,     acc )\n",
    "plt.plot  ( epochs, val_acc )\n",
    "plt.title ('Acurácia de treino e validação')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plotar a perda de treinamento e validação por época\n",
    "#------------------------------------------------\n",
    "plt.plot  ( epochs,     loss )\n",
    "plt.plot  ( epochs, val_loss )\n",
    "plt.title ('Perda de treino e validação'   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgmSjUST4qoS"
   },
   "source": [
    "Como você pode ver, o modelo está **sobreajustando** como se estivesse saindo de moda. A acurácia do treinamento (em azul) se aproxima de 100%, enquanto a acurácia da validação (em laranja) fica em 70%. A perda de validação atinge seu mínimo após apenas cinco épocas.\n",
    "\n",
    "Como você tem um número relativamente pequeno de exemplos de treinamento (2000), o ajuste excessivo deve ser a principal preocupação. O ajuste excessivo ocorre quando um modelo exposto a um número muito pequeno de exemplos aprende padrões que não se generalizam para novos dados, ou seja, quando o modelo começa a usar recursos irrelevantes para fazer previsões. Por exemplo, se você, como ser humano, vir apenas três imagens de pessoas que são lenhadores e três imagens de pessoas que são marinheiros e, entre elas, a única pessoa que usa um boné é um lenhador, você pode começar a pensar que usar um boné é um sinal de ser um lenhador e não um marinheiro. Assim, você seria um péssimo classificador de lenhador/marinheiro.\n",
    "\n",
    "O ajuste excessivo é o problema central do aprendizado de máquina: se estivermos ajustando os parâmetros do nosso modelo a um determinado conjunto de dados, como podemos ter certeza de que as representações aprendidas pelo modelo serão aplicáveis a dados que ele nunca viu antes? Como você evita aprender coisas que são específicas dos dados de treinamento?\n",
    "\n",
    "No próximo exercício, você verá maneiras de evitar o ajuste excessivo nesse modelo de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4IBgYCYooGD"
   },
   "source": [
    "## Limpar\n",
    "\n",
    "Antes de executar o próximo exercício, execute a seguinte célula para encerrar o kernel e liberar recursos de memória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "651IgjLyo-Jx"
   },
   "outputs": [],
   "source": [
    "import os, signal\n",
    "\n",
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C2_W1_Lab_1_cats_vs_dogs.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/adding_C2/C2/W1/ungraded_lab/C2_W1_Lab_1_cats_vs_dogs.ipynb",
     "timestamp": 1639176164507
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
