{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/cert_prof_convnets/class_04/7%20-%20C2_W4_Lab_1_multi_class_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adaptado de [Certificado Profissional Desenvolvedor do TensorFlow](https://www.coursera.org/professional-certificates/tensorflow-in-practice) de [Laurence Moroney](https://laurencemoroney.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UD-1_xY-h2u"
   },
   "source": [
    "# Classificador multiclasse\n",
    "\n",
    "Neste laboratório, você verá como criar um modelo para distinguir entre mais de duas classes.\n",
    "\n",
    "O código será semelhante ao que você usou anteriormente, com algumas alterações importantes no modelo e nos parâmetros de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvwVR5lHA8q_"
   },
   "source": [
    "## Faça o download e prepare o conjunto de dados\n",
    "\n",
    "Você usará o [_Rock-Paper-Scissors dataset_](https://www.tensorflow.org/datasets/catalog/rock_paper_scissors), uma galeria de imagens de mãos em poses de pedra, papel e tesoura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faça o download do conjunto de treino\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week4/rps.zip\n",
    "\n",
    "# Faça o download do conjunto de teste\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week4/rps-test-set.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnYP_HhYNVUK"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Extrair o arquivo\n",
    "local_zip = './rps.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('tmp/rps-train')\n",
    "zip_ref.close()\n",
    "\n",
    "local_zip = './rps-test-set.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('tmp/rps-test')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3vqjYrpB0hI"
   },
   "source": [
    "Como de costume, você atribuirá os nomes dos diretórios a variáveis e examinará os nomes dos arquivos como uma verificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrxdR83ANgjS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'tmp/rps-train/rps'\n",
    "\n",
    "rock_dir = os.path.join(base_dir, 'rock')\n",
    "paper_dir = os.path.join(base_dir, 'paper')\n",
    "scissors_dir = os.path.join(base_dir, 'scissors')\n",
    "\n",
    "print('Total de imagens de pedra:', len(os.listdir(rock_dir)))\n",
    "print('Total de imagens de papel:', len(os.listdir(paper_dir)))\n",
    "print('Total de imagens de tesoura:', len(os.listdir(scissors_dir)))\n",
    "\n",
    "rock_files = os.listdir(rock_dir)\n",
    "print(rock_files[:10])\n",
    "\n",
    "paper_files = os.listdir(paper_dir)\n",
    "print(paper_files[:10])\n",
    "\n",
    "scissors_files = os.listdir(scissors_dir)\n",
    "print(scissors_files[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t_CNSs6B-8y"
   },
   "source": [
    "Você também pode inspecionar algumas das imagens para ver a variedade das entradas do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jp9dLel9N9DS"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "pic_index = 2\n",
    "\n",
    "next_rock = [os.path.join(rock_dir, fname) \n",
    "                for fname in rock_files[pic_index-2:pic_index]]\n",
    "next_paper = [os.path.join(paper_dir, fname) \n",
    "                for fname in paper_files[pic_index-2:pic_index]]\n",
    "next_scissors = [os.path.join(scissors_dir, fname) \n",
    "                for fname in scissors_files[pic_index-2:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_rock+next_paper+next_scissors):\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "  plt.axis('Off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ufa0YF5oCpYw"
   },
   "source": [
    "## Construir o modelo\n",
    "\n",
    "Em seguida, você criará sua CNN.\n",
    "\n",
    "Você usará 4 camadas de convolução com filtros 64-64-128-128 e, em seguida, anexará uma camada `Dropout` para evitar o _overfitting_ e algumas camadas densas para a classificação.\n",
    "\n",
    "A camada de saída seria uma camada densa de 3 neurônios ativada por [Softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax).\n",
    "\n",
    "Você já viu antes quando estava treinando com o Fashion MNIST. Ela dimensiona sua saída para um conjunto de probabilidades que somam 1.\n",
    "\n",
    "A ordem dessa saída de 3 neurônios seria `papel`-`pedra`-`tesoura` (por exemplo, uma saída `[0,8 0,2 0,0]` significa que o modelo está prevendo 80% de probabilidade para papel e 20% de probabilidade para pedra.\n",
    "\n",
    "Você pode examinar a arquitetura com `model.summary()` abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgvGg2nsCj-0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Observe que a forma de entrada é o tamanho desejado da imagem 150x150 com 3 bytes de cor\n",
    "    # Essa é a primeira convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # A segunda convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A terceira convolução\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A quarta convolução\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Achatar os resultados para alimentar um DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Camada oculta de 512 neurônios\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Imprimir o resumo do modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P1iuHGiFrPV"
   },
   "source": [
    "Em seguida, você compilará o modelo.\n",
    "\n",
    "A principal mudança aqui é a função `loss`.\n",
    "\n",
    "Antes você estava usando `binary_crossentropy` para 2 classes, mas agora alterará para [categorical_crossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-function) para estendê-la a mais classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OskuZ2ThFqmg"
   },
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ps7kIRaFRIC"
   },
   "source": [
    "## Preparar o ImageDataGenerator\n",
    "\n",
    "Você preparará os geradores como antes.\n",
    "\n",
    "Você definirá o conjunto de treinamento para o aumento de dados para que ele possa imitar outras poses que o modelo precisa aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWTisYLQM1aM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "TRAINING_DIR = \"tmp/rps-train/rps\"\n",
    "training_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255,\n",
    "\t    rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "VALIDATION_DIR = \"tmp/rps-test/rps-test-set\"\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "\tTRAINING_DIR,\n",
    "\ttarget_size=(150,150),\n",
    "\tclass_mode='categorical',\n",
    "  batch_size=126\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "\tVALIDATION_DIR,\n",
    "\ttarget_size=(150,150),\n",
    "\tclass_mode='categorical',\n",
    "  batch_size=126\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Orf1QQlGGyOe"
   },
   "source": [
    "## Treine o modelo e avalie os resultados\n",
    "\n",
    "Você treinará por 25 épocas e avaliará os resultados em seguida.\n",
    "\n",
    "Observe como a acurácia do treinamento e da validação estão tendendo para cima.\n",
    "\n",
    "Essa é uma boa indicação de que o modelo não está se sobreajustando ao conjunto de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mHX5L7HFXQ7"
   },
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "history = model.fit(train_generator, epochs=25, steps_per_epoch=20, validation_data = validation_generator, verbose = 1, validation_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeTRVCr6aosw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Acurácia de Treino')\n",
    "plt.plot(epochs, val_acc, 'b', label='Acurácia de Validação')\n",
    "plt.title('Acurácia de Treino e Validação')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3ps8Q1tpYMG"
   },
   "source": [
    "# Previsão de modelo\n",
    "\n",
    "Você deve conseguir fazer upload de uma imagem aqui e classificá-la sem travamentos.\n",
    "\n",
    "**OBSERVAÇÃO: Esse bloco de código só funcionará no Google Colab.**\n",
    "\n",
    "Você pode usar suas próprias imagens ou usar as que estão disponíveis [aqui](https://storage.googleapis.com/tensorflow-1-public/course2/week4/rps-validation.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZABJp7T3VLCU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import files\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  path = fn\n",
    "  img = load_img(path, target_size=(150, 150))\n",
    "  x = img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(fn)\n",
    "  print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHRufhQYJJLU"
   },
   "source": [
    "## Encerramento\n",
    "\n",
    "Isso conclui este breve exercício sobre os classificadores multiclasse.\n",
    "\n",
    "Você viu que, com apenas algumas alterações, foi possível converter seus classificadores binários para prever mais classes.\n",
    "\n",
    "Você usou as mesmas técnicas de preparação de dados e modelos e conseguiu obter resultados relativamente bons em apenas 25 épocas. \n",
    "\n",
    "Para praticar, você pode procurar outros conjuntos de dados (por exemplo, [aqui](https://archive.ics.uci.edu/datasets)) com mais classes e revisar o modelo para acomodá-los.\n",
    "\n",
    "Tente fazer experimentos com diferentes camadas e técnicas de aumento de dados para melhorar suas métricas."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C2_W4_Lab_1_multi_class_classifier.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
