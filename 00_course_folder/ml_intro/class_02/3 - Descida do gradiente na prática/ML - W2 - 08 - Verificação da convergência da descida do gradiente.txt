Ao executar a descida de gradiente, como saber se ela está convergindo? Ou seja, se está ajudando você a encontrar parâmetros próximos ao mínimo global da função de custo. Ao aprender a reconhecer a
Reproduza o vídeo começando em ::15 e siga a transcrição0:15
aparência de uma implementação bem executada de gradiente descendente, também poderemos, em um vídeo posterior, escolher melhor uma boa taxa de aprendizado Alpha. Vamos dar uma olhada. Como lembrete, aqui está a regra de descida do gradiente. Uma das principais opções é a escolha da taxa de aprendizado Alpha. Aqui está algo que eu costumo fazer para garantir que o gradiente descendente esteja funcionando bem. Lembre-se de que o trabalho do gradiente descendente é encontrar os parâmetros w e b que, esperançosamente, minimizem a função de custo J. O que geralmente faço é traçar a função de custo J, que é calculada no conjunto de treinamento, e traçar o valor de J em cada iteração do gradiente descendente. Lembre-se de que cada iteração significa após cada atualização simultânea dos parâmetros w e b. Neste gráfico, o eixo horizontal é o número de iterações de gradiente descendente que você executou até agora. Você pode obter uma curva parecida com esta. Observe que o eixo horizontal é o número de iterações do gradiente descendente e não um parâmetro como w ou b. Isso difere dos gráficos anteriores que você viu em que o eixo vertical custava J e o eixo horizontal era um único parâmetro, como w ou b. Essa curva também é chamada de curva de aprendizado. Observe que existem alguns tipos diferentes de curvas de aprendizado usados no aprendizado de máquina, e você também verá alguns tipos mais adiante neste curso. Concretamente, se você observar aqui neste ponto da curva, isso significa que depois de executar a descida de gradiente para 100 iterações, ou seja, 100 atualizações simultâneas dos parâmetros, você aprendeu alguns valores para w e b. Se você calcular o custo J, w, b para esses valores de w e b, aqueles obtidos após 100 iterações, você obtém esse valor pelo custo J. Esse é esse ponto no eixo vertical. Esse ponto aqui corresponde ao valor de J para os parâmetros que você obteve após 200 iterações de gradiente descendente. A análise deste gráfico ajuda você a ver como seu custo J muda após cada iteração do gradiente descendente. Se o gradiente descendente estiver funcionando corretamente, o custo J deverá diminuir após cada iteração. Se J aumentar após uma iteração, isso significa que ou Alpha foi mal escolhido, e geralmente significa que Alpha é muito grande, ou pode haver um bug no código. Outra coisa útil que esta parte pode lhe dizer é que, se você observar essa curva, quando atingir talvez 300 iterações também, o custo J está se estabilizando e não está mais diminuindo muito. Em 400 iterações, parece que a curva se achatou. Isso significa que a descida do gradiente convergiu mais ou menos porque a curva não está mais diminuindo. Observando essa curva de aprendizado, você pode tentar identificar se o gradiente descendente está convergindo ou não. A propósito, o número de iterações que a descida de gradiente faz em uma conversão pode variar muito entre diferentes aplicativos. Em um aplicativo, ele pode convergir após apenas 30 iterações. Para um aplicativo diferente, podem ser necessárias 1.000 ou 100.000 iterações. Acontece que é muito difícil dizer com antecedência quantas iterações o gradiente descendente precisa convergir, e é por isso que você pode criar um gráfico como esse, uma curva de aprendizado. Tente descobrir quando você pode começar a treinar seu modelo específico. Outra forma de decidir quando seu modelo termina o treinamento é com um teste automático de convergência.
Reproduza o vídeo começando em :4:29 e siga a transcrição4:29
Aqui está o alfabeto grego épsilon. Vamos deixar épsilon ser uma variável representando um número pequeno, como 0,001 ou 10^-3. Se o custo J diminuir em menos do que esse número épsilon em uma iteração, provavelmente você está nessa parte achatada da curva que você vê à esquerda e pode declarar convergência. Lembre-se, convergência, espero que, no caso de você encontrar os parâmetros w e b próximos ao valor mínimo possível de J. Eu geralmente acho que escolher o limite certo épsilon é bem difícil. Na verdade, costumo olhar para gráficos como este à esquerda, em vez de confiar em testes de convergência automática. Observar a figura sólida pode dizer que darei um aviso prévio se talvez a descida do gradiente também não esteja funcionando corretamente. Agora você viu como deve ser a curva de aprendizado quando o gradiente descendente está funcionando bem. Vamos analisar esses insights e, no próximo vídeo, dar uma olhada em como escolher uma taxa de aprendizado apropriada.
