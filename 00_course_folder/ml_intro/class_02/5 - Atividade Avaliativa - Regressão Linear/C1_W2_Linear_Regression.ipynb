{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório Prático: Regressão Linear\n",
    "\n",
    "Bem-vindo ao seu primeiro laboratório prático! Neste laboratório, você implementará a regressão linear com uma variável para prever os lucros de uma franquia de restaurantes.\n",
    "\n",
    "\n",
    "# Tópicos\n",
    "- [ 1 - Pacotes ](#1)\n",
    "- [ 2 - Regressão Linear com uma variável ](#2)\n",
    "  - [ 2.1 Definição do problema](#2.1)\n",
    "  - [ 2.2  Conjunto de Dados](#2.2)\n",
    "  - [ 2.3 Revisão em Regressão Linear](#2.3)\n",
    "  - [ 2.4  Cálculo do custo](#2.4)\n",
    "    - [ Exercício 1](#ex01)\n",
    "  - [ 2.5 Gradiente Descendente ](#2.5)\n",
    "    - [ Exercício 2](#ex02)\n",
    "  - [ 2.6 Aprendendo os parâmetros utilizando gradiente descedente em lote (_batch gradient descent_) ](#2.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-04 14:05:52--  https://github.com/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/ml_intro/class_02/5%20-%20Atividade%20Avaliativa%20-%20Regress%C3%A3o%20Linear/lab_utils_ml_intro_assig_week_2.zip\n",
      "Resolving github.com (github.com)... 20.201.28.151\n",
      "Connecting to github.com (github.com)|20.201.28.151|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7797 (7.6K) [text/plain]\n",
      "Saving to: ‘lab_utils_ml_intro_assig_week_2.zip.1’\n",
      "\n",
      "lab_utils_ml_intro_ 100%[===================>]   7.61K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-03-04 14:05:53 (35.6 MB/s) - ‘lab_utils_ml_intro_assig_week_2.zip.1’ saved [7797/7797]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baixar arquivos adicionais para o laboratório\n",
    "!wget https://github.com/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/ml_intro/class_02/5%20-%20Atividade%20Avaliativa%20-%20Regress%C3%A3o%20Linear/lab_utils_ml_intro_assig_week_2.zip\n",
    "!unzip -n -q lab_utils_ml_intro_assig_week_2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 - Pacotes \n",
    "\n",
    "Primeiro, vamos executar a célula abaixo para importar todos os pacotes de que você precisará durante esta tarefa.\n",
    "- [numpy](www.numpy.org) é o pacote fundamental para trabalhar com matrizes em Python.\n",
    "- [matplotlib](http://matplotlib.org) é uma biblioteca famosa para plotar gráficos em Python.\n",
    "- ``utils.py`` contém funções auxiliares para este caderno. Não é necessário modificar o código desse arquivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import copy\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "# 2 - Regressão Linear com uma variável "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "## 2.1 -  Definição do problema\n",
    "Suponha que você seja o CEO de uma franquia de restaurantes e esteja considerando diferentes cidades para abrir um novo ponto de venda.\n",
    "- Você gostaria de expandir seus negócios para cidades que possam proporcionar lucros maiores ao seu restaurante.\n",
    "- A rede já tem restaurantes em várias cidades e você tem dados sobre os lucros e a população dessas cidades.\n",
    "- Você também tem dados sobre cidades que são candidatas a um novo restaurante. \n",
    "    - Para essas cidades, você tem a população da cidade.\n",
    "\n",
    "Você pode usar os dados para ajudá-lo a identificar quais cidades podem potencialmente proporcionar maiores lucros à sua empresa?\n",
    "<a name=\"2.2\"></a>\n",
    "## 2.2 - Conjunto de Dados\n",
    "\n",
    "Você começará carregando o conjunto de dados para essa tarefa.\n",
    "- A função`load_data()` abaixo carrega os dados nas variáveis `x_train` e `y_train`\n",
    "  - `x_train` é a população da cidade\n",
    "  - `y_train` é o lucro do restaurante naquela cidade. um valor negativo para o lucro indica um prejuízo.   \n",
    "  - `x_train` e `y_train` são _numpy arrays_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Carregar o conjunto de dados\n",
    "x_train, y_train = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize as variáveis\n",
    "Antes de iniciar qualquer tarefa em aprendizado de máquina, é útil se familiarizar mais com seu conjunto de dados.  \n",
    "- Um bom ponto de partida é simplesmente imprimir cada variável e ver o que ela contém.\n",
    "\n",
    "O código abaixo imprime a variável `x_train` e o tipo da variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de x_train: <class 'numpy.ndarray'>\n",
      "Os primeiros cinco elementos de x_train são:\n",
      " [6.1101 5.5277 8.5186 7.0032 5.8598]\n"
     ]
    }
   ],
   "source": [
    "# Imprimir x_train\n",
    "print(\"Tipo de x_train:\",type(x_train))\n",
    "print(\"Os primeiros cinco elementos de x_train são:\\n\", x_train[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_train` é um _numpy array_ que contém valores decimais que são todos maiores que zero.\n",
    "- Esses valores representam os valores das popuilações da cidade vezes 10.000\n",
    "- Por exemplo, 6,1101 significa que a população dessa cidade é de 61.101 pessoas\n",
    "  \n",
    "Agora, vamos imprimir `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de y_train: <class 'numpy.ndarray'>\n",
      "Os primeiros cinco elementos de y_train são:\n",
      " [17.592   9.1302 13.662  11.854   6.8233]\n"
     ]
    }
   ],
   "source": [
    "# imprimir y_train\n",
    "print(\"Tipo de y_train:\",type(y_train))\n",
    "print(\"Os primeiros cinco elementos de y_train são:\\n\", y_train[:5])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma, `y_train` é um _numpy array_ que tem valores decimais, alguns negativos, outros positivos.\n",
    "- Eles representam os lucros médios mensais de seu restaurante em cada cidade, em unidades de \\$10.000.\n",
    "  - Por exemplo, 17,592 representa \\$175.920 em lucros mensais médios para essa cidade.\n",
    "  -2,6807 representa -\\$26.807 de perda média mensal para aquela cidade.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifique as dimensões de suas variáveis\n",
    "\n",
    "Outra maneira útil de se familiarizar com seus dados é visualizar suas dimensões.\n",
    "\n",
    "Imprima a forma de `x_train` e `y_train` e veja quantos exemplos de treinamento você tem em seu conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O formato de x_train é: (97,)\n",
      "O formato de y_train é:  (97,)\n",
      "Quantidade de exemplos de treinamento (m): 97\n"
     ]
    }
   ],
   "source": [
    "print ('O formato de x_train é:', x_train.shape)\n",
    "print ('O formato de y_train é: ', y_train.shape)\n",
    "print ('Quantidade de exemplos de treinamento (m):', len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _numpy array_ de população da cidade tem 97 pontos de dados e a média mensal de lucros também tem 97 pontos de dados. Essas são matrizes NumPy 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize seus dados\n",
    "\n",
    "Muitas vezes é útil entender os dados visualizando-os. \n",
    "- Para esse conjunto de dados, você pode usar um gráfico de dispersão para visualizar os dados, já que ele tem apenas duas propriedades a serem plotadas (lucro e população). \n",
    "- Muitos outros problemas que você encontrará na vida real têm mais de duas propriedades (por exemplo, população, renda familiar média, lucros mensais, vendas mensais). Quando você tem mais de duas propriedades, ainda pode usar um gráfico de dispersão para ver a relação entre cada par de propriedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crie um gráfico de dispersão dos dados. Para alterar os marcadores para \"x\" vermelho,\n",
    "# usamos os parâmetros 'marker' e 'c'\n",
    "plt.scatter(x_train, y_train, marker='x', c='r') \n",
    "\n",
    "# Defina o título\n",
    "plt.title(\"Lucros vs. População por Cidade\")\n",
    "# Defina o rótulo do eixo y\n",
    "plt.ylabel('Lucro em $10,000')\n",
    "# Defina o rótulo do eixo x\n",
    "plt.xlabel('População da cidade em 10,000s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seu objetivo é criar um modelo de regressão linear para ajustar esses dados.\n",
    "- Com esse modelo, você pode inserir a população de uma nova cidade e fazer com que o modelo estime os possíveis lucros mensais do seu restaurante para essa cidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\"></a>\n",
    "## 2.3 - Revisão de Regressão Linear\n",
    "\n",
    "Nesse laboratório prático, você irá ajustar os parâmetros de regressão linear $(w,b)$ ao seu conjunto de dados.\n",
    "- A função do modelo para regressão linear, que é uma função que mapeia de `x` (população da cidade) para `y` (o lucro mensal de seu restaurante naquela cidade) é representada como\n",
    "    $$f_{w,b}(x) = wx + b$$\n",
    "    \n",
    "\n",
    "- Para treinar um modelo de regressão linear, você deseja encontrar os melhores parâmetros $(w,b)$ que se ajustem ao seu conjunto de dados.\n",
    "\n",
    "    - Para comparar como uma escolha de $(w,b)$ é melhor ou pior que outra escolha, você pode avaliá-la com uma função de custo J(w,b)$\n",
    "      - $J$ é uma função de $(w,b)$. Isso é, o valo do custo $J(w,b)$ depende do valor de $(w,b)$.\n",
    "  \n",
    "    - A escolha de $(w,b)$ que melhor se ajusta a seus dados é aquele que tem o menor custo $J(w,b)$.\n",
    "\n",
    "- Para encontrar os valores $(w,b)$ que obtêm o menor custo possível $J(w,b)$, você pode usar um método chamado gradiente descendente (**gradient descent**). \n",
    "  - A cada etapa da descida do gradiente, seus parâmetros $(w,b)$ se aproximam dos valores ideais que atingirão o menor custo $J(w,b)$.\n",
    "  \n",
    "\n",
    "- O modelo de regressão linear treinado pode, então, pegar o recurso de entrada $x$ (população da cidade) e gerar uma previsão $f_{w,b}(x)$ (lucro mensal previsto para um restaurante naquela cidade)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.4\"></a>\n",
    "## 2.4 - Calcular custo\n",
    "\n",
    "A descida gradiente envolve etapas repetidas para ajustar o valor do seu parâmetro $(w,b)$ para obter gradualmente um custo cada vez menor $J(w,b)$.\n",
    "- Em cada etapa da descida do gradiente, será útil monitorar seu progresso calculando o custo $J(w,b)$ à medida que $(w,b)$ for atualizado. \n",
    "- Nesta seção, você implementará uma função para calcular $J(w,b)$ de modo que possa verificar o progresso da implementação da descida gradiente.\n",
    "\n",
    "\n",
    "#### Função de custo\n",
    "Como você deve se lembrar da aula, para uma variável, a função de custo da regressão linear $J(w,b)$ é definida como\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ \n",
    "\n",
    "- Você pode pensar em $f_{w,b}(x^{(i)})$ como a previsão do modelo do lucro do seu restaurante, em oposição a $y^{(i)}$, que é o lucro real registrado nos dados.\n",
    "- $m$ é o número de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "\n",
    "#### Predição do modelo\n",
    "\n",
    "- Para regressão linear com uma variável, a previsão do modelo $f_{w,b}$ para um exemplo $x^{(i)}$ é representada como:\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b$$\n",
    "\n",
    "Esta é a equação de uma reta, com uma interceptação $b$ e uma inclinação $w$\n",
    "\n",
    "#### Implementação\n",
    "\n",
    "Preencha a função `compute_cost()` abaixo para calcular o custo $J(w,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercício 1\n",
    "\n",
    "Complete a função `compute_cost` abaixo:\n",
    "\n",
    "* Iterar sobre os exemplos de treinamento e, para cada exemplo, calcular:\n",
    "    * A previsão do modelo para esse exemplo\n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b \n",
    "    $$\n",
    "   \n",
    "    * O custo para esse exemplo $$cost^{(i)} =  (f_{wb} - y^{(i)})^2$$\n",
    "    \n",
    "\n",
    "* Retorne o custo total sobre todos os exemplos\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} cost^{(i)}$$\n",
    "  * Aqui, $m$ é o número de exemplos de treinamento e $\\sum$ é o operador de soma\n",
    "\n",
    "Se tiver dúvidas, você pode conferir as dicas apresentadas após a célula abaixo para ajudá-lo com a implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcula a função de custo para regressão linear.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Formato (m,) Entrada pra o modelo (População das cidades) \n",
    "        y (ndarray): Formato (m,) Valores alvo (Lucro real por cidade)\n",
    "        w, b (scalar): Parâmetros do modelo\n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): O custo de usar w,b como parâmetros para a regressão linear\n",
    "               para ajustar os pontos de dados em x e y\n",
    "    \"\"\"\n",
    "    # quantidade de exemplos de treino\n",
    "    m = x.shape[0] \n",
    "    \n",
    "    # você precisa retornar essa variável corretamente\n",
    "    total_cost = 0\n",
    "    \n",
    "    ### INICIE SEU CÓDIGO AQUI ###\n",
    "    \n",
    "    ### TERMINE SEU CÓDIGO AQUI ###\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click aqui para dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * Você pode representar um operador de soma, por exemplo: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ no seguinte código:\n",
    "    \n",
    "    ```python \n",
    "    h = 0\n",
    "    for i in range(m):\n",
    "        h = h + 2*i\n",
    "    ```\n",
    "  \n",
    "   * Nesse caso, você pode iterar sobre todos os exemplos em `x` usando um loop for e adicionar o `cost` de cada iteração a uma variável (`cost_sum`) inicializada fora do loop.\n",
    "\n",
    "   * Em seguida, você pode retornar o `total_cost` como `cost_sum` dividido por `2m`.\n",
    "   * Se você for novato em Python, verifique se o código está devidamente recuado com espaços ou tabulações consistentes.\n",
    "   \n",
    "   Caso contrário, ele poderá produzir uma saída diferente ou gerar um erro `IndentationError: unexpected indent`. Você pode consultar [esse tópico](https://community.deeplearning.ai/t/indentation-in-python-indentationerror-unexpected-indent/159398) in our community for details.\n",
    "   \n",
    "\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Click para mais dicas</b></font></summary>\n",
    "        \n",
    "    * Veja como você pode estruturar a implementação geral dessa função\n",
    "    \n",
    "    ```python \n",
    "    def compute_cost(x, y, w, b):\n",
    "        # quantidade de exemplos de treino\n",
    "        m = x.shape[0] \n",
    "    \n",
    "        # Você precisa retornar essa variável corretamente\n",
    "        total_cost = 0\n",
    "    \n",
    "        ### INICIE SEU CÓDIGO AQUI ###\n",
    "        # Variável para manter o controle da soma do custo de cada exemplo\n",
    "        cost_sum = 0\n",
    "    \n",
    "        # Fazer um loop sobre os exemplos de treinamento\n",
    "        for i in range(m):\n",
    "            # Seu código aqui para obter a previsão f_wb para o i-ésimo exemplo            \n",
    "            f_wb = \n",
    "            # Seu código aqui para obter o custo associado ao i-ésimo exemplo            \n",
    "            cost = \n",
    "        \n",
    "            # Adicionar à soma do custo de cada exemplo\n",
    "            cost_sum = cost_sum + cost \n",
    "\n",
    "        # Obtenha o custo total como a soma dividida por (2*m)\n",
    "        total_cost = (1 / (2 * m)) * cost_sum\n",
    "        ### TERMINE SEU CÓDIGO AQUI ###\n",
    "\n",
    "        return total_cost\n",
    "    ```\n",
    "    \n",
    "    * Se ainda estiver com dúvidas, você pode consultar as dicas apresentadas abaixo para descobrir como calcular `f_wb` e `cost`.\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate f_wb</b></font></summary>\n",
    "           &emsp; &emsp; Para os escalares  $a$, $b$ e $c$ (<code>x[i]</code>, <code>w</code> e <code>b</code> são todos escalares), você pode calcular a equação $h = ab + c$ no código como <code>h = a * b + c</code>\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; More hints to calculate f</b></font></summary>\n",
    "               &emsp; &emsp; You can compute f_wb as <code>f_wb = w * x[i] + b </code>\n",
    "           </details>\n",
    "    </details>\n",
    "\n",
    "     <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate cost</b></font></summary>\n",
    "          &emsp; &emsp; You can calculate the square of a variable z as z**2\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; More hints to calculate cost</b></font></summary>\n",
    "              &emsp; &emsp; You can compute cost as <code>cost = (f_wb - y[i]) ** 2</code>\n",
    "          </details>\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if your implementation was correct by running the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Compute cost with some initial values for paramaters w, b\n",
    "initial_w = 2\n",
    "initial_b = 1\n",
    "\n",
    "cost = compute_cost(x_train, y_train, initial_w, initial_b)\n",
    "print(type(cost))\n",
    "print(f'Cost at initial w: {cost:.3f}')\n",
    "\n",
    "# Public tests\n",
    "from public_tests import *\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Cost at initial w:<b> 75.203 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.5\"></a>\n",
    "## 2.5 - Gradient descent \n",
    "\n",
    "In this section, you will implement the gradient for parameters $w, b$ for linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the lecture videos, the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; & \\phantom {0000} w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{1}  \\; & \n",
    "\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $w, b$ are both updated simultaniously and where  \n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{w,b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$, is the target value\n",
    "\n",
    "\n",
    "You will implement a function called `compute_gradient` which calculates $\\frac{\\partial J(w)}{\\partial w}$, $\\frac{\\partial J(w)}{\\partial b}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex02\"></a>\n",
    "### Exercise 2\n",
    "\n",
    "Please complete the `compute_gradient` function to:\n",
    "\n",
    "* Iterate over the training examples, and for each example, compute:\n",
    "    * The prediction of the model for that example \n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b \n",
    "    $$\n",
    "   \n",
    "    * The gradient for the parameters $w, b$ from that example \n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)}) \n",
    "        $$\n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial w}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \n",
    "        $$\n",
    "    \n",
    "\n",
    "* Return the total gradient update from all the examples\n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial b}^{(i)}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial w}^{(i)} \n",
    "    $$\n",
    "  * Here, $m$ is the number of training examples and $\\sum$ is the summation operator\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_gradient\n",
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "      y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "      w, b (scalar): Parameters of the model  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "   * You can represent a summation operator eg: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ in code as follows:\n",
    "    \n",
    "   ```python \n",
    "    h = 0\n",
    "    for i in range(m):\n",
    "        h = h + 2*i\n",
    "   ```\n",
    "    \n",
    "   * In this case, you can iterate over all the examples in `x` using a for loop and for each example, keep adding the gradient from that example to the variables `dj_dw` and `dj_db` which are initialized outside the loop. \n",
    "\n",
    "   * Then, you can return `dj_dw` and `dj_db` both divided by `m`.    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Click for more hints</b></font></summary>\n",
    "        \n",
    "    * Here's how you can structure the overall implementation for this function\n",
    "    \n",
    "    ```python \n",
    "    def compute_gradient(x, y, w, b): \n",
    "        \"\"\"\n",
    "        Computes the gradient for linear regression \n",
    "        Args:\n",
    "          x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "          y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "          w, b (scalar): Parameters of the model  \n",
    "        Returns\n",
    "          dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "          dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "        \"\"\"\n",
    "    \n",
    "        # Number of training examples\n",
    "        m = x.shape[0]\n",
    "    \n",
    "        # You need to return the following variables correctly\n",
    "        dj_dw = 0\n",
    "        dj_db = 0\n",
    "    \n",
    "        ### START CODE HERE ### \n",
    "        # Loop over examples\n",
    "        for i in range(m):  \n",
    "            # Your code here to get prediction f_wb for the ith example\n",
    "            f_wb = \n",
    "            \n",
    "            # Your code here to get the gradient for w from the ith example \n",
    "            dj_dw_i = \n",
    "        \n",
    "            # Your code here to get the gradient for b from the ith example \n",
    "            dj_db_i = \n",
    "     \n",
    "            # Update dj_db : In Python, a += 1  is the same as a = a + 1\n",
    "            dj_db += dj_db_i\n",
    "        \n",
    "            # Update dj_dw\n",
    "            dj_dw += dj_dw_i\n",
    "    \n",
    "        # Divide both dj_dw and dj_db by m\n",
    "        dj_dw = dj_dw / m\n",
    "        dj_db = dj_db / m\n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        return dj_dw, dj_db\n",
    "    ```\n",
    "        \n",
    "    * If you're still stuck, you can check the hints presented below to figure out how to calculate `f_wb` and `cost`.\n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate f_wb</b></font></summary>\n",
    "           &emsp; &emsp; You did this in the previous exercise! For scalars $a$, $b$ and $c$ (<code>x[i]</code>, <code>w</code> and <code>b</code> are all scalars), you can calculate the equation $h = ab + c$ in code as <code>h = a * b + c</code>\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; More hints to calculate f</b></font></summary>\n",
    "               &emsp; &emsp; You can compute f_wb as <code>f_wb = w * x[i] + b </code>\n",
    "           </details>\n",
    "    </details>\n",
    "        \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate dj_dw_i</b></font></summary>\n",
    "           &emsp; &emsp; For scalars $a$, $b$ and $c$ (<code>f_wb</code>, <code>y[i]</code> and <code>x[i]</code> are all scalars), you can calculate the equation $h = (a - b)c$ in code as <code>h = (a-b)*c</code>\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; More hints to calculate f</b></font></summary>\n",
    "               &emsp; &emsp; You can compute dj_dw_i as <code>dj_dw_i = (f_wb - y[i]) * x[i] </code>\n",
    "           </details>\n",
    "    </details>\n",
    "        \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate dj_db_i</b></font></summary>\n",
    "             &emsp; &emsp; You can compute dj_db_i as <code> dj_db_i = f_wb - y[i] </code>\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to check your implementation of the `compute_gradient` function with two different initializations of the parameters $w$,$b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Compute and display gradient with w initialized to zeroes\n",
    "initial_w = 0\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)\n",
    "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)\n",
    "\n",
    "compute_gradient_test(compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm implemented above on our dataset.\n",
    "\n",
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Gradient at initial , b (zeros)<b></td>\n",
    "    <td> -65.32884975 -5.83913505154639</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Compute and display cost and gradient with non-zero w\n",
    "test_w = 0.2\n",
    "test_b = 0.2\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Gradient at test w, b:', tmp_dj_dw, tmp_dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Gradient at test w<b></td>\n",
    "    <td> -47.41610118 -4.007175051546391</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.6\"></a>\n",
    "### 2.6 Learning parameters using batch gradient descent \n",
    "\n",
    "You will now find the optimal parameters of a linear regression model by using batch gradient descent. Recall batch refers to running all the examples in one iteration.\n",
    "- You don't need to implement anything for this part. Simply run the cells below. \n",
    "\n",
    "- A good way to verify that gradient descent is working correctly is to look\n",
    "at the value of $J(w,b)$ and check that it is decreasing with each step. \n",
    "\n",
    "- Assuming you have implemented the gradient and computed the cost correctly and you have an appropriate value for the learning rate alpha, $J(w,b)$ should never increase and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x :    (ndarray): Shape (m,)\n",
    "      y :    (ndarray): Shape (m,)\n",
    "      w_in, b_in : (scalar) Initial values of parameters of the model\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha : (float) Learning rate\n",
    "      num_iters : (int) number of iterations to run gradient descent\n",
    "    Returns\n",
    "      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(x)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration — primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b )  \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(x, y, w, b)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            w_history.append(w)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm above to learn the parameters for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "initial_w = 0.\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "w,b,_,_ = gradient_descent(x_train ,y_train, initial_w, initial_b, \n",
    "                     compute_cost, compute_gradient, alpha, iterations)\n",
    "print(\"w,b found by gradient descent:\", w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b> w, b found by gradient descent<b></td>\n",
    "    <td> 1.16636235 -3.63029143940436</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the final parameters from gradient descent to plot the linear fit. \n",
    "\n",
    "Recall that we can get the prediction for a single example $f(x^{(i)})= wx^{(i)}+b$. \n",
    "\n",
    "To calculate the predictions on the entire dataset, we can loop through all the training examples and calculate the prediction for each example. This is shown in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "m = x_train.shape[0]\n",
    "predicted = np.zeros(m)\n",
    "\n",
    "for i in range(m):\n",
    "    predicted[i] = w * x_train[i] + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot the predicted values to see the linear fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plot the linear fit\n",
    "plt.plot(x_train, predicted, c = \"b\")\n",
    "\n",
    "# Create a scatter plot of the data. \n",
    "plt.scatter(x_train, y_train, marker='x', c='r') \n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Profits vs. Population per city\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Profit in $10,000')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final values of $w,b$ can also be used to make predictions on profits. Let's predict what the profit would be in areas of 35,000 and 70,000 people. \n",
    "\n",
    "- The model takes in population of a city in 10,000s as input. \n",
    "\n",
    "- Therefore, 35,000 people can be translated into an input to the model as `np.array([3.5])`\n",
    "\n",
    "- Similarly, 70,000 people can be translated into an input to the model as `np.array([7.])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "predict1 = 3.5 * w + b\n",
    "print('For population = 35,000, we predict a profit of $%.2f' % (predict1*10000))\n",
    "\n",
    "predict2 = 7.0 * w + b\n",
    "print('For population = 70,000, we predict a profit of $%.2f' % (predict2*10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b> For population = 35,000, we predict a profit of<b></td>\n",
    "    <td> $4519.77 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> <b> For population = 70,000, we predict a profit of<b></td>\n",
    "    <td> $45342.45 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations on completing this practice lab on linear regression! Next week, you will create models to solve a different type of problem: classification. See you there!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>Please click here if you want to experiment with any of the non-graded code.</b></font></summary>\n",
    "    <p><i><b>Important Note: Please only do this when you've already passed the assignment to avoid problems with the autograder.</b></i>\n",
    "    <ol>\n",
    "        <li> On the notebook’s menu, click “View” > “Cell Toolbar” > “Edit Metadata”</li>\n",
    "        <li> Hit the “Edit Metadata” button next to the code cell which you want to lock/unlock</li>\n",
    "        <li> Set the attribute value for “editable” to:\n",
    "            <ul>\n",
    "                <li> “true” if you want to unlock it </li>\n",
    "                <li> “false” if you want to lock it </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> On the notebook’s menu, click “View” > “Cell Toolbar” > “None” </li>\n",
    "    </ol>\n",
    "    <p> Here's a short demo of how to do the steps above: \n",
    "        <br>\n",
    "        <img src=\"https://drive.google.com/uc?export=view&id=14Xy_Mb17CZVgzVAgq7NCjMVBvSae3xO1\" align=\"center\" alt=\"unlock_cells.gif\">\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
