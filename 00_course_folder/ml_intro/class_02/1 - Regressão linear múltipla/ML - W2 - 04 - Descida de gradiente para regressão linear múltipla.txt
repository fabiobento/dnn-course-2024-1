Você aprendeu sobre descidas de gradiente, regressão linear múltipla e também vetorização.
Reproduza o vídeo começando em ::9 e siga a transcrição0:09
Vamos juntar tudo para implementar o gradiente descendente para regressão linear múltipla com vetorização. Isso seria legal. Vamos analisar rapidamente a aparência da regressão linear múltipla. Usando nossa notação anterior, vamos ver como você pode escrevê-la de forma mais sucinta usando a notação vetorial. Temos parâmetros w_1 a w_n e b. Mas em vez de pensar em w_1 a w_n como números separados, ou seja, parâmetros separados, vamos começar a reunir todos os w's em um vetor w para que agora w seja um vetor de comprimento n. Vamos pensar nos parâmetros desse modelo como um vetor w, bem como b, onde b ainda é um número igual ao anterior. Por outro lado, antes precisávamos encontrar uma regressão linear múltipla como essa, agora usando notação vetorial, podemos escrever o modelo como f_w, b de x é igual ao vetor w dot product com o vetor x mais b. Lembre-se de que esse ponto aqui significa.produto. Nossa função de custo pode ser definida como J de w_1 até w_n, b. Mas em vez de apenas pensar em J como uma função desses e de diferentes parâmetros w_j e b, vamos escrever J como uma função do vetor de parâmetros w e do número b. Esse w_1 a w_n é substituído por esse vetor W e J agora pega essa entrada do vetor w e um número b e retorna um número. Aqui está a aparência do gradiente descendente.
Reproduza o vídeo começando em :1:56 e siga a transcrição1:56
Vamos atualizar repetidamente cada parâmetro w_j para ser w_j menos Alpha vezes a derivada do custo J, onde J tem os parâmetros w_1 a w_n e b. Mais uma vez, escrevemos isso como J do vetor w e número b. Vamos ver como isso acontece quando você implementa gradiente descendente e, em particular, vamos dar uma olhada no termo derivado. Veremos que o gradiente descendente se torna um pouco diferente com vários recursos em comparação com apenas um recurso. Aqui está o que tínhamos quando tínhamos um gradiente descendente com um recurso. Nós tínhamos uma regra de atualização para w e uma regra de atualização separada para b. Espero que elas pareçam familiares para você. Esse termo aqui é a derivada da função de custo J em relação ao parâmetro w. Da mesma forma, temos uma regra de atualização para o parâmetro b, com regressão univariada, tínhamos apenas uma característica. Chamamos esse recurso de xi sem nenhum subscrito. Agora, aqui está uma nova notação para onde temos n características, onde n são duas ou mais. Recebemos essa regra de atualização para gradiente descendente. Atualize w_1 para w_1 menos Alpha vezes essa expressão aqui e essa fórmula é, na verdade, a derivada do custo J em relação a w_1. A fórmula para a derivada de J em relação a w_1 à direita é muito semelhante ao caso de uma característica à esquerda. O termo de erro ainda usa uma previsão f de x menos o alvo y. Uma diferença é que w e x agora são vetores e, assim como w à esquerda agora se tornou w_1 aqui à direita, xi aqui à esquerda agora é xi _1 aqui à direita e isso é apenas para J igual a 1.
Reproduza o vídeo começando em :4:7 e siga a transcrição4:07
Para regressão linear múltipla, temos J variando de 1 a n e, portanto, atualizaremos os parâmetros w_1, w_2 até w_n e, como antes, atualizaremos b. Se você implementar isso, obterá gradiente descendente para regressão múltipla. Isso é tudo para gradiente descendente para regressão múltipla. Antes de prosseguir com este vídeo, quero fazer um aparte rápido ou uma nota lateral rápida sobre uma forma alternativa de encontrar w e b para regressão linear. Esse método é chamado de equação normal. Embora a descida de gradiente seja um ótimo método para minimizar a função de custo J para encontrar w e b, há um outro algoritmo que funciona apenas para regressão linear e praticamente nenhum dos outros algoritmos que você vê nesta especialização para resolver w e b e esse outro método não precisa de um algoritmo de descida de gradiente iterativo. Chamado de método de equação normal, é possível usar uma biblioteca avançada de álgebra linear para resolver apenas w e b em um objetivo sem iterações. Algumas desvantagens do método de equação normal são: primeiro, ao contrário do gradiente descendente, isso não é generalizado para outros algoritmos de aprendizado, como o algoritmo de regressão logística que você aprenderá na próxima semana ou as redes neurais ou outros algoritmos que você verá posteriormente nesta especialização. O método de equação normal também é bastante lento se o número de características for tão grande. Quase nenhum profissional de aprendizado de máquina deve implementar o método de equação normal sozinho, mas se você estiver usando uma biblioteca madura de aprendizado de máquina e chamar a regressão linear, há uma chance de que, no back-end, ele o use para resolver w e b. Se você estiver na entrevista de emprego e ouvir o termo equação normal, é a isso que isso se refere. Não se preocupe com os detalhes de como a equação normal funciona. Esteja ciente de que algumas bibliotecas de aprendizado de máquina podem usar esse método complicado no back-end para resolver w e b. Mas para a maioria dos algoritmos de aprendizado, incluindo a forma como você mesmo implementa a regressão linear, as descidas em gradiente oferecem uma maneira melhor de realizar o trabalho. No laboratório opcional que segue este vídeo, você verá como definir uma codificação de modelo de regressão múltipla e também como calcular a previsão f de x. Você também verá como calcular o custo e implementar o gradiente descendente para um modelo de regressão linear múltipla. Isso usará a biblioteca NumPy do Python. Se algum código parecer muito novo, tudo bem, mas você também deve dar uma olhada no laboratório opcional anterior que apresenta o NumPy e a vetorização para uma atualização das funções do NumPy e como implementá-las no código. É isso mesmo. Agora você conhece a regressão linear múltipla. Esse é provavelmente o algoritmo de aprendizado mais usado no mundo atualmente. Mas tem mais. Com apenas alguns truques, como escolher e dimensionar recursos de forma adequada e também escolher a taxa de aprendizado alfa de forma adequada, você realmente faria isso funcionar muito melhor. Só mais alguns vídeos para assistir esta semana. Vamos ao próximo vídeo para ver esses pequenos truques que ajudarão você a fazer com que a regressão linear múltipla funcione muito melhor.
