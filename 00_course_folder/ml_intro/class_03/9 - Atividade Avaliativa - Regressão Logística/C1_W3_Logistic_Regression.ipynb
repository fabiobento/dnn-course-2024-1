{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[adaptado de [Programa de cursos integrados Aprendizado de máquina](https://www.coursera.org/specializations/machine-learning-introduction) de [Andrew Ng](https://www.coursera.org/instructor/andrewng)  ([Stanford University](http://online.stanford.edu/), [DeepLearning.AI](https://www.deeplearning.ai/) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar arquivos adicionais para o laboratório.\n",
    "!wget https://github.com/fabiobento/dnn-course-2024-1/raw/main/00_course_folder/ml_intro/class_03/9%20-%20Atividade%20Avaliativa%20-%20Regress%C3%A3o%20Log%C3%ADstica/lab_utils_ml_intro_assig_week_3.zip\n",
    "!unzip -n -q lab_utils_ml_intro_assig_week_3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar se estamos no Google Colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipympl in /home/fabio/.local/lib/python3.11/site-packages (0.9.3)\n",
      "Requirement already satisfied: ipython<9 in /usr/local/lib/python3.11/dist-packages (from ipympl) (8.18.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipympl) (1.26.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/fabio/.local/lib/python3.11/site-packages (from ipympl) (0.2.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ipympl) (10.1.0)\n",
      "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.11/dist-packages (from ipympl) (5.14.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (8.1.1)\n",
      "Requirement already satisfied: matplotlib<4,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (3.8.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<9->ipympl) (4.9.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.4.0->ipympl) (2.8.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.12)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib<4,>=3.4.0->ipympl) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython<9->ipympl) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython<9->ipympl) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython<9->ipympl) (0.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística\n",
    "\n",
    "Neste exercício, você implementará a regressão logística e a aplicará a dois conjuntos de dados diferentes.\n",
    "\n",
    "\n",
    "# Tópicos\n",
    "- [ 1 - Pacotes ](#1)\n",
    "- [ 2 - Regressão Logística](#2)\n",
    "  - [ 2.1 Definição do Problema](#2.1)\n",
    "  - [ 2.2 Carregando e visualizando os dados](#2.2)\n",
    "  - [ 2.3  Função Sigmoid](#2.3)\n",
    "  - [ 2.4 Função de custo para regressão logística](#2.4)\n",
    "  - [ 2.5 Gradiente para regressão logística](#2.5)\n",
    "  - [ 2.6 Parâmetros de aprendizagem usando gradiente descendente ](#2.6)\n",
    "  - [ 2.7 Plotando a fronteira de decisão](#2.7)\n",
    "  - [ 2.8 Avaliando a regressão logística](#2.8)\n",
    "- [ 3 - Regressão Lógística Regularizada](#3)\n",
    "  - [ 3.1 Definição do Problema](#3.1)\n",
    "  - [ 3.2 Carregando e Visualizando os Dados](#3.2)\n",
    "  - [ 3.3 Mapeamento de Características](#3.3)\n",
    "  - [ 3.4 Função de custo para regressão logística regularizada](#3.4)\n",
    "  - [ 3.5 Gradiente para regressão logística regularizada](#3.5)\n",
    "  - [ 3.6 Parâmetros de aprendizagem usando gradiente descendente](#3.6)\n",
    "  - [ 3.7 Plotando a fronteira de decisão](#3.7)\n",
    "  - [ 3.8 Avaliando modelo de regressão logística regularizado](#3.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Pacotes \n",
    "\n",
    "Primeiro, vamos executar a célula abaixo para importar todos os pacotes que você precisará durante esta tarefa.\n",
    "- [numpy](www.numpy.org) é o pacote fundamental para computação científica com Python.\n",
    "- [matplotlib](http://matplotlib.org) é uma biblioteca famosa para plotar gráficos em Python.\n",
    "- ``utils.py`` contém funções auxiliares para esta tarefa. Você não precisa modificar o código neste arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Regressão Logística\n",
    "\n",
    "Nesta parte do exercício, você construirá um modelo de regressão logística para prever se um aluno será admitido em uma universidade.\n",
    "\n",
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Definição do problema\n",
    "\n",
    "Suponha que você seja o administrador de um departamento universitário e queira determinar a chance de admissão de cada candidato com base nos resultados de dois exames.\n",
    "* Você tem dados históricos de candidatos anteriores que podem ser usados como conjunto de treinamento para regressão logística.\n",
    "* Para cada exemplo de treinamento, você tem as notas do candidato em dois exames e a decisão de admissão.\n",
    "* Sua tarefa é construir um modelo de classificação que estime a probabilidade de admissão de um candidato com base nas notas desses dois exames.\n",
    "\n",
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Carregando e visualizando os dados\n",
    "\n",
    "Você começará carregando o conjunto de dados para esta tarefa.\n",
    "- A função `load_dataset()` mostrada abaixo carrega os dados nas variáveis `X_train` e `y_train`\n",
    "   - `X_train` contém notas de exames em dois exames para um aluno\n",
    "   - `y_train` é a decisão de admissão\n",
    "       - `y_train = 1` se o aluno foi admitido\n",
    "       - `y_train = 0` se o aluno não foi admitido\n",
    "   - Tanto `X_train` quanto `y_train` são arrays numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# carregar o conjunto de dados\n",
    "X_train, y_train = load_data(\"data/ex2data1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Veja as variáveis\n",
    "Vamos nos familiarizar mais com seu conjunto de dados.\n",
    "- Um bom lugar para começar é simplesmente imprimir cada variável e ver o que ela contém.\n",
    "\n",
    "O código abaixo imprime os primeiros cinco valores de `X_train` e o tipo da variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os primeiros cinco elementos em X_train são:\n",
      " [[34.62365962 78.02469282]\n",
      " [30.28671077 43.89499752]\n",
      " [35.84740877 72.90219803]\n",
      " [60.18259939 86.3085521 ]\n",
      " [79.03273605 75.34437644]]\n",
      "Tipo de X_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Os primeiros cinco elementos em X_train são:\\n\", X_train[:5])\n",
    "print(\"Tipo de X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora imprima os primeiros cinco valores de `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os primeiros cinco elementos em y_train são:\n",
      " [0. 0. 0. 1. 1.]\n",
      "Tipo de y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Os primeiros cinco elementos em y_train são:\\n\", y_train[:5])\n",
    "print(\"Tipo de y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifique as dimensões de suas variáveis\n",
    "\n",
    "Outra forma útil de se familiarizar com seus dados é visualizar suas dimensões. Vamos imprimir a forma de `X_train` e `y_train` e ver quantos exemplos de treinamento temos em nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A forma do X_train é: (100, 2)\n",
      "A forma de y_train é: (100,)\n",
      "Temos m = 100 exemplos de treinamento\n"
     ]
    }
   ],
   "source": [
    "print('A forma do X_train é: ' + str(X_train.shape))\n",
    "print('A forma de y_train é: ' + str(y_train.shape))\n",
    "print('Temos m = %d exemplos de treinamento' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize seus dados\n",
    "\n",
    "Antes de começar a implementar qualquer algoritmo de aprendizagem, é sempre bom visualizar os dados, se possível.\n",
    "- O código abaixo exibe os dados em um gráfico 2D (conforme mostrado abaixo), onde os eixos são as notas dos dois exames e os exemplos positivos e negativos são mostrados com marcadores diferentes.\n",
    "- Usamos uma função auxiliar no arquivo ``utils.py`` para gerar este gráfico.\n",
    "\n",
    "<img src=\"images/figure 1.png\" width=\"450\" height=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotar os exmplos\n",
    "plot_data(X_train, y_train[:], pos_label=\"Aprovado\", neg_label=\"Não-aprovado \")\n",
    "\n",
    "# Definir o rótulo do eixo y\n",
    "plt.ylabel('Pontuação na prova 2') \n",
    "# Definir o rótulo do eixo x\n",
    "plt.xlabel('Pontuação na prova 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seu objetivo é construir um modelo de regressão logística que se ajuste a esses dados.\n",
    "- Com este modelo, você pode prever se um novo aluno será admitido com base nas notas dos dois exames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### 2.3 Função sigmóide\n",
    "\n",
    "Lembre-se que para regressão logística, o modelo é representado como\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "onde a função $g$ é a função sigmóide. A função sigmóide é definida como:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Vamos implementar a função sigmóide primeiro, para que ela possa ser usada no restante desta tarefa.\n",
    "\n",
    "<a name='ex-01'></a>\n",
    "### Exercício 1\n",
    "Complete a função `sigmoid` para calcular\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Observe que\n",
    "- `z` nem sempre é um número único, mas também pode ser uma matriz de números.\n",
    "- Se a entrada for uma matriz de números, gostaríamos de aplicar a função sigmóide a cada valor da matriz de entrada.\n",
    "\n",
    "Se tiver dúvidas, você pode conferir as dicas apresentadas após a célula abaixo para ajudá-lo na implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcular o sigmoide de z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): vetor numérica de qualquer tamanho.\n",
    "\n",
    "    Retorns:\n",
    "        g (ndarray): sigmoid(z), com a mesma forma de z\n",
    "         \n",
    "    \"\"\"\n",
    "          \n",
    "    ### INICIE SEU CÓDIGO AQUI ### \n",
    "    \n",
    "    ### TERMINE SEU CÓDIGO AQUI ### \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para dicas</b></font></summary>\n",
    "       \n",
    "   * O `numpy` tem uma função chamada [`np.exp()`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html), que oferece uma maneira conveniente de calcular o exponencial ( $e^{z}$) de todos os elementos da matriz de entrada (`z`).\n",
    "\n",
    " \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Clique para ainda mais dicas</b></font></summary>\n",
    "        \n",
    "  - Você pode traduzir $e^{-z}$ em código como `np.exp(-z)` \n",
    "    \n",
    "  - Você pode traduzir $1/e^{-z}$ em código como `1/np.exp(-z)` \n",
    "    \n",
    "    Se você ainda estiver com dificuldades, pode verificar as dicas apresentadas abaixo para descobrir como calcular `g` \n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular g</b></font></summary>\n",
    "        <code>g = 1 / (1 + np.exp(-z))</code>\n",
    "    </details>\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando terminar, tente testar alguns valores chamando `sigmoid(x)` na célula abaixo. \n",
    "- Para valores positivos grandes de x, o sigmoide deve estar próximo de 1, enquanto para valores negativos grandes, o sigmoide deve estar próximo de 0. \n",
    "- A avaliação de `sigmoid(0)` deve lhe dar exatamente 0,5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Observação: você pode editar esse valor\n",
    "value = 0\n",
    "\n",
    "print (f\"sigmoid({value}) = {sigmoid(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>sigmoid(0)<b></td>\n",
    "    <td> 0.5 </td> \n",
    "  </tr>\n",
    "</table>\n",
    "    \n",
    "- Como mencionado anteriormente, seu código também deve funcionar com vetores e matrizes. Para uma matriz, sua função deve executar a função sigmoide em cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print (\"sigmoid([ -1, 0, 1, 2]) = \" + str(sigmoid(np.array([-1, 0, 1, 2]))))\n",
    "\n",
    "# TESTE DA UNIDADE\n",
    "from public_tests import *\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>sigmoid([-1, 0, 1, 2])<b></td> \n",
    "    <td>[0.26894142        0.5           0.73105858        0.88079708]</td> \n",
    "  </tr>    \n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.4\"></a>\n",
    "### 2.4 Função de custo para regressão logística\n",
    "\n",
    "Nesta seção, você implementará a função de custo para regressão logística.\n",
    "\n",
    "<a name='ex-02'></a>\n",
    "### Exercício 2\n",
    "\n",
    "Complete a função `compute_cost` usando as equações abaixo.\n",
    "\n",
    "Lembre-se de que, para a regressão logística, a função de custo tem a forma \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "onde\n",
    "* m é o número de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ é o custo de um único ponto de dados, que é - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ é a previsão do modelo, enquanto $y^{(i)}$, que é o rótulo real\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ m que a função $g$ é a função sigmoide.\n",
    "    * Pode ser útil calcular primeiro uma variável intermediária $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ onde $n$ é o número de recursos, antes de calcular $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$\n",
    "\n",
    "Observação:\n",
    "* Enquanto estiver fazendo isso, lembre-se de que as variáveis `X_train` e `y_train` não são valores escalares, mas matrizes de forma ($m, n$) e ($𝑚$,1), respectivamente, em que $𝑛$ é o número de recursos e $𝑚$ é o número de exemplos de treinamento.\n",
    "* Você pode usar a função sigmoide que implementou acima para essa parte.\n",
    "\n",
    "Se tiver dúvidas, consulte as dicas apresentadas após a célula abaixo para ajudá-lo na implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Calcula o custo de todos os exemplos\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m examples by n features\n",
    "      y : (ndarray Shape (m,))  valor alvo\n",
    "      w : (ndarray Shape (n,))  valores dos parâmetros do modelo      \n",
    "      b : (scalar)              valores dos parâmetros do modelo      \n",
    "      *argv : não utilizado, para compatibilidade com a versão regularizada abaixo\n",
    "    Returns:\n",
    "      total_cost : (escalar) custo \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### INICIE SEU CÓDIGO AQUI ### \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ### TERMINE SEU CÓDIGO AQUI ###\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para Dicas</b></font></summary>\n",
    "    \n",
    "    * Você pode representar um operador de soma, por exemplo: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ no código da seguinte forma:   \n",
    "    ```python \n",
    "        h = 0\n",
    "        for i in range(m):\n",
    "            h = h + 2*i\n",
    "    ```\n",
    "   * Em seguida, você pode retornar o `total_cost` como `loss_sum` dividido por `m`.\n",
    "   * Se você for novato em Python, verifique se o código está devidamente recuado com espaços ou tabulações consistentes. Caso contrário, ele poderá produzir uma saída diferente ou gerar um erro `IndentationError: unexpected indent`. Para obter detalhes, consulte [esse tópico](https://community.deeplearning.ai/t/indentation-in-python-indentationerror-unexpected-indent/159398) no fórum de DeepLearning.AI.\n",
    "     \n",
    "   <details>\n",
    "    <summary><font size=\"2\" color=\"darkblue\"><b> Clique para mai Dicas</b></font></summary>\n",
    "        \n",
    "    * Veja como você pode estruturar a implementação geral dessa função\n",
    "        \n",
    "    ```python \n",
    "    def compute_cost(X, y, w, b, *argv):\n",
    "        m, n = X.shape\n",
    "    \n",
    "        ### INICIE SEU CÓDIGO AQUI ###\n",
    "        loss_sum = 0 \n",
    "        \n",
    "        # Fazer um loop em cada exemplo de treinamento\n",
    "        for i in range(m): \n",
    "            \n",
    "            # Primeiro calcule z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b\n",
    "            z_wb = 0 \n",
    "            # Fazer um loop em cada recurso\n",
    "            for j in range(n): \n",
    "                # Adicione o termo correspondente a z_wb\n",
    "                z_wb_ij = # Seu código para calcular w[j] * X[i][j]\n",
    "                z_wb += z_wb_ij # equivalente a z_wb = z_wb + z_wb_ij\n",
    "            # Adicione o termo de polarização a z_wb\n",
    "            z_wb += b #equivalente a z_wb = z_wb + b\n",
    "        \n",
    "            f_wb = # Seu código aqui para calcular a previsão f_wb para um exemplo de treinamento\n",
    "            loss =  # Seu código aqui para calcular a perda para um exemplo de treinamento\n",
    "            \n",
    "            loss_sum += loss # equivalent to loss_sum = loss_sum + loss\n",
    "        \n",
    "        total_cost = (1 / m) * loss_sum  \n",
    "        ### TERMINE SEU CÓDIGO AQUI ### \n",
    "        \n",
    "        return total_cost\n",
    "    ```\n",
    "       Se ainda estiver com dúvidas, você pode consultar as dicas apresentadas abaixo para descobrir como calcular `z_wb_ij`, `f_wb` e `cost`.\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica pra calcular z_wb_ij</b></font></summary>\n",
    "           &emsp; &emsp; <code>z_wb_ij = w[j]*X[i][j] </code>\n",
    "    </details>\n",
    "        \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica pra calcular f_wb</b></font></summary>\n",
    "           &emsp; &emsp; $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$ onde $g$ é a função sigmoid. Você pode simplesmente chamar a função `sigmoid` implementada acima.\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular f</b></font></summary>\n",
    "               &emsp; &emsp; Você pode calcular f_wb como <code>f_wb = sigmoid(z_wb) </code>\n",
    "           </details>\n",
    "    </details>\n",
    "\n",
    "     <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular a perda</b></font></summary>\n",
    "          &emsp; &emsp; Você pode usar função <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.log.html\">np.log</a> para calcular o log\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular a perda</b></font></summary>\n",
    "              &emsp; &emsp; Você pode calcular a perda com <code>loss =  -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)</code>\n",
    "          </details>\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as células abaixo para verificar sua implementação da função `compute_cost` com duas inicializações diferentes dos parâmetros $w$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Calcular e exibir o custo com w e b inicializados como zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Custo em w e b iniciais (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo inicial w e b (zeros)<b></td>\n",
    "    <td> 0.693 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o custo com w e b diferentes de zero\n",
    "test_w = np.array([0.2, 0.2])\n",
    "test_b = -24.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Custo de w e b (não-zeros) no conjunto de teste: {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "# TESTE DA UNIDADE\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo de w e b (não-zeros) no conjunto de teste:<b></td>\n",
    "    <td> 0.218 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.5\"></a>\n",
    "### 2.5 Gradiente para regressão logística\n",
    "\n",
    "Nesta seção, você implementará o gradiente para a regressão logística.\n",
    "\n",
    "Lembre-se de que o algoritmo de descida de gradiente é:\n",
    "\n",
    "$$\\begin{align*}& \\text{repita até a convergência:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{para j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "onde os parâmetros $b$, $w_j$ são atualizados simultaneamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name='ex-03'></a>\n",
    "### Exercício 3\n",
    "\n",
    "Complete a função `compute_gradient` para calcular $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ das equações (2) e (3) abaixo.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m é o número de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ é a predição do modelo, enquanto $y^{(i)}$ é o rótulo verdadeiro\n",
    "\n",
    "\n",
    "**Nota**: Embora esse gradiente pareça idêntico ao gradiente da regressão linear, a fórmula é, na verdade, diferente porque a regressão linear e a regressão logística têm definições diferentes de $f_{\\mathbf{w},b}(x)$.\n",
    "\n",
    "Como antes, você pode usar a função sigmoide que implementou acima e, se tiver dúvidas, pode consultar as dicas apresentadas após a célula abaixo para ajudá-lo com a implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, *argv): \n",
    "    \"\"\"\n",
    "    Calcula o gradiente para regressão logística \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos n características\n",
    "      y : (ndarray Shape (m,))  valor alvo\n",
    "      w : (ndarray Shape (n,))  valores dos parâmetros do modelo\n",
    "      b : (scalar)              valores dos parâmetros do modelo\n",
    "      *argv : não utilizado, para compatibilidade com a versão regularizada abaixo\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) O gradiente do custo em relação aos parâmetros w. \n",
    "      dj_db : (scalar)             O gradiente do custo em relação aos parâmetros b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    ### INICIE SEU CÓDIGO AQUI ### \n",
    "    for i in range(m):\n",
    "        z_wb = None\n",
    "        for j in range(n): \n",
    "            z_wb += None\n",
    "        z_wb += None\n",
    "        f_wb = None\n",
    "        \n",
    "        dj_db_i = None\n",
    "        dj_db += None\n",
    "        \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = None\n",
    "            \n",
    "    dj_dw = None\n",
    "    dj_db = None\n",
    "    ### TERMINE SEU CÓDIGO AQUI ### \n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para Dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como você pode estruturar a implementação geral dessa função\n",
    "    ```python \n",
    "       def compute_gradient(X, y, w, b, *argv): \n",
    "            m, n = X.shape\n",
    "            dj_dw = np.zeros(w.shape)\n",
    "            dj_db = 0.\n",
    "        \n",
    "            ### INICIE SEU CÓDIGO AQUI ### \n",
    "            for i in range(m):\n",
    "                # Calcule f_wb (exatamente como você fez na função compute_cost acima)\n",
    "                f_wb = \n",
    "        \n",
    "                # Calcule o gradiente para b a partir deste exemplo\n",
    "                dj_db_i = # Seu código aqui para calcular o erro\n",
    "        \n",
    "                # some a dj_db\n",
    "                dj_db += dj_db_i\n",
    "        \n",
    "                # obter dj_dw para cada atributo\n",
    "                for j in range(n):\n",
    "                    # Você codifica aqui para calcular o gradiente do i-ésimo exemplo para o j-ésimo atributo\n",
    "                    dj_dw_ij =  \n",
    "                    dj_dw[j] += dj_dw_ij\n",
    "        \n",
    "            # dividir dj_db e dj_dw pelo número total de exemplos\n",
    "            dj_dw = dj_dw / m\n",
    "            dj_db = dj_db / m\n",
    "            ### TERMINE SEU CÓDIGO AQUI ### \n",
    "       \n",
    "            return dj_db, dj_dw\n",
    "    ```\n",
    "   * Se você for novato em Python, verifique se o código está devidamente recuado com espaços ou tabulações consistentes. Caso contrário, ele poderá produzir uma saída diferente ou gerar um erro `IndentationError: unexpected indent`. Para obter detalhes, consulte [esse tópico](https://community.deeplearning.ai/t/indentation-in-python-indentationerror-unexpected-indent/159398) no fórum de DeepLearning.AI.\n",
    "\n",
    "\n",
    "    * Se ainda tiver dúvidas, você pode consultar as dicas apresentadas abaixo para descobrir como calcular `f_wb`, `dj_db_i` e `dj_dw_ij` \n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate f_wb</b></font></summary>\n",
    "           &emsp; &emsp; Lembre-se de que você calculou f_wb em <code>compute_cost</code> acima - para obter dicas detalhadas sobre como calcular cada termo intermediário, consulte a seção de dicas abaixo desse exercício\n",
    "           <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas pra calcular f_wb</b></font></summary>\n",
    "              &emsp; &emsp; Você pode calcular f_wb com\n",
    "               <pre>\n",
    "               for i in range(m):   \n",
    "                   # Calcule f_wb (exatamente como você fez na função compute_cost acima)\n",
    "                   z_wb = 0\n",
    "                   # Fazer um loop em cada recurso\n",
    "                   for j in range(n): \n",
    "                       # Adicione o termo correspondente a z_wb\n",
    "                       z_wb_ij = X[i, j] * w[j]\n",
    "                       z_wb += z_wb_ij\n",
    "            \n",
    "                   # Adicionar termo de viés \n",
    "                   z_wb += b\n",
    "        \n",
    "                   # Calcular a previsão do modelo\n",
    "                   f_wb = sigmoid(z_wb)\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular dj_db_i</b></font></summary>\n",
    "           &emsp; &emsp; Você pode calcular dj_db_i como <code>dj_db_i = f_wb - y[i]</code>\n",
    "    </details>\n",
    "        \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica pra calcular dj_dw_ij</b></font></summary>\n",
    "        &emsp; &emsp; Você pode calcular dj_dw_ij como <code>dj_dw_ij = (f_wb - y[i])* X[i][j]</code>\n",
    "    </details>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as células abaixo para verificar sua implementação da função `compute_gradient` com duas inicializações diferentes dos parâmetros $w$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o gradiente com w e b inicializados como zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print(f'dj_db em w and b (zeros) iniciais:{dj_db}' )\n",
    "print(f'dj_dw e, w and b (zeros) iniciais:{dj_dw.tolist()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db em w and b (zeros) iniciais:<b></td>\n",
    "    <td> -0.1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>dj_dw e, w and b (zeros) iniciais:<b></td>\n",
    "    <td> [-12.00921658929115, -11.262842205513591] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o custo e o gradiente com w e b diferentes de zero\n",
    "test_w = np.array([ 0.2, -0.5])\n",
    "test_b = -24\n",
    "dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('dj_db em w and b de teste:', dj_db)\n",
    "print('dj_dw em w and b de teste:', dj_dw.tolist())\n",
    "\n",
    "# TESTE DA UNDADE    \n",
    "compute_gradient_test(compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db em w and b de teste:<b></td>\n",
    "    <td> -0.5999999999991071 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>dj_dw em w and b de teste:<b></td>\n",
    "    <td>  [-44.8313536178737957, -44.37384124953978] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.6\"></a>\n",
    "### 2.6 Parâmetros de aprendizagem usando a descida de gradiente \n",
    "\n",
    "Da mesma forma que na tarefa anterior, agora você encontrará os parâmetros ideais de um modelo de regressão logística usando a descida gradiente. \n",
    "- Você não precisa implementar nada para esta parte. Basta executar as células abaixo. \n",
    "\n",
    "- Uma boa maneira de verificar se a descida de gradiente está funcionando corretamente é observar\n",
    "o valor de $J(\\mathbf{w},b)$ e verificar se ele está diminuindo a cada etapa. \n",
    "\n",
    "- Supondo que você tenha implementado o gradiente e calculado o custo corretamente, o valor de $J(\\mathbf{w},b)$ nunca deve aumentar e deve convergir para um valor estável no final do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Executa a descida do gradiente em lote para aprender theta. Atualiza theta tomando \n",
    "    num_iters etapas de gradiente com taxa de aprendizado alfa\n",
    "    \n",
    "    Args:\n",
    "      X :    (ndarray Shape (m, n) dados, m exemplos n características\n",
    "      y :    (ndarray Shape (m,))  valor alvo\n",
    "      w_in : (ndarray Shape (n,))  valores dos parâmetros do modelo\n",
    "      b_in : (scalar)              valores dos parâmetros do modelo\n",
    "      cost_function :              função para calcular custo\n",
    "      gradient_function :          função para calcular o gradiente\n",
    "      alpha : (float)              taxa de aprendizado\n",
    "      num_iters : (int)            número de iterações para executar a descida do gradiente\n",
    "      lambda_ : (scalar, float)    constante de regularização\n",
    "      \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Valores atualizados dos parâmetros do modelo após executar a descida do gradiente\n",
    "      b : (scalar)             Valor atualizado do parâmetro do modelo após a execução da descida do gradiente\n",
    "    \"\"\"\n",
    "    \n",
    "    # Número de exemplos de treinamento\n",
    "    m = len(X)\n",
    "    \n",
    "    # um vetor para armazenar os custos J e w em cada iteração, principalmente para gráficos posteriores\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calcular o gradiente e atualizar os parâmetros\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Atualizar parâmetros usando w, b, alfa e gradiente\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Salvar o custo J em cada iteração\n",
    "        if i<100000:      # Evitar o esgotamento de recursos \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Imprimir o custo a cada intervalo de 10 vezes ou tantas iterações se < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteração {i:4}: Custo {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #retornar w e J,e histórico de w para gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos executar o algoritmo de descida de gradiente acima para aprender os parâmetros do nosso conjunto de dados.\n",
    "\n",
    "**Nota**\n",
    "O bloco de código abaixo leva alguns minutos para ser executado, especialmente com uma versão não vetorizada. Você pode reduzir as `iterações` para testar sua implementação e iterar mais rapidamente. Se você tiver tempo mais tarde, tente executar 100.000 iterações para obter melhores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "# Algumas configurações de descida de gradiente\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>Saída Esperada: Custo     0.30, (Clique para ver detalhes):</b>\n",
    "</summary>\n",
    "\n",
    "    # Com as seguintes configurações\n",
    "    np.random.seed(1)\n",
    "    initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "    initial_b = -8\n",
    "    iterations = 10000\n",
    "    alpha = 0.001\n",
    "    #\n",
    "\n",
    "```\n",
    "Iteração    0: Custo     0.96   \n",
    "Iteração 1000: Custo     0.31   \n",
    "Iteração 2000: Custo     0.30   \n",
    "Iteração 3000: Custo     0.30   \n",
    "Iteração 4000: Custo     0.30   \n",
    "Iteração 5000: Custo     0.30   \n",
    "Iteração 6000: Custo     0.30   \n",
    "Iteração 7000: Custo     0.30   \n",
    "Iteração 8000: Custo     0.30   \n",
    "Iteração 9000: Custo     0.30   \n",
    "Iteração 9999: Custo     0.30   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.7\"></a>\n",
    "### 2.7 Traçando a fronteira  de decisão\n",
    "\n",
    "Agora usaremos os parâmetros finais da descida de gradiente para plotar o ajuste linear. Se você implementou as partes anteriores corretamente, deverá ver um gráfico semelhante ao seguinte:   \n",
    "<img src=\"images/figure 2.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "Usaremos uma função auxiliar no arquivo `utils.py` para criar esse gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(w, b, X_train, y_train)\n",
    "# Definir o rótulo do eixo y\n",
    "plt.ylabel('Pontuação da prova 2') \n",
    "# Definir o rótulo do eixo x\n",
    "plt.xlabel('Pontuação da prova 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.8\"></a>\n",
    "### 2.8 Avaliação da regressão logística\n",
    "\n",
    "Podemos avaliar a qualidade dos parâmetros que encontramos verificando a qualidade da previsão do modelo aprendido em nosso conjunto de treinamento. \n",
    "\n",
    "Você implementará a função `predict` abaixo para fazer isso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-04'></a>\n",
    "### Exercício 4\n",
    "\n",
    "Complete a função `predict` para produzir previsões `1` ou `0` com um conjunto de dados e um vetor de parâmetros aprendidos $w$ e $b$.\n",
    "- Primeiro, você precisa calcular a previsão do modelo $f(x^{(i)}) = g(w \\cdot x^{(i)} + b)$ para cada exemplo \n",
    "    - Você já implementou isso antes nas partes acima\n",
    "- Interpretamos o resultado do modelo ($f(x^{(i)})$) como a probabilidade de que $y^{(i)}=1$ dado $x^{(i)}$ e parametrizado por $w$.\n",
    "- Portanto, para obter uma previsão final ($y^{(i)}=0$ ou $y^{(i)}=1$) do modelo de regressão logística, você pode usar a seguinte heurística -\n",
    "  if $f(x^{(i)}) >= 0.5$, predizer $y^{(i)}=1$\n",
    "  \n",
    "  if $f(x^{(i)}) < 0.5$, predizer $y^{(i)}=0$\n",
    "    \n",
    "Se tiver dúvidas, você pode consultar as dicas apresentadas após a célula abaixo para ajudá-lo na implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Prever se o rótulo é 0 ou 1 usando os parâmetros de regressão logística aprendidos\n",
    "    de regressão logística aprendidos w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados,m exemplos, n recursos\n",
    "      w : (ndarray Shape (n,))  valores dos parâmetros do modelo\n",
    "      b : (scalar)              valores dos parâmetros do modelo\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) As previsões para X usando um limite de 0,5\n",
    "    \"\"\"\n",
    "    # Número de exemplos de treinamento\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### INICIE SEU CÓDIGO AQUI ### \n",
    "    # Fazer um loop em cada exemplo\n",
    "    for i in range(m):   \n",
    "        z_wb = None\n",
    "        # Fazer um loop em cada recurso\n",
    "        for j in range(n): \n",
    "            # Adicione o termo correspondente a z_wb\n",
    "            z_wb += None\n",
    "        \n",
    "        # Adicionar termo de viés \n",
    "        z_wb += None\n",
    "        \n",
    "        # Calcule a previsão para este exemplo\n",
    "        f_wb = None\n",
    "\n",
    "        # Aplicar o limiar (thershold)\n",
    "        p[i] = None\n",
    "        \n",
    "    ### TERMINE SEU CÓDIGO AQUI ### \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para Dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como você pode estruturar a implementação geral dessa função\n",
    "    ```python \n",
    "       def predict(X, w, b): \n",
    "            # Número de exemplos de treinamento\n",
    "            m, n = X.shape   \n",
    "            p = np.zeros(m)\n",
    "   \n",
    "            ### INICIE SEU CÓDIGO AQUI ### \n",
    "            # Fazer um loop em cada exemplo\n",
    "            for i in range(m):   \n",
    "                \n",
    "                # Calcular f_wb (exatamente como você fez na função compute_cost acima) \n",
    "                # usando algumas linhas de código\n",
    "                f_wb = \n",
    "\n",
    "                # Calcule a previsão para esse exemplo de treinamento \n",
    "                p[i] = # Seu código aqui para calcular a previsão com base em f_wb\n",
    "        \n",
    "            ### TERMINE SEU CÓDIGO AQUI ### \n",
    "            return p\n",
    "    ```\n",
    "  \n",
    "        Se ainda estiver com dúvidas, você pode consultar as dicas apresentadas abaixo para descobrir como calcular `f_wb` e `p[i]` \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular f_wb</b></font></summary>\n",
    "           &emsp; &emsp; Lembre-se de que você calculou f_wb em <code>compute_cost</code> acima — Para obter dicas detalhadas sobre como calcular cada termo intermediário, consulte a seção de dicas abaixo desse exercício\n",
    "           <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular f_wb</b></font></summary>\n",
    "              &emsp; &emsp; Você pode calcular f_wb como\n",
    "               <pre>\n",
    "               for i in range(m):   \n",
    "                    # Calcule f_wb (exatamente como você fez na função compute_cost acima)\n",
    "                    z_wb = 0\n",
    "                    # Fazer um loop em cada recurso\n",
    "                    for j in range(n): \n",
    "                       # Adicionar o termo correspondente a z_wb\n",
    "                       z_wb_ij = X[i, j] * w[j]\n",
    "                       z_wb += z_wb_ij\n",
    "                    # Adicionar o termo de viés\n",
    "                    z_wb += b\n",
    "                # Calcule a previsão do modelo\n",
    "                f_wb = sigmoid(z_wb)\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular p[i]</b></font></summary>\n",
    "           &emsp; &emsp; Por exemplo, se você quiser dizer que x = 1 se y for menor que 3 e 0 caso contrário, você pode expressar isso em código como <code>x = y < 3 </code>. Agora faça o mesmo apra p[i] = 1 if f_wb >= 0.5 e 0 caso contrário. \n",
    "           <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular p[i]</b></font></summary>\n",
    "              &emsp; &emsp; Você pode calcular p[i] como <code>p[i] = f_wb >= 0.5</code>\n",
    "          </details>\n",
    "    </details>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de concluir a função `predict`, vamos executar o código abaixo para informar a precisão do treinamento de seu classificador, calculando a porcentagem de exemplos corretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Teste seu código de previsão\n",
    "np.random.seed(1)\n",
    "tmp_w = np.random.randn(2)\n",
    "tmp_b = 0.3    \n",
    "tmp_X = np.random.randn(4, 2) - 0.5\n",
    "\n",
    "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
    "print(f'Saída da previsão: formato {tmp_p.shape}, valor {tmp_p}')\n",
    "\n",
    "# TESTES DA UNIDADE\n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Espereda** \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Saída da previsão: formato (4,), valor [0. 1. 1. 1.]<b></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos usar isso para calcular a precisão no conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Calcule a precisão em nosso conjunto de treinamento\n",
    "p = predict(X_train, w,b)\n",
    "print('Acurácia de Treino: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Acurácia de Treino (aprox):<b></td>\n",
    "    <td> 92.00 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Regressão logística regularizada\n",
    "\n",
    "Nesta parte do exercício, você implementará a regressão logística regularizada para prever se os microchips de uma fábrica serão aprovados no controle de qualidade (QA). Durante o QA, cada microchip passa por vários testes para garantir que esteja funcionando corretamente. \n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Declaração do problema\n",
    "\n",
    "Suponha que você seja o gerente de produtos da fábrica e tenha os resultados dos testes de alguns microchips em dois testes diferentes. \n",
    "- A partir desses dois testes, você gostaria de determinar se os microchips devem ser aceitos ou rejeitados. \n",
    "- Para ajudá-lo a tomar a decisão, você tem um conjunto de dados de resultados de testes de microchips anteriores, a partir dos quais pode criar um modelo de regressão logística.\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Carregando e visualizando os dados\n",
    "\n",
    "De forma semelhante às partes anteriores deste exercício, vamos começar carregando o conjunto de dados para esta tarefa e visualizando-o. \n",
    " \n",
    "- A função `load_dataset()` mostrada abaixo carrega os dados nas variáveis `X_train` e `y_train`\n",
    "  - `X_train` contém os resultados dos testes para os microchips de dois testes\n",
    "  - `y_train` contém os resultados do controle de qualidade  \n",
    "      - `y_train = 1` se o microchip foi aceito \n",
    "      - `y_train = 0` se o microchip foi rejeitado \n",
    "  - Tanto `X_train` quanto `y_train` são matrizes numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Carregar o conjunto de dados\n",
    "X_train, y_train = load_data(\"data/ex2data2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exibir as variáveis\n",
    "\n",
    "O código abaixo imprime os cinco primeiros valores de `X_train` e `y_train` e o tipo das variáveis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# imprimir X_train\n",
    "print(\"X_train:\", X_train[:5])\n",
    "print(\"Tipo de X_train:\",type(X_train))\n",
    "\n",
    "# imprimir y_train\n",
    "print(\"y_train:\", y_train[:5])\n",
    "print(\"Tipo de y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifique as dimensões de suas variáveis\n",
    "\n",
    "Outra maneira útil de se familiarizar com seus dados é visualizar suas dimensões. Vamos imprimir a forma de `X_train` e `y_train` e ver quantos exemplos de treinamento temos em nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print ('O formaro de X_train é: ' + str(X_train.shape))\n",
    "print ('O formato de y_train é: ' + str(y_train.shape))\n",
    "print ('Temo m = %d exemplos de treinamento' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize seus dados\n",
    "\n",
    "A função auxiliar `plot_data` (de `utils.py`) é usada para gerar uma figura como a Figura 3, em que os eixos são as duas pontuações de teste, e os exemplos positivos (y = 1, aceito) e negativos (y = 0, rejeitado) são mostrados com marcadores diferentes.\n",
    "\n",
    "<img src=\"images/figure 3.png\"  width=\"450\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar os exemplos\n",
    "plot_data(X_train, y_train[:], pos_label=\"Aceito\", neg_label=\"Rejeitado\")\n",
    "\n",
    "# Definir o rótulo do eixo y\n",
    "plt.ylabel('Teste 2 de Microchip 2') \n",
    "# Definir o rótulo do eixo x\n",
    "plt.xlabel('Teste 2 de Microchip 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Figura 3 mostra que nosso conjunto de dados não pode ser separado em exemplos positivos e negativos por uma linha reta através do gráfico. Portanto, uma aplicação direta da regressão logística não terá um bom desempenho nesse conjunto de dados, pois a regressão logística só conseguirá encontrar um limite de decisão linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Mapeamento de recursos\n",
    "\n",
    "Uma maneira de ajustar melhor os dados é criar mais recursos a partir de cada ponto de dados. Na função fornecida `map_feature`, mapearemos os recursos em todos os termos polinomiais de $x_1$ e $x_2$ até a sexta potência.\n",
    "\n",
    "$$\\mathrm{map\\_feature}(x) = \n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_1^2\\\\\n",
    "x_1 x_2\\\\\n",
    "x_2^2\\\\\n",
    "x_1^3\\\\\n",
    "\\vdots\\\\\n",
    "x_1 x_2^5\\\\\n",
    "x_2^6\\end{array}\\right]$$\n",
    "\n",
    "Como resultado desse mapeamento, nosso vetor de dois recursos (as pontuações em dois testes de controle de qualidade) foi transformado em um vetor de 27 dimensões. \n",
    "\n",
    "- Um classificador de regressão logística treinado nesse vetor de recursos de dimensão mais alta terá um limite de decisão mais complexo e será não linear quando desenhado em nosso gráfico bidimensional. \n",
    "- Fornecemos a função `map_feature` para você em utils.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"Formato original dos dados:\", X_train.shape)\n",
    "\n",
    "mapped_X =  map_feature(X_train[:, 0], X_train[:, 1])\n",
    "print(\"Forma após o mapeamento de recursos:\", mapped_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos também imprimir os primeiros elementos de `X_train` e `mapped_X` para ver a transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"X_train[0]:\", X_train[0])\n",
    "print(\"X_train[0] mapeado:\", mapped_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora o mapeamento de recursos nos permita criar um classificador mais expressivo, ele também é mais suscetível ao ajuste excessivo. Nas próximas partes do exercício, você implementará a regressão logística regularizada para ajustar os dados e também verá por si mesmo como a regularização pode ajudar a combater o problema do ajuste excessivo.\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Função de custo para regressão logística regularizada\n",
    "\n",
    "Nesta parte, você implementará a função de custo para a regressão logística regularizada.\n",
    "\n",
    "Lembre-se de que, para a regressão logística regularizada, a função de custo tem a forma\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "Compare isso com a função de custo sem regularização (que você implementou acima), que tem o seguinte formato \n",
    "\n",
    "$$ J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$\n",
    "\n",
    "A diferença é o termo de regularização, que é $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ \n",
    "Observe que o parâmetro $b$ não é regularizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-05'></a>\n",
    "### Exercício 5\n",
    "\n",
    "Complete a função `compute_cost_reg` abaixo para calcular o seguinte termo para cada elemento em $w$ \n",
    "$$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "O código inicial adiciona isso ao custo sem regularização (que você calculou acima em `compute_cost`) para calcular o custo com regulatização.\n",
    "\n",
    "Se tiver dúvidas, você pode consultar as dicas apresentadas após a célula abaixo para ajudá-lo na implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Calcula o custo de todos os exemplos\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos, n recursos\n",
    "      y : (ndarray Shape (m,))  valor alvo\n",
    "      w : (ndarray Shape (n,))  valores dos parametros do modelo\n",
    "      b : (escalar)              valores dos parametros do modelo\n",
    "      lambda_ : (escalar, float) Controla a quantidade de regularização\n",
    "    Returns:\n",
    "      total_cost : (escalar)     custo \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Chama a função compute_cost que você implementou acima\n",
    "    cost_without_reg = compute_cost(X, y, w, b) \n",
    "    \n",
    "    # Você precisa calcular esse valor\n",
    "    reg_cost = 0.\n",
    "    \n",
    "    ### INICIE SEU CÓDIGO AQUI ###\n",
    "    \n",
    "        \n",
    "    \n",
    "    ### TERMINE SEU CÓDIGO AQUI ###\n",
    "    \n",
    "    # Adicione o custo de regularização para obter o custo total\n",
    "    total_cost = cost_without_reg + reg_cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como você pode estruturar a implementação geral dessa função\n",
    "    ```python \n",
    "       def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "   \n",
    "           m, n = X.shape\n",
    "    \n",
    "            # Chama a função compute_cost que você implementou acima\n",
    "            cost_without_reg = compute_cost(X, y, w, b) \n",
    "    \n",
    "            # Você precisa calcular esse valor\n",
    "            reg_cost = 0.\n",
    "    \n",
    "            ### INICIE SEU CÓDIGO AQUI ###\n",
    "            for j in range(n):\n",
    "                reg_cost_j = # Seu código aqui para calcular o custo de w[j]\n",
    "                reg_cost = reg_cost + reg_cost_j\n",
    "            reg_cost = (lambda_/(2 * m)) * reg_cost\n",
    "            ### TERMINE SEU CÓDIGO AQUI ###\n",
    "    \n",
    "            # Adicione o custo de regularização para obter o custo total\n",
    "            total_cost = cost_without_reg + reg_cost\n",
    "\n",
    "        return total_cost\n",
    "    ```\n",
    "      Se ainda tiver dúvidas, você pode consultar as dicas apresentadas abaixo para descobrir como calcular o `reg_cost_j` \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular reg_cost_j</b></font></summary>\n",
    "           &emsp; &emsp; Você pode calcular reg_cost_j como <code>reg_cost_j = w[j]**2 </code> \n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to check your implementation of the `compute_cost_reg` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1]) - 0.5\n",
    "initial_b = 0.5\n",
    "lambda_ = 0.5\n",
    "cost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(\"Custo Regularizado :\", cost)\n",
    "\n",
    "# TESTE DA UNIDADE\n",
    "compute_cost_reg_test(compute_cost_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo Regularizado : <b></td>\n",
    "    <td> 0.6618252552483948 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "### 3.5 Gradiente para regressão logística regularizada\n",
    "\n",
    "Nesta seção, você implementará o gradiente para regressão logística regularizada.\n",
    "\n",
    "\n",
    "O gradiente da função de custo regularizado tem dois componentes. O primeiro, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ é um escalar, o outro é um vetor com a mesma forma dos parâmetros $\\mathbf{w}$, em que o elemento $j^\\mathrm{th}$ é definido da seguinte forma:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\text{para} \\, j=0...(n-1)$$\n",
    "\n",
    "Compare isso com o gradiente da função de custo sem regularização (que você implementou acima), que tem a forma \n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "\n",
    "\n",
    "Como você pode ver,$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ é o mesmo, a diferença é o seguinte termo em $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, que é $$\\frac{\\lambda}{m} w_j  \\quad\\, \\text{para}\\,  j=0...(n-1)$$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-06'></a>\n",
    "### Exercício 6\n",
    "\n",
    "Preencha a função `compute_gradient_reg` abaixo para modificar o código abaixo e calcular o seguinte termo\n",
    "\n",
    "$$\\frac{\\lambda}{m} w_j \\quad\\, \\, \\text{for} \\, j=0...(n-1)$$\n",
    "\n",
    "O código inicial adicionará esse termo ao $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$ retornado de `compute_gradient` acima para obter o gradiente da função de custo regularizado.\n",
    "\n",
    "\n",
    "Se tiver dúvidas, você pode consultar as dicas apresentadas após a célula abaixo para ajudá-lo na implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "    \"\"\"\n",
    "    Calcula o gradiente para regressão logística com regularização\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados,m exemplos, n recursos\n",
    "      y : (ndarray Shape (m,))  valor alvo\n",
    "      w : (ndarray Shape (n,))  valores de parâmetros do modelo\n",
    "      b : (scalar)                valores de parâmetros do modelo\n",
    "      lambda_ : (scalar,float)  constante de regularização\n",
    "    Returns\n",
    "      dj_db : (scalar)             O gradiente do custo em relação ao parâmetro b. \n",
    "      dj_dw : (ndarray Shape (n,)) O gradiente do custo em relação ao parâmetro w. \n",
    "\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "\n",
    "    ### INICIE SEU CÓDIGO AQUI ###\n",
    "    \n",
    "        \n",
    "    ### TERMINE SEU CÓDIGO AQUI ###\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como você pode estruturar a implementação geral dessa função\n",
    "    ```python \n",
    "    def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "        m, n = X.shape\n",
    "    \n",
    "        dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "\n",
    "        ### INICIE SEU CÓDIGO AQUI ###\n",
    "        # Fazer um loop sobre os elementos de w\n",
    "        for j in range(n): \n",
    "            \n",
    "            dj_dw_j_reg = # Seu código aqui para calcular o termo de regularização para dj_dw[j]\n",
    "            \n",
    "            # Adicione o termo de regularização ao elemento correspondente de dj_dw\n",
    "            dj_dw[j] = dj_dw[j] + dj_dw_j_reg\n",
    "        \n",
    "        ### TERMINE SEU CÓDIGO AQUI ###\n",
    "        \n",
    "        return dj_db, dj_dw\n",
    "    ```\n",
    "  \n",
    "    Se ainda tiver dúvidas, consulte as dicas apresentadas abaixo para descobrir como calcular o `dj_dw_j_reg` \n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dicar para calcular dj_dw_j_reg</b></font></summary>\n",
    "           &emsp; &emsp; Você pode calcular dj_dw_j_reg com <code>dj_dw_j_reg = (lambda_ / m) * w[j] </code> \n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para verificar sua implementação da função `compute_gradient_reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1) \n",
    "initial_w  = np.random.rand(X_mapped.shape[1]) - 0.5 \n",
    "initial_b = 0.5\n",
    " \n",
    "lambda_ = 0.5\n",
    "dj_db, dj_dw = compute_gradient_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(f\"dj_db: {dj_db}\", )\n",
    "print(f\"Primeiros elementos de regularização dj_dw:\\n {dj_dw[:4].tolist()}\", )\n",
    "\n",
    "# TESTE DA UNIDADE    \n",
    "compute_gradient_reg_test(compute_gradient_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db:</b>0.07138288792343</td> </tr>\n",
    "  <tr>\n",
    "      <td> <b>Primeiros elementos de regularização dj_dw:</b> </td> </tr>\n",
    "   <tr>\n",
    "   <td> [[-0.010386028450548], [0.011409852883280], [0.0536273463274], [0.003140278267313]] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.6\"></a>\n",
    "### 3.6 Aprendendo parâmetros usando a descida gradiente\n",
    "\n",
    "Da mesma forma que nas partes anteriores, você usará a função de descida de gradiente implementada acima para aprender os parâmetros ideais $w$,$b$. \n",
    "- Se você tiver concluído corretamente o custo e o gradiente da regressão logística regularizada, deverá ser capaz de passar para a próxima célula para aprender os parâmetros $w$. \n",
    "- Depois de treinar nossos parâmetros, nós os usaremos para traçar o limite de decisão. \n",
    "\n",
    "**Nota**\n",
    "\n",
    "O bloco de código abaixo leva um bom tempo para ser executado, especialmente com uma versão não vetorizada. Você pode reduzir as `iterações` para testar sua implementação e iterar mais rapidamente. Se você tiver tempo mais tarde, execute 100.000 iterações para ver melhores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Inicialização dos parâmetros de ajuste\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
    "initial_b = 1.\n",
    "\n",
    "# Defina o parâmetro de regularização lambda_ (você pode tentar variar isso)\n",
    "lambda_ = 0.01    \n",
    "\n",
    "# Algumas configurações de descida de gradiente\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_mapped, y_train, initial_w, initial_b, \n",
    "                                    compute_cost_reg, compute_gradient_reg, \n",
    "                                    alpha, iterations, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>Saída Esperada: Custo < 0.5  (Clique para detalhes)</b>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "# Com as seguintes configurações\n",
    "#np.random.seed(1)\n",
    "#initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
    "#initial_b = 1.\n",
    "#lambda_ = 0.01;                                          \n",
    "#iterations = 10000\n",
    "#alpha = 0.01\n",
    "Iteração    0: Custo     0.72   \n",
    "Iteração 1000: Custo     0.59   \n",
    "Iteração 2000: Custo     0.56   \n",
    "Iteração 3000: Custo     0.53   \n",
    "Iteração 4000: Custo     0.51   \n",
    "Iteração 5000: Custo     0.50   \n",
    "Iteração 6000: Custo     0.48   \n",
    "Iteração 7000: Custo     0.47   \n",
    "Iteração 8000: Custo     0.46   \n",
    "Iteração 9000: Custo     0.45   \n",
    "Iteração 9999: Custo     0.45       \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.7\"></a>\n",
    "### 3.7 Plotar fronteira de decisão\n",
    "Para ajudá-lo a visualizar o modelo aprendido por esse classificador, usaremos a nossa função `plot_decision_boundary` que plota o limite de decisão (não linear) que separa os exemplos positivos dos negativos. \n",
    "\n",
    "- Na função, plotamos o limite de decisão não linear calculando as previsões do classificador em uma grade com espaçamento uniforme e, em seguida, desenhamos um gráfico de contorno de onde as previsões mudam de y = 0 para y = 1.\n",
    "\n",
    "- Depois de aprender os parâmetros $w$,$b$, a próxima etapa é traçar um limite de decisão semelhante ao da Figura 4.\n",
    "\n",
    "<img src=\"images/figure 4.png\"  width=\"450\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(w, b, X_mapped, y_train)\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Teste 2 de Microchip 2') \n",
    "# Set the x-axis label\n",
    "plt.xlabel('Teste 2 de Microchip 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8\"></a>\n",
    "### 3.8 Avaliação do modelo de regressão logística regularizada\n",
    "\n",
    "Você usará a função `predict` que implementou acima para calcular a precisão do modelo de regressão logística regularizada no conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Calcule a precisão no conjunto de treinamento\n",
    "p = predict(X_mapped, w, b)\n",
    "\n",
    "print('Acurácia de Treino: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Acurácia de Treino:</b>~ 80%</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parabéns por ter concluído esse laboratório! No próximo tópico da disciplina você usará algoritmos de aprendizado mais avançados: redes neurais!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
