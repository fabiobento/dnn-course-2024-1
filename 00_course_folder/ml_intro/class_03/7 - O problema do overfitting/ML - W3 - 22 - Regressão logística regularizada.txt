Neste vídeo, você vê como implementar a regressão logística regularizada.
Reproduza o vídeo começando em ::11 e siga a transcrição0:11
Assim como a atualização de gradiente para regressão logística parece surpreendentemente semelhante à atualização de gradiente para regressão linear, você descobre que a atualização de gradiente descendente para regressão logística regularizada também será semelhante à atualização para regressão linear regularizada. Vamos dar uma olhada. Aqui está a ideia. Vimos anteriormente que a regressão logística pode estar sujeita a ajustes excessivos se você ajustá-la com características polinomiais de ordem muito alta como essa. Aqui, z é um polinômio de alta ordem que é passado para a função sigmóide da mesma forma para calcular f. Em particular, você pode acabar com um limite de decisão que é excessivamente complexo e se sobrepõe ao conjunto de treinamento. De forma mais geral, quando você treina a regressão logística com muitos recursos, sejam recursos polinomiais ou outros recursos, pode haver um risco maior de sobreajuste. Essa foi a função de custo da regressão logística. Se você quiser modificá-lo para usar a regularização, tudo o que você precisa fazer é adicionar o seguinte termo. Vamos adicionar lambda ao parâmetro de regularização mais de 2m vezes a soma de j igual a 1 a n, onde n é o número de características, como de costume, de wj ao quadrado. Quando você minimiza essa função de custo em função de w e b, ela tem o efeito de penalizar os parâmetros w_1, w_2 a w_n e evitar que sejam muito grandes. Se você fizer isso, mesmo ajustando um polinômio de alta ordem com muitos parâmetros, você ainda terá um limite de decisão semelhante a este. Algo que parece mais razoável para separar exemplos positivos e negativos e, ao mesmo tempo, generalizar para novos exemplos que não estão no conjunto de treinamento. Ao usar a regularização, mesmo quando você tem muitos recursos. Como você pode realmente implementar isso? Como você pode realmente minimizar essa função de custo j de wb que inclui o termo de regularização? Bem, vamos usar o gradiente descendente como antes. Aqui está uma função de custo que você deseja minimizar. Para implementar o gradiente descendente, como antes, faremos as seguintes atualizações simultâneas em wj e b. Essas são as regras de atualização usuais para descida de gradiente. Assim como a regressão linear regularizada, quando você calcula onde existem esses termos derivados, a única coisa que muda agora é que a derivada em relação a wj recebe esse termo adicional, lambda sobre m vezes wj adicionado aqui no final. Novamente, parece muito com a atualização para regressão linear regularizada. Na verdade, é exatamente a mesma equação, exceto pelo fato de que a definição de f agora não é mais a função linear, é a função logística aplicada a z. Semelhante à regressão linear, regularizaremos apenas os parâmetros w, j, mas não o parâmetro b, e é por isso que a atualização que você fará para b. No último laboratório opcional desta semana, você revisa o sobreajuste. No gráfico interativo no laboratório opcional, agora você pode optar por regularizar seus modelos, tanto de regressão quanto de classificação, ativando a regularização durante a descida do gradiente selecionando um valor para lambda. Dê uma olhada no código para implementar a regressão logística regularizada em particular, porque você mesmo implementará isso no laboratório prático no final desta semana. Agora você sabe como implementar a regressão logística regularizada. Quando eu ando pelo Vale do Silício, há muitos engenheiros usando o aprendizado de máquina para criar muito valor, às vezes ganhando muito dinheiro para as empresas. Eu sei que você só estuda essas coisas há algumas semanas, mas se você entende e pode aplicar regressão linear e regressão logística, isso é tudo o que você precisa para criar alguns aplicativos muito valiosos. Embora os resultados de aprendizagem específicos que você usa sejam importantes, saber coisas como quando e como reduzir o sobreajuste também é uma das habilidades mais valiosas no mundo real. Quero parabenizar você pelo quão longe você chegou e quero dizer um ótimo trabalho por ter terminado até o final deste vídeo. Espero que você também resolva os laboratórios práticos e os questionários. Dito isso, ainda há muitas outras coisas interessantes para aprender. No segundo curso dessa especialização, você aprenderá sobre redes neurais, também chamadas de algoritmos de aprendizado profundo. As redes neurais são responsáveis por muitas das mais recentes descobertas dos olhos atualmente, desde o reconhecimento prático de fala até computadores que reconhecem objetos e imagens com precisão e carros autônomos. A forma como a rede neural é construída, na verdade, usa muito do que você já aprendeu, como funções de custo, gradiente descendente e funções sigmóides. Mais uma vez, parabéns por chegar ao final desta terceira e última semana do Curso 1. Espero que você tenha [inaudível] e nos vemos no material da próxima semana sobre redes neurais.
