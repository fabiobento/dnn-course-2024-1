Neste vídeo, descobriremos como fazer com que o gradiente descendente funcione com regressão linear regularizada. Vamos entrar. Aqui está uma função de custo que criamos no último vídeo para regressão linear regularizada. A primeira parte é a função de custo de erro quadrático usual, e agora você tem esse termo de regularização adicional, em que Lambda é o parâmetro de regularização, e gostaria de encontrar os parâmetros w e b que minimizem a função de custo regularizada.
Reproduza o vídeo começando em ::34 e siga a transcrição0:34
Anteriormente, usávamos gradiente descendente para a função de custo original, apenas o primeiro termo antes de adicionarmos o segundo termo de regularização e, anteriormente, tínhamos o seguinte algoritmo de descida de gradiente, ou seja, atualizamos repetidamente os parâmetros w, j e b para j é igual a 1 a n de acordo com essa fórmula e b também é atualizado de forma semelhante. Novamente, Alpha é um número positivo muito pequeno chamado taxa de aprendizado. Na verdade, as atualizações para uma regressão linear regularizada parecem exatamente as mesmas, exceto que agora o custo, J, é definido de forma um pouco diferente. Anteriormente, a derivada de J em relação a w_j era dada por essa expressão aqui, e a derivada em relação a b era dada por essa expressão aqui.
Reproduza o vídeo começando em :1:30 e siga a transcrição1:30
Agora que adicionamos esse termo adicional de regularização, a única coisa que muda é que a expressão da derivada em relação a w_j termina com um termo adicional, isso mais Lambda sobre m vezes w_j. E, em particular, para a nova definição da função de custo j, essas duas expressões aqui, essas são as novas derivadas de J em relação a w_j e a derivada de J em relação a b. Lembre-se disso não regularizamos b, então não estamos tentando reduzir B. É por isso que o B atualizado permanece o mesmo que antes, enquanto o w atualizado muda porque o termo de regularização faz com que tentemos reduzir w_j. Vamos pegar essas definições para as derivadas e colocá-las de volta na expressão à esquerda para escrever o algoritmo de gradiente descendente para regressão linear regularizada. Para implementar a descida de gradiente para regressão linear regularizada, isso é o que você faria com que seu código fizesse. Aqui está a atualização para w_j, para j é igual a 1 a n, e aqui está a atualização para b. Como de costume, lembre-se de realizar atualizações simultâneas para todos esses parâmetros. Agora, para que esse algoritmo funcione, isso é tudo que você precisa saber. Mas o que eu gostaria de fazer no restante deste vídeo é revisar algum material opcional para transmitir uma intuição um pouco mais profunda sobre o que essa fórmula está realmente fazendo, bem como conversar brevemente sobre como esses derivados são derivados. O resto deste vídeo é totalmente opcional. Tudo bem se você pular o resto deste vídeo, mas se você tem um grande interesse em matemática, fique comigo. É sempre bom estar com você aqui e, por meio dessas equações, talvez você possa construir uma intuição mais profunda sobre o que a matemática e os derivados também estão fazendo. Vamos dar uma olhada. Vamos examinar a regra de atualização para w_j e reescrevê-la de outra forma. Estamos atualizando w_j como 1 vezes w_j menos Alpha vezes Lambda em vez de m vezes w_j. Mudei o termo do final para a frente aqui. Então menos Alpha vezes 1 sobre m, e então o resto desse termo ali. Acabamos de reorganizar um pouco os termos. Se simplificarmos, estamos dizendo que w_j é atualizado como w_j vezes 1 menos Alpha vezes Lambda sobre m, menos Alpha vezes esse outro termo aqui. Você pode reconhecer o segundo termo como a atualização usual de gradiente descendente para regressão linear não regularizada. Essa é a atualização da regressão linear antes da regularização, e esse é o termo que vimos na semana 2 deste curso. A única alteração que adicionamos à regularização é que, em vez de w_j ser definido como igual a w_j menos Alpha vezes, esse termo agora é w vezes esse número menos a atualização usual. Isso é o que tivemos na semana 1 deste curso. Qual é esse primeiro termo aqui? Bem, Alpha é um número positivo muito pequeno, digamos 0,01. Lambda geralmente é um número pequeno, digamos 1 ou talvez 10. Digamos que lambda seja 1 neste exemplo e m seja o tamanho do conjunto de treinamento, digamos 50.
Reproduza o vídeo começando em :5:22 e siga a transcrição5:22
Quando você multiplica Alpha Lambda por m, digamos 0,01 vezes 1 dividido por 50, esse termo acaba sendo um pequeno número positivo , digamos 0,0002 e, portanto, 1 menos Alpha Lambda sobre m será um número um pouco menor que 1, neste caso, 0,9998.
Reproduza o vídeo começando em :5:46 e siga a transcrição5:46
O efeito desse termo é que, em cada iteração de gradiente descendente, você pega w_j e o multiplica por 0,9998, ou seja, por alguns números um pouco menores que um, e para realizar a atualização usual. O que a regularização está fazendo em cada iteração é multiplicar w por um número um pouco menor que 1, e isso tem o efeito de reduzir um pouco o valor de w_j. Isso nos dá outra visão de por que a regularização tem o efeito de reduzir um pouco os parâmetros w_j em cada iteração, e é assim que a regularização funciona. Se você está curioso sobre como esses termos derivados foram calculados, tenho apenas um último slide opcional que mostra apenas um pequeno cálculo do termo derivado. Novamente, este slide e o restante deste vídeo são totalmente opcionais, o que significa que você não precisará de nada disso para fazer os laboratórios práticos e os questionários. Vamos passar rapidamente para o cálculo de derivadas. A derivada de J em relação a w_j se parece com isso.
Reproduza o vídeo começando em :7:10 e siga a transcrição7:10
Lembre-se de que f de x para regressão linear é definido como w ponto x mais b ou w ponto produto x mais b. Acontece que, pelas regras do cálculo, as derivadas são assim: 1 sobre 2m vezes a soma i é igual a 1 a m de w ponto x mais b menos y vezes 2x_j mais a derivada do termo de regularização, que é Lambda acima de 2m vezes 2 w_j Observe que o segundo termo não tem mais a soma do termo de j igual a 1 a n. Os 2 são cancelados aqui e aqui, e também aqui e aqui. Isso simplifica essa expressão aqui.
Reproduza o vídeo começando em :8:7 e siga a transcrição8:07
Finalmente, lembre-se de que wx mais b é f de x, então você pode reescrevê-lo como essa expressão aqui embaixo. É por isso que essa expressão é usada para calcular o gradiente na regressão linear regularizada. Agora você sabe como implementar a regressão linear regularizada. Com isso, você realmente reduz o sobreajuste quando tem muitos recursos e um conjunto de treinamento relativamente pequeno. Isso deve permitir que a regressão linear funcione muito melhor em muitos problemas. No próximo vídeo, pegaremos essa ideia de regularização e a aplicaremos à regressão logística para evitar também o ajuste excessivo para a regressão logística. Vamos dar uma olhada nisso no próximo vídeo.
