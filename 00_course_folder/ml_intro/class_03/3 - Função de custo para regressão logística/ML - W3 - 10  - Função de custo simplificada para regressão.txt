No último vídeo, você viu a função de perda e a função de custo para regressão logística. Neste vídeo, você verá uma maneira um pouco mais simples de escrever as funções de perda e custo, de forma que a implementação possa ser um pouco mais simples quando chegarmos ao gradiente descendente para ajustar os parâmetros de um modelo de regressão logística. Vamos dar uma olhada. Como lembrete, aqui está a função de perda que definimos no vídeo anterior para regressão logística. Como ainda estamos trabalhando em um problema de classificação binária, y é zero ou um. Como y é zero ou um e não pode assumir nenhum valor diferente de zero ou um, poderemos criar uma maneira mais simples de escrever essa função de perda. Você pode escrever a função de perda da seguinte forma. Dada uma previsão f de x e o rótulo alvo y, a perda é igual a menos y vezes log de f menos 1 menos y vezes log de 1 menos f. Acontece que essa equação, que acabamos de escrever em uma linha, é completamente equivalente a essa fórmula mais complexa aqui em cima. Vamos ver por que esse é o caso. Lembre-se de que y só pode assumir os valores de um ou zero. No primeiro caso, digamos que y seja igual a 1. Esse primeiro y aqui é um e esse 1 menos y é 1 menos 1, o que, portanto, é igual a 0. Assim, a perda se torna negativa 1 vezes log de f de x menos 0 vezes um monte de coisas. Isso se torna zero e desaparece. Quando y é igual a 1, a perda é de fato o primeiro termo no topo, logaritmo negativo de f de x. Vejamos o segundo caso, quando y é igual a 0. Nesse caso, esse y aqui é igual a 0, então esse primeiro termo desaparece e o segundo termo é 1 menos 0 vezes esse termo logarítmico. A perda se torna menos 1 vezes log de 1 menos f de x. Isso é exatamente igual a esse segundo termo aqui em cima. No caso de y igual a 0, também recuperamos a função de perda original, conforme definido acima. O que você vê é que, se y é um ou zero, essa única expressão aqui é equivalente à expressão mais complexa aqui em cima, e é por isso que isso nos dá uma maneira mais simples de escrever a perda com apenas uma equação sem separar esses dois casos, como fizemos no topo. Usando essa função de perda simplificada, vamos voltar e escrever a função de custo para regressão logística. Aqui está novamente a função de perda simplificada. Lembre-se de que o custo J é apenas a perda média, média de todo o conjunto de treinamento de m exemplos. Então, é 1 sobre n vezes a soma da perda de i igual a 1 a m. Se você inserir a definição da perda simplificada acima, então fica assim: 1 sobre m vezes a soma do termo acima. Se você trouxer os sinais negativos e movê-los para fora , acabará com essa expressão aqui, e essa é a função de custo. A função de custo que quase todo mundo usa para treinar a regressão logística. Você pode estar se perguntando: por que escolhemos essa função específica quando poderia haver muitas outras funções de custos que poderíamos ter escolhido? Embora não tenhamos tempo para entrar em grandes detalhes sobre isso nesta aula, gostaria de mencionar que essa função de custo específica é derivada de estatísticas usando um princípio estatístico chamado estimativa de máxima verossimilhança, que é uma ideia da estatística sobre como encontrar parâmetros de forma eficiente para diferentes modelos. Essa função de custo tem a boa propriedade de ser convexa. Mas não se preocupe em aprender os detalhes da máxima verossimilhança. É apenas uma justificativa e uma justificativa mais profundas por trás dessa função de custo específica. O próximo laboratório opcional mostrará como a função de custo logístico é implementada no código. Eu recomendo dar uma olhada, porque você implementa isso mais tarde no laboratório prático no final da semana. Este próximo laboratório opcional também mostra como duas escolhas diferentes dos parâmetros levarão a cálculos de custo diferentes. Você pode ver no gráfico que o limite de decisão azul mais adequado tem um custo menor em relação ao limite de decisão magenta. Portanto, com a função de custo simplificada, agora estamos prontos para começar a aplicar o gradiente descendente à regressão logística. Vamos ver isso no próximo vídeo.
(Obrigatória)
pt-BR
​

