Agora, vamos mergulhar mais profundamente na descida de gradiente para ter uma melhor intuição sobre o que ela está fazendo e por que ela pode fazer sentido. Aqui está o algoritmo de gradiente descendente que você viu no vídeo anterior. Como lembrete, essa variável, esse símbolo grego Alpha, é a taxa de aprendizado. A taxa de aprendizado controla o tamanho do passo que você dá ao atualizar os parâmetros do modelo, w e b. Esse termo aqui, esse d sobre dw, é um termo derivado. Por convenção em matemática, este d é escrito com esta fonte engraçada aqui.
Reproduza o vídeo começando em ::42 e siga a transcrição0:42
Caso alguém que esteja assistindo isso tenha doutorado em matemática ou seja especialista em cálculo multivariado, pode estar se perguntando: essa não é a derivada, é a derivada parcial. Sim, eles estão certos. Mas, para fins de implementação de um algoritmo de aprendizado de máquina, vou chamá-lo de derivado. Não se preocupe com essas pequenas distinções. O que vamos focar agora é ter mais intuição sobre o que essa taxa de aprendizado e o que essa derivada estão fazendo e por que, quando multiplicada dessa forma, ela resulta em atualizações nos parâmetros w e b. Isso faz sentido. Para fazer isso, vamos usar um exemplo um pouco mais simples em que trabalhamos para minimizar apenas um parâmetro. Digamos que você tenha uma função de custo J de apenas um parâmetro w com w sendo um número. Isso significa que a descida do gradiente agora se parece com isso. W é atualizado para w menos a taxa de aprendizado Alpha vezes d sobre dw de J de w. Você está tentando minimizar o custo ajustando o parâmetro w. É como no exemplo anterior em que definimos temporariamente b igual a 0 com um parâmetro w em vez de dois. Você pode ver gráficos bidimensionais da função de custo j, em vez de gráficos tridimensionais. Vamos ver o que o gradiente descendente faz apenas na função J de w. Aqui no eixo horizontal está o parâmetro w, e no eixo vertical está o custo j de w. Agora, menos gradiente descendente inicializado com algum valor inicial para w. Vamos inicializá-lo neste local. Imagine que você comece neste ponto aqui mesmo na função J, o que o gradiente descendente fará é atualizar w para w menos a taxa de aprendizado Alpha vezes d sobre dw de J de w. Vamos ver o que esse termo derivado aqui significa. Uma forma de pensar sobre a derivada nesse ponto da reta é desenhar uma linha tangente, que é uma linha reta que toca essa curva naquele ponto. Basta, a inclinação dessa linha é a derivada da função j neste ponto. Para obter a inclinação, você pode desenhar um pequeno triângulo como este. Se você calcular a altura dividida pela largura desse triângulo, essa é a inclinação. Por exemplo, essa inclinação pode ser 2 sobre 1, por exemplo, e quando a linha tangente está apontando para cima e para a direita, a inclinação é positiva, o que significa que essa derivada é um número positivo, então é maior que 0. O w atualizado será w menos a taxa de aprendizado multiplicado por algum número positivo. A taxa de aprendizado é sempre um número positivo. Se você pegar w menos um número positivo, você acaba com um novo valor para w, que é menor. No gráfico, você está se movendo para a esquerda, está diminuindo o valor de w. Você pode notar que essa é a coisa certa a fazer se sua meta é diminuir o custo J, porque quando nos movemos para a esquerda nessa curva, o custo j diminui e você está se aproximando do mínimo para J, que está aqui. Até agora, o gradiente descendente parece estar fazendo a coisa certa. Agora, vamos ver outro exemplo. Vamos usar a mesma função j de w acima, e agora digamos que você inicializou o gradiente descendente em um local diferente. Digamos escolhendo um valor inicial para w que está aqui à esquerda. Esse é esse ponto da função j. Agora, o termo derivado, lembre-se de que é d sobre dw de J de w, e quando olhamos para a reta tangente neste ponto, a inclinação dessa linha é uma derivada de J neste ponto. Mas essa linha tangente está inclinada para a direita. Essa linha inclinada para a direita tem uma inclinação negativa. Em outras palavras, a derivada de J neste ponto é um número negativo. Por exemplo, se você desenhar um triângulo, a altura como essa é menos 2 e a largura é 1, a inclinação é menos 2 dividida por 1, que é menos 2, que é um número negativo. Quando você atualiza w, você obtém w menos a taxa de aprendizado multiplicado por um número negativo. Isso significa que você subtrai de w, um número negativo. Mas subtrair um número negativo significa adicionar um número positivo e, portanto, você acaba aumentando w. Porque subtrair um número negativo é o mesmo que adicionar um número positivo a w. Essa etapa de declive faz com que w aumente, o que significa que você está se movendo para a direita do gráfico e seu custo J diminuiu até aqui. Novamente, parece que o gradiente descendente está fazendo algo razoável, está aproximando você do mínimo. Espero que esses dois últimos exemplos mostrem um pouco da intuição por trás do que um termo derivado está fazendo e por que essa descida do gradiente do hospedeiro muda w para aproximá-lo do mínimo. Espero que este vídeo tenha dado uma ideia de por que o termo derivado em gradiente descendente faz sentido. Uma outra grandeza importante no algoritmo de gradiente descendente é a taxa de aprendizado Alpha. Como você escolhe o Alpha? O que acontece se for muito pequeno ou o que acontece quando é muito grande? No próximo vídeo, vamos dar uma olhada mais profunda no parâmetro Alpha para ajudar a criar intuições sobre o que ele faz, bem como sobre como fazer uma boa escolha para um bom valor de Alpha para sua implementação de gradiente descendente.
