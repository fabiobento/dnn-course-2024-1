0:02
Vamos ver o que acontece quando você executa a descida de gradiente para regressão linear. Vamos ver o algoritmo em ação. Aqui está um gráfico do modelo e dos dados no canto superior esquerdo e um gráfico de contorno da função de custo no canto superior direito e na parte inferior está o gráfico de superfície da mesma função de custo. Freqüentemente, w e b serão inicializados com 0, mas para esta demonstração, vamos inicializar w = -0,1 e b = 900. Então, isso corresponde a f (x) = -0,1x + 900.
Reproduza o vídeo começando em ::44 e siga a transcrição0:44
Agora, se dermos um passo usando gradiente descendente, acabamos indo desse ponto da função de custo aqui até este ponto logo abaixo e para a direita e percebemos que o ajuste da linha reta também muda um pouco.
Reproduza o vídeo começando em :1:4 e siga a transcrição1:04
Vamos dar outro passo.
Reproduza o vídeo começando em :1:6 e siga a transcrição1:06
A função de custo agora foi movida para esta terceira e, novamente, a função f (x) também mudou um pouco.
Reproduza o vídeo começando em :1:15 e siga a transcrição1:15
À medida que você executa mais dessas etapas, o custo diminui a cada atualização. Portanto, os parâmetros w e b estão seguindo essa trajetória.
Reproduza o vídeo começando em :1:28 e siga a transcrição1:28
E se você olhar para a esquerda, obterá esse ajuste de linha reta correspondente que se ajusta cada vez melhor aos dados até atingirmos o mínimo global. O mínimo global corresponde a esse ajuste em linha reta, que é um ajuste relativamente bom aos dados. Quero dizer, isso não é legal. Então isso é gradiente descendente e vamos usá-lo para ajustar um modelo aos dados de retenção. E agora você pode usar esse modelo f (x) para prever o preço da casa do seu cliente ou da casa de qualquer outra pessoa. Por exemplo, se o tamanho da casa do seu amigo é de 1250 pés quadrados, agora você pode ler o valor e prever que talvez ele possa receber, sei lá, $250.000 pela casa. Para ser mais preciso, esse processo de descida de gradiente é chamado de descida de gradiente em lote. O termo descida gradual se refere ao fato de que, em cada etapa da descida do gradiente, estamos analisando todos os exemplos de treinamento, em vez de apenas um subconjunto dos dados de treinamento.
Reproduza o vídeo começando em :2:46 e siga a transcrição2:46
Assim, ao calcular a gradação descendente, ao calcular derivadas, ao calcular a soma de i = 1 a m. E a descida do gradiente bash é analisar todo o lote de exemplos de treinamento em cada atualização. Sei que a porcentagem de avaliação do bash pode não ser o nome mais intuitivo, mas é assim que as pessoas da comunidade de aprendizado de máquina a chamam. Se você já ouviu falar do boletim informativo The Batch, ele foi publicado pela DeepLearning.AI. O boletim informativo The batch também recebeu esse nome em homenagem a esse conceito em aprendizado de máquina.
Reproduza o vídeo começando em :3:18 e siga a transcrição3:18
E então acontece que existem outras versões do gradiente descendente que não analisam todo o conjunto de treinamento, mas analisam subconjuntos menores dos dados de treinamento em cada etapa de atualização. Mas usaremos o gradiente descendente em lote para regressão linear.
Reproduza o vídeo começando em :3:34 e siga a transcrição3:34
Então é isso para regressão linear. Parabéns por concluir seu primeiro modelo de aprendizado de máquina. Espero que você vá comemorar ou, sei lá, talvez tire uma soneca na sua rede. No laboratório opcional que segue este vídeo. Você verá uma análise do algoritmo de gradiente descendente e mostrará como implementá-lo no código. Você também verá um gráfico que mostra como o custo diminui à medida que você continua treinando mais iterações. E você também verá um gráfico de contorno, vendo como o custo se aproxima do mínimo global à medida que o gradiente descendente encontra valores cada vez melhores para os parâmetros w e b. Então lembre-se disso para fazer o laboratório opcional. Você só precisa ler e executar esse código. Você precisará escrever qualquer código sozinho e espero que dedique alguns minutos para fazer isso. E também se familiarize com o código de gradiente descendente, pois isso o ajudará a implementar esse e outros algoritmos similares no futuro.
Reproduza o vídeo começando em :4:36 e siga a transcrição4:36
Obrigado por ficar comigo até o final deste último vídeo da primeira semana e parabéns por ter feito isso até aqui. Você está prestes a se tornar uma pessoa que gosta de aprendizado de máquina. Além dos laboratórios opcionais, se você ainda não o fez. Espero que você também confira os questionários práticos, que são uma boa maneira de verificar sua própria compreensão dos conceitos. Também está tudo bem, se você não acertar na primeira vez. E você também pode responder aos questionários várias vezes até obter a pontuação desejada. Agora você sabe como implementar a regressão linear com uma variável e isso nos leva ao final desta semana. Na próxima semana, aprenderemos a tornar a regressão linear muito mais poderosa em vez de um recurso, como o tamanho de uma casa. Você aprenderá como fazê-la funcionar com muitos recursos. Você também aprenderá como fazer com que ele se ajuste a curvas não lineares. Essas melhorias tornarão o algoritmo muito mais útil e valioso. Por fim, também abordaremos algumas dicas práticas que realmente ajudarão a fazer com que a regressão linear funcione em aplicações práticas. Estou muito feliz de ter você aqui comigo nesta aula e espero vê-lo na próxima semana.
Mas usaremos o gradiente descendente em lote para regressão linear.: adicionado à seleção. Pressione [CTRL + S] para salvar como anotação
(Obrigatória)
pt-BR
​

