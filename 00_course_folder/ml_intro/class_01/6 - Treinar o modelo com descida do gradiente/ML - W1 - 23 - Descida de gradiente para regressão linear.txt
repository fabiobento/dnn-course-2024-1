Anteriormente, você examinou o modelo de regressão linear e, em seguida, a função de custo e, em seguida, o algoritmo de gradiente descendente. Neste vídeo, vamos reunir e usar a função de custo de erro quadrático para o modelo de regressão linear com gradiente descendente. Isso nos permitirá treinar o modelo de regressão linear para ajustar uma linha reta para obter os dados de treinamento. Vamos direto ao assunto. Aqui está o modelo de regressão linear. À direita está a função quadrada do custo do erro. Abaixo está o algoritmo de gradiente descendente. Acontece que, se você calcular esses derivativos, esses são os termos que você obteria. A derivada em relação a W é 1 sobre m, soma de i é igual a 1 a m. Então, o termo de erro, que é a diferença entre os valores previstos e reais vezes a característica de entrada xi. A derivada em relação a b é essa fórmula aqui, que parece a mesma da equação acima, exceto que ela não tem esse termo xi no final. Se você usar essas fórmulas para calcular essas duas derivadas e implementar o gradiente descendente dessa forma, isso funcionará. Agora, você pode estar se perguntando, de onde eu tirei essas fórmulas? Eles são derivados usando cálculo. Se você quiser ver a derivação completa, examinarei rapidamente a derivação no próximo slide. Mas se você não se lembra ou não está interessado no cálculo, não se preocupe. Você pode pular completamente os materiais do próximo slide e ainda ser capaz de implementar a descida de gradiente e finalizar esta aula e tudo funcionará bem. Neste slide, que é um dos slides mais matemáticos de toda a especialização e, novamente, totalmente opcional, mostraremos como calcular os termos derivados. Vamos começar com o primeiro termo. A derivada da função de custo J em relação a w. Começaremos inserindo a definição da função de custo J. J do WP é essa. 1 sobre 2 milhões de vezes essa soma dos termos de erro quadrados. Agora lembre-se também de que f de wb de X^i é igual a esse termo aqui, que é Wx^i mais b. O que gostaríamos de fazer é calcular a derivada, também chamada de derivada parcial em relação a w dessa equação aqui à direita. Se você fez uma aula de cálculo antes e, novamente, está tudo bem se não fez, você deve saber que, pelas regras do cálculo, a derivada é igual a esse termo aqui. É por isso que os dois aqui e os dois aqui se cancelam, deixando-nos com essa equação que você viu no slide anterior. É por isso que tivemos que encontrar a função de custo com o 1.5 no início desta semana, porque isso torna a derivada parcial mais organizada. Ele cancela os dois que aparecem ao calcular a derivada. Para a outra derivada em relação a b, isso é bastante semelhante. Eu posso escrever assim e, mais uma vez, inserir a definição de f de x^i, dando essa equação. Pelas regras do cálculo, isso é igual a isso em que não há mais X^i no final. Os 2 cancelam uma pequena e você acaba com essa expressão para a derivada em relação a b. Agora você tem essas duas expressões para as derivadas. Você pode conectá-los ao algoritmo de descida de gradiente. Aqui está o algoritmo de gradiente descendente para regressão linear. Você realiza repetidamente essas atualizações em w e b até a convergência. Lembre-se de que esse f de x é um modelo de regressão linear, portanto igual a w vezes x mais b. Essa expressão aqui é a derivada da função de custo em relação a w. Essa expressão é a derivada da função de custo em relação a b. Apenas como lembrete, você deseja atualizar w e b simultaneamente em cada etapa. Agora, vamos nos familiarizar com o funcionamento do gradiente descendente. Um dos sapatos que vimos com o gradiente descendente é que ele pode levar a um mínimo local em vez de um mínimo global. Se mínimo global significa o ponto que tem o menor valor possível para a função de custo J de todos os pontos possíveis. Você deve se lembrar deste terreno de superfície que parece um parque ao ar livre com algumas colinas com o processo e os pássaros como um relaxante Hobo Hill. Essa função tem mais de um mínimo local. Lembre-se de que, dependendo de onde você inicializa os parâmetros w e b, você pode acabar em mínimos locais diferentes. Você pode acabar aqui, ou você pode acabar aqui. Mas acontece que quando você está usando uma função de custo de erro quadrado com regressão linear, a função de custo não tem e nunca terá vários mínimos locais. Ele tem um único mínimo global por causa desse formato de tigela. O termo técnico para isso é que essa função de custo é uma função convexa. Informalmente, uma função convexa tem uma função em forma de tigela e não pode ter nenhum mínimo local além do mínimo global único. Quando você implementa o gradiente descendente em uma função convexa, uma boa propriedade é que, desde que sua taxa de aprendizado seja escolhida adequadamente, ela sempre convergirá para o mínimo global. Parabéns, agora você sabe como implementar o gradiente descendente para regressão linear. Temos apenas um último vídeo para esta semana. Nesse vídeo, veremos esse algoritmo em ação. Vamos ao último vídeo.
