Bem vindo de volta. No último vídeo, vimos visualizações da função de custo j e como você pode tentar diferentes opções dos parâmetros w e b e ver qual valor de custo eles oferecem. Seria bom se tivéssemos uma forma mais sistemática de encontrar os valores de w e b, que resultasse no menor custo possível, j de w, b. Acontece que há um algoritmo chamado gradiente descendente que você pode usar para fazer isso. O gradiente descendente é usado em todos os lugares no aprendizado de máquina, não apenas para regressão linear, mas para treinar, por exemplo, alguns dos modelos de rede neural mais avançados, também chamados de modelos de aprendizado profundo. Os modelos de aprendizado profundo são algo que você aprendeu no segundo curso. Aprender esses dois de gradiente descendente preparará você com um dos blocos de construção mais importantes do aprendizado de máquina. Aqui está uma visão geral do que faremos com o gradiente descendente. Você tem a função de custo j de w, b aqui que deseja minimizar. No exemplo que vimos até agora, essa é uma função de custo para regressão linear, mas acontece que o gradiente descendente é um algoritmo que você pode usar para tentar minimizar qualquer função, não apenas uma função de custo para regressão linear. Só para tornar essa discussão sobre gradiente descendente mais geral, verifica-se que a descida de gradiente se aplica a funções mais gerais, incluindo outras funções de custo que funcionam com modelos que têm mais de dois parâmetros.
Reproduza o vídeo começando em :1:40 e siga a transcrição1:40
Por exemplo, se você tem uma função de custo J em função de w_1, w_2 até w_n e b, seu objetivo é minimizar j sobre os parâmetros w_1 a w_n e b. Em outras palavras, você quer escolher valores de w_1 até w_n e b, o que lhe dá o menor valor possível de j. Acontece que o gradiente descendente é um algoritmo que você pode aplicar para tentar para minimizar essa função de custo j também. O que você vai fazer é começar com algumas suposições iniciais para w e b. Na regressão linear, não importará muito quais são os valores iniciais, então uma escolha comum é definir ambos como 0. Por exemplo, você pode definir w como 0 e b como 0 como estimativa inicial. Com o algoritmo de gradiente descendente, o que você vai fazer é continuar alterando os parâmetros w e b um pouco toda vez para tentar reduzir o custo j de w, b até que j se estabeleça no mínimo ou próximo dele. Uma coisa que devo observar é que, para algumas funções j que podem não ser em forma de arco ou rede, é possível que haja mais de um mínimo possível. Vamos dar uma olhada em um exemplo de um gráfico de superfície j mais complexo para ver o que o gradiente está fazendo. Essa função não é uma função de custo de erro quadrático. Para regressão linear com a função de custo de erro quadrado, você sempre acaba com uma forma de arco ou uma forma de rede. Mas esse é um tipo de função de custo que você pode obter se estiver treinando um modelo de rede neural. Observe os eixos, ou seja, w e b no eixo inferior. Para valores diferentes de w e b, você obtém pontos diferentes nessa superfície, j de w, b, onde a altura da superfície em algum ponto é o valor da função de custo. Agora, vamos imaginar que esse terreno é na verdade uma vista de um parque ao ar livre ligeiramente montanhoso ou de um campo de golfe onde os pontos altos são colinas e os pontos baixos são vales como esse. Eu gostaria que você imaginasse, se quisesse, que você está fisicamente parado neste ponto da colina. Se isso te ajudar a relaxar, imagine que há muita grama verde muito bonita e borboletas e flores é uma colina muito bonita. Seu objetivo é começar aqui e chegar ao fundo de um desses vales da forma mais eficiente possível. O que o algoritmo de gradiente descendente faz é girar em torno de 360 graus , olhar em volta e se perguntar se eu desse um pequeno passo em uma direção e quero descer a ladeira o mais rápido possível até um desses vales. Que direção eu escolho para dar esse pequeno passo? Bem, se você quiser descer essa colina da forma mais eficiente possível, acontece que se você estiver neste ponto da colina e olhar em volta, notará que a melhor direção para dar o próximo passo é aproximadamente nessa direção. Matematicamente, essa é a direção da descida mais íngreme. Isso significa que quando você dá um pequeno passo, isso o leva ladeira abaixo mais rápido do que um pequeno passo que você poderia ter dado em qualquer outra direção. Depois de dar o primeiro passo, você está agora neste ponto da colina aqui. Agora vamos repetir o processo. Nesse novo ponto, você girará novamente em torno de 360 graus e se perguntará: em que direção darei o próximo pequeno passo para descer a ladeira? Se você fizer isso e der outro passo, acabará se movendo um pouco nessa direção e poderá continuar. A partir desse novo ponto, você pode novamente olhar em volta e decidir em qual direção você desceria a ladeira mais rapidamente. Dê outro passo, outro passo, e assim por diante, até se encontrar no fundo deste vale, nesse mínimo local, aqui mesmo. O que você acabou de fazer foi passar por várias etapas de gradiente descendente. Acontece que o gradiente descendente tem uma propriedade interessante. Lembre-se de que você pode escolher um ponto de partida na superfície escolhendo valores iniciais para os parâmetros w e b. Quando você executa a descida de gradiente há pouco, você começou neste ponto aqui. Agora, imagine se você tentasse o gradiente descendente novamente, mas desta vez escolhesse um ponto de partida diferente escolhendo parâmetros que coloquem seu ponto de partida apenas alguns passos à direita aqui. Se você repetir o processo de descida do gradiente, o que significa que você olha em volta, dê um pequeno passo na direção da subida mais íngreme para chegar aqui. Então você olha novamente em volta, dá outro passo e assim por diante. Se você fizesse uma descida de gradiente pela segunda vez, começando apenas alguns passos à direita de onde fizemos isso na primeira vez , você acabaria em um vale totalmente diferente. Esse mínimo diferente aqui à direita. Os fundos do primeiro e do segundo vales são chamados de mínimos locais. Porque se você começar a descer o primeiro vale, a descida em gradiente não o levará ao segundo vale, e o mesmo acontece se você começar a descer o segundo vale, você permanece no segundo mínimo e não encontra o caminho para o primeiro mínimo local. Essa é uma propriedade interessante do algoritmo de gradiente descendente, e você verá mais sobre isso posteriormente. Neste vídeo, você viu como a descida do gradiente ajuda você a descer ladeiras. No próximo vídeo, vamos ver as expressões matemáticas que você pode implementar para fazer o gradiente descendente funcionar. Vamos para o próximo vídeo.
