A escolha da taxa de aprendizado, alfa, terá um grande impacto na eficiência de sua implementação do gradiente descendente. E se for alfa, a taxa de aprendizado for escolhida, a baixa taxa de descida pode nem mesmo funcionar. Neste vídeo, vamos dar uma olhada mais profunda na taxa de aprendizado. Isso também ajudará você a escolher melhores taxas de aprendizado para suas implementações de gradiente descendente. Então, aqui, novamente, está a regra de descida do gradiente. W é atualizado para ser W menos a taxa de aprendizado, alfa vezes o termo derivado. Para saber mais sobre o que a taxa de aprendizado alfa está fazendo. Vamos ver o que pode acontecer se a taxa de aprendizado alfa for muito pequena ou muito grande. Para o caso em que a taxa de aprendizado é muito pequena. Aqui está um gráfico em que o eixo horizontal é W e o eixo vertical é o custo J. E aqui está o gráfico da função J de W. Vamos começar a graduar a descida neste ponto aqui, se a taxa de aprendizado for muito pequena. Então, o que acontece é que você multiplica seu termo derivado por algum número muito, muito pequeno. Então você vai multiplicar pelo número alfa. Isso é muito pequeno, como 0,0000001. E então você acaba dando um pequeno passo como esse. Então, a partir desse ponto, você vai dar outro pequeno passo de bebê. Mas como a taxa de aprendizado é muito pequena, a segunda etapa também é minúscula. O resultado desse processo é que você acaba diminuindo o custo J, mas de forma incrivelmente lenta. Então, aqui está outra etapa e outra etapa, outra pequena etapa até você finalmente se aproximar do mínimo. Mas, como você pode notar, você precisará de várias etapas para chegar ao mínimo. Então, para resumir, se a taxa de aprendizado for muito pequena, as descidas do gradiente funcionarão, mas serão lentas. Vai levar muito tempo porque serão necessários esses pequenos passos. E precisará de várias etapas antes de chegar perto do mínimo. Agora, vamos ver um caso diferente. O que acontece se a taxa de aprendizado for muito grande? Aqui está outro gráfico da função de custo. E digamos que comecemos a gradar a descida com W nesse valor aqui. Então, na verdade, já está bem próximo do mínimo. Então, a decoração aponta para a direita. Mas se a taxa de aprendizado for muito grande, você atualiza um passo muito grande para chegar até aqui. E esse é esse ponto aqui na função J. Então você se move desse ponto à esquerda, até esse ponto à direita. E agora o custo realmente piorou. Ele aumentou porque começou com esse valor aqui e depois de uma etapa, na verdade aumentou para esse valor aqui. Agora, a derivada neste novo ponto diz diminuir W, mas quando a taxa de aprendizado é muito grande. Então você pode dar um grande passo indo daqui até aqui. Então, agora você chegou a esse ponto várias vezes, se a taxa de aprendizado for muito grande. Em seguida, você dá outro grande passo com uma aceleração e supera o mínimo novamente. Então agora você está neste ponto à direita e mais uma vez você faz outra atualização. E acabe até aqui, então você está agora neste ponto aqui. Então, como você pode notar, você está realmente se afastando cada vez mais do mínimo. Portanto, se a taxa de aprendizado for muito grande, a criação do sentido pode ser exagerada e pode nunca atingir o mínimo. E outra forma de dizer isso é que uma grande interseção pode falhar em convergir e pode até mesmo divergir. Então, aqui está outra pergunta, você pode estar se perguntando que um dos seus parâmetros W já está neste momento aqui. Para que seu custo J já esteja no mínimo local. O que você acha? Uma etapa de descida de gradiente será suficiente se você já tiver atingido o mínimo? Então, essa é uma pergunta complicada. Quando eu estava aprendendo essas coisas pela primeira vez, na verdade demorei muito tempo para descobrir. Mas vamos resolver isso juntos. Vamos supor que você tenha alguma função de custo J. E a que você vê aqui não é uma função de custo de erro quadrado e essa função de custo tem dois mínimos locais correspondentes aos dois vales que você vê aqui. Agora, vamos supor que, após alguns passos de descida do gradiente, seu parâmetro W esteja aqui, digamos, igual a cinco. Então esse é o valor atual de W. Isso significa que você está neste ponto na função de custo J. E isso é um mínimo local, acontece se você chamar a atenção para a função neste momento. A inclinação dessa linha é zero e, portanto, o termo derivado. Aqui é igual a zero para o valor atual de W. E então você está avaliando a atualização descendente se torna W é atualizada para W menos a taxa de aprendizado vezes zero. Estamos aqui porque o termo derivado é igual a zero. E isso é o mesmo que dizer que vamos definir W como igual a W. Então, isso significa que, se você já estiver em um mínimo local, a descida do gradiente deixa W inalterado. Porque ele apenas atualiza o novo valor de W para ser exatamente o mesmo valor antigo de W. Então, concretamente, digamos que o valor atual de W seja cinco. E alfa é 0,1 após uma iteração, você atualiza W como W menos alfa vezes zero e ainda é igual a cinco. Portanto, se seus parâmetros já trouxeram você a um mínimo local, então a descida do gradiente se transforma em absolutamente nada. Ele não altera os parâmetros que você deseja, pois mantém a solução no mínimo local. Isso também explica por que o gradiente descendente pode atingir um mínimo local, mesmo com uma taxa de aprendizado fixa alfa. Aqui está o que quero dizer, para ilustrar isso, vamos ver outro exemplo. Aqui está a função de custo J de W que queremos minimizar. Vamos inicializar a descida do gradiente aqui neste momento. Se dermos uma etapa de atualização, talvez ela nos leve a esse ponto. E como essa derivada é muito grande, a gradação e a descida dão um passo relativamente grande, para a direita. Agora, estamos neste segundo ponto em que damos outro passo. E você pode notar que a inclinação não é tão íngreme quanto era no primeiro ponto. Portanto, o derivado não é tão grande. Portanto, a próxima etapa de atualização não será tão grande quanto a primeira etapa. Agora, leia este terceiro ponto aqui e a derivada é menor do que era na etapa anterior. E daremos um passo ainda menor à medida que nos aproximamos do mínimo. O decorativo se aproxima cada vez mais de zero. Então, à medida que executamos o gradiente descendente, eventualmente estamos dando passos muito pequenos até que você finalmente alcance um mínimo local. Então, apenas para recapitular, à medida que nos aproximamos de um gradiente mínimo local, a descida de gradiente ocorrerá automaticamente em etapas menores. E isso porque, à medida que nos aproximamos do mínimo local, a derivada fica menor automaticamente. E isso significa que as etapas de atualização também diminuem automaticamente. Mesmo que a taxa de aprendizado alfa seja mantida em algum valor fixo. Então esse é o algoritmo de gradiente descendente, você pode usá-lo para tentar minimizar qualquer função de custo J. Não apenas a função de custo do erro quadrático médio que estamos usando para a nova regressão. No próximo vídeo, vamos pegar a função J e defini-la de volta para ser exatamente a função de custo dos modelos de regressão linear. A função de custo médio quadrático do erro que apresentamos anteriormente. E, além disso, discordam muito dessa função de custo que lhe dará seu primeiro algoritmo de aprendizado, o algoritmo de regressão linear.
