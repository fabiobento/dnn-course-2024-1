Para implementar a regressão linear, a primeira etapa importante é primeiro definir algo chamado função de custo. Isso é algo que criaremos neste vídeo, e a função de custo nos dirá o desempenho do modelo para que possamos tentar melhorá-lo. Vamos ver o que isso significa. Lembre-se de que você tem um conjunto de treinamento que contém recursos de entrada x e alvos de saída y. O modelo que você usará para ajustar esse conjunto de treinamento é essa função linear f_w, b de x igual a w vezes x mais b. Para introduzir um pouco mais de terminologia, w e b são chamados de parâmetros do modelo. No aprendizado de máquina, os parâmetros do modelo são as variáveis que você pode ajustar durante o treinamento para melhorar o modelo. Às vezes, você também ouve os parâmetros w e b chamados de coeficientes ou pesos. Agora vamos dar uma olhada no que esses parâmetros w e b fazem. Dependendo dos valores escolhidos para w e b, você obtém uma função diferente f de x, que gera uma linha diferente no gráfico. Lembre-se de que podemos escrever f de x como uma abreviatura para f_w, b de x. Vamos dar uma olhada em alguns gráficos de f de x em um gráfico. Talvez você já esteja familiarizado com o desenho de linhas em gráficos, mas mesmo que esta seja uma análise para você, espero que isso ajude você a criar uma intuição sobre como w e b os parâmetros determinam f. Quando w é igual a 0 e b é igual a 1,5, então f se parece com essa linha horizontal. Nesse caso, a função f de x é 0 vezes x mais 1,5, então f é sempre um valor constante. Ele sempre prevê 1,5 para o valor estimado de y. Y que é sempre igual a b e aqui b também é chamado de interceptação y porque é aí que ele cruza o eixo vertical ou o eixo y neste gráfico. Como segundo exemplo, se w é 0,5 e b é igual a 0, então f de x é 0,5 vezes x. Quando x é 0, a previsão também é 0, e quando x é 2, a previsão é 0,5 vezes 2, que é 1. Você obtém uma linha parecida com essa e percebe que a inclinação é 0,5 dividida por 1. O valor de w fornece a inclinação da linha, que é 0,5. Finalmente, se w é igual a 0,5 e b é igual a 1, então f de x é 0,5 vezes x mais 1 e quando x é 0, então f de x é igual a b, que é 1, então a linha cruza o eixo vertical em b, a interceptação y. Além disso, quando x é 2, então f de x é 2, então a linha fica assim. Novamente, essa inclinação é 0,5 dividida por 1, então o valor de w fornece a inclinação que é 0,5. Lembre-se de que você tem um conjunto de treinamento como o mostrado aqui. Com a regressão linear, o que você quer fazer é escolher valores para os parâmetros w e b para que a linha reta obtida da função f de alguma forma se ajuste bem aos dados. Como talvez esta linha mostrada aqui. Quando vejo que a linha se ajusta visualmente aos dados, você pode pensar que isso significa que a linha definida por f está passando aproximadamente ou em algum lugar próximo aos exemplos de treinamento, em comparação com outras linhas possíveis que não estão tão próximas desses pontos. Só para lembrá-lo de alguma notação, um exemplo de treinamento como este aqui é definido por x sobrescrito i, y sobrescrito i, onde y é o alvo. Para uma determinada entrada x^i, a função f também gera um valor preditivo para y e um valor que ela prediz para y é y, mostrado aqui.
Reproduza o vídeo começando em :4:41 e siga a transcrição4:41
Para nossa escolha de um modelo f de x^i é w vezes x^i mais b. Dito de outra forma, a previsão y hat i é f de wb de x^i, onde para o modelo que estamos usando f de x^i é igual a wx^i mais b. Agora, a questão é como encontrar valores para w e b de forma que a previsão y hat i esteja próxima do verdadeiro alvo y^i para muitos ou talvez todos os exemplos de treinamento x^i, y^i. Para responder a essa pergunta, vamos primeiro dar uma olhada em como medir o quão bem uma linha se ajusta aos dados de treinamento. Para fazer isso, vamos construir uma função de custo. A função de custo pega a previsão y hat e a compara com a meta y usando y hat menos y. Essa diferença é chamada de erro, estamos medindo a distância entre a previsão e a meta. Em seguida, vamos calcular o quadrado desse erro. Além disso, vamos querer calcular esse termo para diferentes exemplos de treinamento i no conjunto de treinamento. Ao medir o erro, por exemplo i, calcularemos esse termo de erro quadrado. Finalmente, queremos medir o erro em todo o conjunto de treinamento. Em particular, vamos resumir os erros quadrados desta forma. Vamos somar de i igual a 1,2, 3 até m e lembrar que m é o número de exemplos de treinamento, que é 47 para esse conjunto de dados. Observe que, se tivermos mais exemplos de treinamento, m é maior e sua função de custo calculará um número maior. Isso está resumindo mais exemplos. Para criar uma função de custo que não aumenta automaticamente à medida que o tamanho do conjunto de treinamento aumenta por convenção, calcularemos o erro quadrático médio em vez do erro quadrático total e faremos isso dividindo por m desta forma. Estamos quase lá. Só uma última coisa. Por convenção, a função de custo que as pessoas que usam aprendizado de máquina na verdade se divide por 2 vezes m. A divisão extra por 2 serve apenas para fazer com que alguns de nossos cálculos posteriores pareçam mais organizados, mas a função de custo ainda funciona independentemente de você incluir essa divisão por 2 ou não. Essa expressão aqui é a função de custo e vamos escrever J de wb para nos referirmos à função de custo. Isso também é chamado de função de custo de erro quadrático, e é chamado assim porque você está tomando o quadrado desses termos de erro. No aprendizado de máquina, pessoas diferentes usarão funções de custo diferentes para aplicativos diferentes, mas a função de custo de erro quadrado é, de longe, a mais comumente usada para regressão linear e, aliás, para todos os problemas de regressão em que parece dar bons resultados para muitos aplicativos.
Reproduza o vídeo começando em :8: e siga a transcrição8:00
Apenas como lembrete, a previsão y hat é igual às saídas do modelo f em x. Podemos reescrever a função de custo J de wb como 1 sobre 2m vezes a soma de i é igual a 1 a m de f de x^i menos y^i a quantidade ao quadrado. Eventualmente, vamos querer encontrar valores de w e b que tornem a função de custo pequena. Mas antes de ir lá, vamos primeiro ter mais intuição sobre o que J of wb está realmente computando. Neste ponto, você pode estar pensando que fizemos muitas contas para definir a função de custo. Mas o que exatamente isso está fazendo? Vamos para o próximo vídeo, onde veremos um exemplo do que a função de custo é realmente computacional, que espero que ajude você a criar uma intuição sobre o que significa se J de wb for grande versus se o custo j for pequeno. Vamos para o próximo vídeo.
