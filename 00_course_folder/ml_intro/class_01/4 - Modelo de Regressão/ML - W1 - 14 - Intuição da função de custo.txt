Estamos vendo a definição matemática da função de custo. Agora, vamos criar uma intuição sobre o que a função de custo está realmente fazendo. Neste vídeo, veremos um exemplo para ver como a função de custo pode ser usada para encontrar os melhores parâmetros para seu modelo. Sei que esse vídeo é um pouco mais longo do que os outros, mas tenha paciência, acho que valerá a pena. Para recapitular, aqui está o que vimos sobre a função de custo até agora. Você quer ajustar uma linha reta aos dados de treinamento, então você tem esse modelo, fw, b de x é w vezes x, mais b. Aqui, os parâmetros do modelo são w e b. Agora, dependendo dos valores escolhidos para esses parâmetros, você obtém linhas retas diferentes como essa. Você deseja encontrar valores para w e b, para que a linha reta se ajuste bem aos dados de treinamento. Para medir o quão bem uma escolha de w e b se ajusta aos dados de treinamento, você tem uma função de custo J. O que a função de custo J faz é medir a diferença entre as previsões do modelo e os valores reais reais de y. O que você verá mais tarde é que a regressão linear tentaria encontrar valores para w e b e, em seguida, fazer com que um J de w fosse o menor possível. Em matemática, escrevemos assim. Queremos minimizar, J em função de w e b. Agora, para que possamos visualizar melhor a função de custo J, este trabalho é uma versão simplificada do modelo de regressão linear. Vamos usar o modelo fw de x, é w vezes x. Você pode pensar nisso como pegar o modelo original à esquerda e se livrar do parâmetro b, ou definir o parâmetro b igual a 0. Ela simplesmente se afasta da equação, então f agora é apenas w vezes x. Agora você tem apenas um parâmetro w, e sua função de custo J é parecida com a que era antes.
Reproduza o vídeo começando em :2:13 e siga a transcrição2:13
Tomando a diferença e elevando-a ao quadrado, exceto que agora, f é igual a w vezes xi, e J agora é uma função de apenas w. A meta também se torna um pouco diferente, porque você tem apenas um parâmetro, w, não w e b. Com esse modelo simplificado, o objetivo é encontrar o valor de w, que minimize J de w. Para ver isso visualmente, o que isso significa é que se b estiver definido como 0, então f define uma linha que se parece com isso. Você vê que a linha passa pela origem aqui, porque quando x é 0, f de x também é 0. Agora, usando esse modelo simplificado, vamos ver como a função de custo muda à medida que você escolhe valores diferentes para o parâmetro w. Em particular, vamos ver os gráficos do modelo f de x e da função de custo J. Vou traçá-los lado a lado e você poderá ver como os dois estão relacionados. Primeiro, observe que para f subscrito w, quando o parâmetro w é fixo, ou seja, é sempre um valor constante, então fw é apenas uma função de x, o que significa que o valor estimado de y depende do valor da entrada x. Em contraste, olhando para a direita, a função de custo J, é uma função de w, onde w controla a inclinação da linha definida por f w. O custo definido por J depende de um parâmetro, neste caso, o parâmetro w. Vamos seguir em frente e traçar essas funções, fw de x e J de w lado a lado para que você possa ver como eles estão relacionados. Começaremos com o modelo, que é a função fw de x à esquerda. Aqui está o recurso de entrada x está no eixo horizontal e o valor de saída y está no eixo vertical. Aqui estão os gráficos de três pontos representando o conjunto de treinamento nas posições 1, 1, 2, 2 e 3,3. Vamos escolher um valor para w. Digamos que w seja 1. Para essa escolha de w, a função fw, eles dirão essa linha reta com uma inclinação de 1. Agora, o que você pode fazer a seguir é calcular o custo J quando w é igual a 1. Você deve se lembrar que a função de custo é definida da seguinte forma, é a função de custo de erro quadrático. Se você substituir fw (X^i) por w vezes X^i, a função de custo ficará assim. Onde essa expressão agora é w vezes X^i menos Y^i. Para esse valor de w, verifica-se que o termo de erro dentro da função de custo, esse w vezes X^i menos Y^i, é igual a 0 para cada um dos três pontos de dados. Porque para esse conjunto de dados, quando x é 1, então y é 1. Quando w também é 1, então f (x) é igual a 1, então f (x) é igual a y para este primeiro exemplo de treinamento, e a diferença é 0. Conectando isso à função de custo J, você obtém 0 ao quadrado. Da mesma forma, quando x é 2, então y é 2 e f (x) também é 2. Novamente, f (x) é igual a y, para o segundo exemplo de treinamento. Na função de custo, o erro quadrático do segundo exemplo também é 0 ao quadrado. Finalmente, quando x é 3, então y é 3 e f (3) também é 3. Em uma função de custo , o terceiro termo de erro quadrado também é 0 ao quadrado. Para todos os três exemplos neste conjunto de treinamento, f (X^i) é igual a Y^i para cada exemplo de treinamento i, então f (X^i) menos Y^i é 0. Para esse conjunto de dados específico, quando w é 1 , o custo J é igual a 0. Agora, o que você pode fazer à direita é traçar a função de custo J. Observe que, como a função de custo é uma função do parâmetro w, o eixo horizontal agora é rotulado como w e não x, e o eixo vertical agora é J e não y. Você tem J (1) igual a 0. Em outras palavras, quando w é igual a 1, J (w) é 0, então deixe-me traçar isso. Agora, vamos ver como F e J mudam para valores diferentes de w. W pode assumir uma faixa de valores, então w pode assumir valores negativos, w pode ser 0 e também pode assumir valores positivos. E se w for igual a 0,5 em vez de 1, como seriam esses gráficos então? Vamos seguir em frente e planejar isso. Vamos definir w como igual a 0,5 e, nesse caso, a função f (x) agora se parece com isso, é uma linha com uma inclinação igual a 0,5. Vamos também calcular o custo J, quando w é 0,5. Lembre-se de que a função de custo está medindo o erro quadrado ou a diferença entre o valor do estimador, ou seja, y hat I, que é F (X^i), e o valor real, que é Y^i para cada exemplo i. Visualmente, você pode ver que o erro ou a diferença é igual à altura dessa linha vertical aqui quando x é igual a 1. Porque essa linha inferior é a lacuna entre o valor real de y e o valor que a função f previu, que está um pouco mais abaixo aqui. Neste primeiro exemplo, quando x é 1, f (x) é 0,5. O erro quadrático no primeiro exemplo é 0,5 menos 1 quadrado. Lembre-se da função de custo. Resumiremos todos os exemplos de treinamento no conjunto de treinamento. Vamos ao segundo exemplo de treinamento. Quando x é 2, o modelo está prevendo que f (x) é 1 e o valor real de y é 2. O erro do segundo exemplo é igual à altura desse pequeno segmento de linha aqui, e o erro quadrado é o quadrado do comprimento desse segmento de linha, então você obtém 1 menos 2 ao quadrado. Vamos fazer o terceiro exemplo. Repetindo esse processo, o erro aqui, também mostrado por esse segmento de linha, é 1,5 menos 3 ao quadrado. Em seguida, resumimos todos esses termos, o que acaba sendo igual a 3,5. Em seguida, multiplicamos esse termo por 1 sobre 2m, onde m é o número de exemplos de treinamento. Como existem três exemplos de treinamento, m é igual a 3, então isso é igual a 1 sobre 2 vezes 3, onde esse m aqui é 3. Se calcularmos a matemática, isso resulta em 3,5 dividido por 6. O custo J é de cerca de 0,58. Vamos seguir em frente e traçar isso ali à direita. Agora, vamos tentar mais um valor para w. Que tal se w for igual a 0? Qual é a aparência dos gráficos de f e J quando w é igual a 0? Acontece que se w é igual a 0, então f de x é apenas essa linha horizontal que está exatamente no eixo x. O erro de cada exemplo é uma linha que vai de cada ponto até a linha horizontal que representa f de x igual a 0.
Reproduza o vídeo começando em :11:20 e siga a transcrição11:20
O custo J quando w é igual a 0 é 1 sobre 2m vezes a quantidade, 1^2 mais 2^2 mais 3^2, e isso é igual a 1 sobre 6 vezes 14, que é cerca de 2,33. Vamos traçar esse ponto onde w é 0 e J de 0 é 2,33 aqui. Você pode continuar fazendo isso para outros valores de w. Como w pode ser qualquer número, ele também pode ser um valor negativo. Se w for menos 0,5, então a linha f é uma linha inclinada para baixo como essa. Acontece que quando w é menos 0,5, você acaba com um custo ainda maior, em torno de 5,25, que é esse ponto aqui em cima. Você pode continuar calculando a função de custo para diferentes valores de w e assim por diante e representá-los graficamente. Acontece que, ao calcular uma faixa de valores, você pode lentamente traçar a aparência da função de custo J e é isso que J é. Recapitulando, cada valor do parâmetro w corresponde a um ajuste de linha reta diferente, f de x, no gráfico à esquerda. Para o conjunto de treinamento fornecido, essa escolha para um valor de w corresponde a um único ponto no gráfico à direita, pois para cada valor de w, você pode calcular o custo J de w. Por exemplo, quando w é igual a 1, isso corresponde a esse ajuste de linha reta nos dados e também corresponde a esse ponto no gráfico de J, onde w é igual a 1 e o custo J de 1 é igual a 0. Já quando w é igual a 0,5, isso dá a você essa linha que tem uma inclinação menor. Essa linha em combinação com o conjunto de treinamento corresponde a esse ponto no gráfico da função de custo em w igual a 0,5. Para cada valor de w, você obtém uma linha diferente e seus custos correspondentes, J de w, e você pode usar esses pontos para traçar esse gráfico à direita. Diante disso, como você pode escolher o valor de w que resulta na função f, ajustando bem os dados? Bem, como você pode imaginar, escolher um valor de w que faça com que J de w seja o menor possível parece uma boa aposta. J é a função de custo que mede o tamanho dos erros quadrados, portanto, escolher w que minimize esses erros quadrados e os torne tão pequenos quanto possível nos dará um bom modelo. Neste exemplo, se você escolhesse o valor de w que resulta no menor valor possível de J de w, acabaria escolhendo w igual a 1. Como você pode ver, essa é realmente uma escolha muito boa. Isso resulta na linha que se ajusta muito bem aos dados de treinamento. É assim que, na regressão linear, você usa a função de custo para encontrar o valor de w que minimiza J. No caso mais geral em que tínhamos os parâmetros w e b em vez de apenas w, você encontra os valores de w e b que minimizam J. Para resumir, você viu gráficos de f e J e descobriu como os dois estão relacionados. Conforme você varia w ou varia w e b, você acaba com linhas retas diferentes e quando essa linha reta passa pelos dados, a causa J é pequena. O objetivo da regressão linear é encontrar os parâmetros w ou w e b que resultem no menor valor possível para a função de custo J. Agora, neste vídeo, analisamos nosso exemplo com um problema simplificado usando apenas w. No próximo vídeo, vamos visualizar a aparência da função de custo para a versão completa da regressão linear usando w e b. Você vê alguns gráficos 3D interessantes. Vamos para o próximo vídeo.
