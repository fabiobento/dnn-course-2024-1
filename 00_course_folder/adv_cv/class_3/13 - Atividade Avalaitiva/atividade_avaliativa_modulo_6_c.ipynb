{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/adv_cv/class_3/13%20-%20Atividade%20Avalaitiva/atividade_avaliativa_modulo_6_c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "adaptado de [Visão computacional avançada com TensorFlow](https://www.coursera.org/learn/advanced-computer-vision-with-tensorflow?specialization=tensorflow-advanced-techniques) de [Laurence Moroney](https://laurencemoroney.com/) e [Andrew Ng](https://www.coursera.org/instructor/andrewng) , [DeepLearning.AI](https://www.deeplearning.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6x_sGPQocpw"
      },
      "source": [
        "# Segmentação de Imagens de Dígitos Manuscritos\n",
        "\n",
        "<img src='https://github.com/fabiobento/dnn-course-2024-1/raw/main/00_course_folder/adv_cv/class_3/13%20-%20Atividade%20Avalaitiva/images/m2nist_segmentation.png' alt='m2nist digits'>\n",
        "\n",
        "Nessa atividade você criará um modelo que prevê as máscaras de segmentação (mapa de rótulos por pixel) de dígitos manuscritos. Esse modelo será treinado no [M2NIST dataset](https://www.kaggle.com/farhanhubble/multimnistm2nist), um MNIST de vários dígitos.\n",
        "\n",
        "Note que muitas das etapas aqui são muito semelhantes ao que você viu na aula [6.8-Redes neurais totalmente convolucionais para segmentação de imagens](https://drive.google.com/file/d/1XLkmlyzmP95mDtArmZXhvA_oJBGgKWZh/view?usp=drive_link).\n",
        "\n",
        "Você criará uma rede neural convolucional (CNN) do zero para o caminho de downsampling e usará uma rede totalmente convolucional, a FCN-8, para fazer o upsample e produzir o mapa de rótulos em pixels. O modelo será avaliado usando a interseção sobre a união (IOU) e o Dice Score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnr0kbM0g2LD"
      },
      "source": [
        "## Exercícios\n",
        "\n",
        "Forneço a você um código padrão para trabalhar e estes são os 5 exercícios que você precisa preencher antes de obter as máscaras de segmentação com êxito.\n",
        "\n",
        "* [Exercício 1 - Definir o Bloco Convolucional Básico](#exercise-1)\n",
        "* [Exercício 2 - Definir o Caminho de Downsampling](#exercise-2)\n",
        "* [Exercício 3 - Definir o decoder FCN-8](#exercise-3)\n",
        "* [Exercício 4 - Compilação do Modelo](#exercise-4)\n",
        "* [Exercício 5 - Treino do Modelo](#exercise-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3g9dJxSxmN"
      },
      "source": [
        "## Importações\n",
        "\n",
        "Como de costume, vamos começar importando os pacotes que você usará nesta atividade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aifz2907kxYN"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RYh6cCzXE6R"
      },
      "source": [
        "## Download do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUGGF3wfqYni"
      },
      "source": [
        "[M2NIST](https://www.kaggle.com/farhanhubble/multimnistm2nist) é um [MNIST](http://yann.lecun.com/exdb/mnist/) **multidígito**.\n",
        "Cada imagem tem até 3 dígitos de dígitos do MNIST e o arquivo de rótulos correspondente tem as máscaras de segmentação.\n",
        "\n",
        "O conjunto de dados está disponível no [Kaggle](https://www.kaggle.com) e você pode encontrá-lo [aqui](https://www.kaggle.com/farhanhubble/multimnistm2nist)\n",
        "\n",
        "Para facilitar, ele está hospedado no Google Cloud para que você possa fazer o download sem as credenciais do Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROok0i9rMcu0"
      },
      "outputs": [],
      "source": [
        "# download do dataset zipado\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/m2nist.zip \\\n",
        "    -O /tmp/m2nist.zip\n",
        "\n",
        "# localizar e extrair para uma pasta local ('/tmp/training')\n",
        "local_zip = '/tmp/m2nist.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/training')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy17LYR7XJNa"
      },
      "source": [
        "## Carregar e Preprocessar o Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXGMrWnkrvpK"
      },
      "source": [
        "Esse conjunto de dados pode ser facilmente pré-processado, pois está disponível como **Numpy Array Files (.npy)**\n",
        "\n",
        "1. O **combined.npy** tem os arquivos de imagem que contêm os vários dígitos do MNIST. Cada imagem tem o tamanho **64 x 84** (altura x largura, em pixels).\n",
        "\n",
        "2. O arquivo **segmented.npy** contém as máscaras de segmentação correspondentes. Cada máscara de segmentação também tem o tamanho de **64 x 84**.\n",
        "\n",
        "Esse conjunto de dados tem **5000** amostras e você pode fazer divisões apropriadas de treinamento, validação e teste, conforme necessário para o problema.\n",
        "\n",
        "Com isso, vamos definir algumas funções utilitárias para carregar e pré-processar o conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy_pw5I2-xLP"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def read_image_and_annotation(image, annotation):\n",
        "  '''\n",
        "  Converte a imagem e a anotação em seu tipo de dados esperado e\n",
        "  normaliza a imagem de entrada de modo que cada pixel esteja no intervalo [-1, 1]\n",
        "\n",
        "  Args:\n",
        "    image (matriz numpy) -- imagem de entrada\n",
        "    annotation (matriz numpy) -- mapa de rótulos de ground-truth\n",
        "\n",
        "  Retorna:\n",
        "    par imagem-anotação pré-processado\n",
        "  '''\n",
        "\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.reshape(image, (image.shape[0], image.shape[1], 1,))\n",
        "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
        "  image = image / 127.5\n",
        "  image -= 1\n",
        "\n",
        "  return image, annotation\n",
        "\n",
        "\n",
        "def get_training_dataset(images, annos):\n",
        "  '''\n",
        "  Prepara lotes embaralhados do conjunto de treinamento.\n",
        "\n",
        "  Args:\n",
        "    images (lista de strings) -- caminhos para cada arquivo de imagem no conjunto de treinamento\n",
        "    annos (lista de strings) -- caminhos para cada mapa de rótulo no conjunto de treinamento\n",
        "\n",
        "  Retorna:\n",
        "    tf Conjunto de dados contendo o conjunto de treinamento pré-processado\n",
        "  '''\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  training_dataset = training_dataset.map(read_image_and_annotation)\n",
        "\n",
        "  training_dataset = training_dataset.shuffle(512, reshuffle_each_iteration=True)\n",
        "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
        "  training_dataset = training_dataset.repeat()\n",
        "  training_dataset = training_dataset.prefetch(-1)\n",
        "\n",
        "  return training_dataset\n",
        "\n",
        "\n",
        "def get_validation_dataset(images, annos):\n",
        "  '''\n",
        "  Prepara os lotes do conjunto de validação.\n",
        "\n",
        "  Args:\n",
        "    images (lista de strings) -- caminhos para cada arquivo de imagem no val set\n",
        "    annos (lista de strings) -- caminhos para cada mapa de rótulo no conjunto de valores\n",
        "\n",
        "  Retorna:\n",
        "    tf Conjunto de dados contendo o conjunto de validação pré-processado\n",
        "  '''\n",
        "  validation_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  validation_dataset = validation_dataset.map(read_image_and_annotation)\n",
        "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.repeat()\n",
        "\n",
        "  return validation_dataset\n",
        "\n",
        "\n",
        "def get_test_dataset(images, annos):\n",
        "  '''\n",
        "  Prepara os lotes do conjunto de teste.\n",
        "\n",
        "  Args:\n",
        "    images (lista de strings) -- caminhos para cada arquivo de imagem no conjunto de teste\n",
        "    annos (lista de strings) -- caminhos para cada mapa de rótulo no conjunto de teste\n",
        "\n",
        "  Retorna:\n",
        "    tf Conjunto de dados contendo o conjunto de validação pré-processado\n",
        "  '''\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  test_dataset = test_dataset.map(read_image_and_annotation)\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return test_dataset\n",
        "\n",
        "\n",
        "def load_images_and_segments():\n",
        "  '''\n",
        "  Carrega as imagens e os segmentos como matrizes numpy de arquivos npy\n",
        "  e faz divisões para conjuntos de dados de treinamento, validação e teste.\n",
        "\n",
        "  Retorna:\n",
        "    3 tuplas contendo as divisões de treinamento, validação e teste\n",
        "  '''\n",
        "\n",
        "  #Carrega imagens e máscaras de segmentação.\n",
        "  images = np.load('/tmp/training/combined.npy')\n",
        "  segments = np.load('/tmp/training/segmented.npy')\n",
        "\n",
        "  #Faz divisões de treinamento, validação e teste a partir de imagens carregadas e máscaras de segmentação.\n",
        "  train_images, val_images, train_annos, val_annos = train_test_split(images, segments, test_size=0.2, shuffle=True)\n",
        "  val_images, test_images, val_annos, test_annos = train_test_split(val_images, val_annos, test_size=0.2, shuffle=True)\n",
        "\n",
        "  return (train_images, train_annos), (val_images, val_annos), (test_images, test_annos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPHO1YbTACcu"
      },
      "source": [
        "Agora você pode carregar o conjunto de dados pré-processado e definir os conjuntos de treino, validação e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIS70_um_Y7n"
      },
      "outputs": [],
      "source": [
        "# Carregar o Dataset\n",
        "train_slices, val_slices, test_slices = load_images_and_segments()\n",
        "\n",
        "# Criar os datasets de treino, validação e teste.\n",
        "training_dataset = get_training_dataset(train_slices[0], train_slices[1])\n",
        "validation_dataset = get_validation_dataset(val_slices[0], val_slices[1])\n",
        "test_dataset = get_test_dataset(test_slices[0], test_slices[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKXJYZi7A0dF"
      },
      "source": [
        "## Vamos dar uma olhada no dataset\n",
        "\n",
        "Talvez você queira inspecionar visualmente o dataset antes e depois do treino. Como acima, incluí funções utilitárias para ajudar a mostrar algumas imagens, bem como suas anotações (ou seja, rótulos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "d46YCbvPafbp"
      },
      "outputs": [],
      "source": [
        "# Utilitários de visualização\n",
        "\n",
        "# Existem 11 classes no conjunto de dados: uma classe para cada dígito (0 a 9)\n",
        "# mais a classe de fundo\n",
        "n_classes = 11\n",
        "\n",
        "# Atribuir uma cor aleatória para cada classe\n",
        "colors = [tuple(np.random.randint(256, size=3) / 255.0) for i in range(n_classes)]\n",
        "\n",
        "def fuse_with_pil(images):\n",
        "  '''\n",
        "  Cria uma imagem em branco e cola as imagens de entrada\n",
        "\n",
        "  Args:\n",
        "    images (lista de matrizes numpy) - representações de matrizes numpy das imagens a serem coladas\n",
        "\n",
        "  Retorna:\n",
        "    Objeto PIL Image contendo as imagens\n",
        "  '''\n",
        "  widths = (image.shape[1] for image in images)\n",
        "  heights = (image.shape[0] for image in images)\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    pil_image = PIL.Image.fromarray(np.uint8(im))\n",
        "    new_im.paste(pil_image, (x_offset,0))\n",
        "    x_offset += im.shape[1]\n",
        "\n",
        "  return new_im\n",
        "\n",
        "\n",
        "def give_color_to_annotation(annotation):\n",
        "  '''\n",
        "  Converte uma anotação 2-D em uma matriz numpy com formato (altura, largura, 3) em que\n",
        "  o terceiro eixo representa o canal de cor. Os valores do rótulo são multiplicados por\n",
        "  255 e colocados nesse eixo para dar cor à anotação\n",
        "\n",
        "  Args:\n",
        "    annotation (matriz numpy) - matriz do mapa de rótulos\n",
        "\n",
        "  Retorna:\n",
        "    a matriz de anotação com um canal/eixo de cor adicional\n",
        "  '''\n",
        "  seg_img = np.zeros( (annotation.shape[0],annotation.shape[1], 3) ).astype('float')\n",
        "\n",
        "  for c in range(n_classes):\n",
        "    segc = (annotation == c)\n",
        "    seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
        "    seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
        "    seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
        "\n",
        "  return seg_img\n",
        "\n",
        "\n",
        "def show_annotation_and_prediction(image, annotation, prediction, iou_list, dice_score_list):\n",
        "  '''\n",
        "  Exibe as imagens com o ground-truth e os mapas de rótulos previstos. Também sobrepõe as métricas.\n",
        "\n",
        "  Args:\n",
        "    image (matriz numpy) -- a imagem de entrada\n",
        "    annotation (matriz numpy) -- o mapa de rótulos da verdade terrestre\n",
        "    prediction (matriz numpy) -- o mapa de rótulos previsto\n",
        "    iou_list (lista de floats) -- os valores de IOU para cada classe\n",
        "    dice_score_list (lista de floats) -- a pontuação de dados para cada classe\n",
        "  '''\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  true_img = give_color_to_annotation(new_ann)\n",
        "  pred_img = give_color_to_annotation(prediction)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "  image = np.uint8(image)\n",
        "  images = [image, np.uint8(pred_img), np.uint8(true_img)]\n",
        "\n",
        "  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0 and idx < 10]\n",
        "  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
        "\n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\".join(display_string_list)\n",
        "\n",
        "  plt.figure(figsize=(15, 4))\n",
        "\n",
        "  for idx, im in enumerate(images):\n",
        "    plt.subplot(1, 3, idx+1)\n",
        "    if idx == 1:\n",
        "      plt.xlabel(display_string)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(im)\n",
        "\n",
        "\n",
        "def show_annotation_and_image(image, annotation):\n",
        "  '''\n",
        "  Exibe a imagem e sua anotação lado a lado\n",
        "\n",
        "  Args:\n",
        "    image (matriz numpy) -- a imagem de entrada\n",
        "    annotation (numpy array) -- o mapa de rótulos\n",
        "  '''\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  seg_img = give_color_to_annotation(new_ann)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "\n",
        "  image = np.uint8(image)\n",
        "  images = [image, seg_img]\n",
        "\n",
        "  images = [image, seg_img]\n",
        "  fused_img = fuse_with_pil(images)\n",
        "  plt.imshow(fused_img)\n",
        "\n",
        "\n",
        "def list_show_annotation(dataset, num_images):\n",
        "  '''\n",
        "  Exibe imagens e suas anotações lado a lado\n",
        "\n",
        "  Args:\n",
        "    dataset (tf Dataset) -- lote de imagens e anotações\n",
        "    num_images (int) -- número de imagens a serem exibidas\n",
        "  '''\n",
        "  ds = dataset.unbatch()\n",
        "\n",
        "  plt.figure(figsize=(20, 15))\n",
        "  plt.title(\"Imagens e Anotações\")\n",
        "  plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
        "\n",
        "  for idx, (image, annotation) in enumerate(ds.take(num_images)):\n",
        "    plt.subplot(5, 5, idx + 1)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    show_annotation_and_image(image.numpy(), annotation.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEfyChmKEFKe"
      },
      "source": [
        "Você pode visualizar um subconjunto de imagens do conjunto de dados com a função `list_show_annotation()` definida acima. Execute as células abaixo para ver a imagem à esquerda e seu mapa de rótulos de ground-truth em pixels à direita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFO_hIhLWYT4"
      },
      "outputs": [],
      "source": [
        "# Obter 10 imagens do conjunto de treinamento\n",
        "list_show_annotation(training_dataset, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdgVkp8wZua0"
      },
      "outputs": [],
      "source": [
        "# Obter 10 imagens do conjunto de validação\n",
        "list_show_annotation(validation_dataset, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkpgIcE2FeKI"
      },
      "source": [
        "Você pode ver nas imagens acima as cores atribuídas a cada classe (ou seja, de 0 a 9 mais o plano de fundo). Se você não gostar dessas cores, sinta-se à vontade para executar novamente a célula em que `colors` está definida para obter outro conjunto de cores aleatórias. Como alternativa, você pode atribuir os valores RGB para cada classe em vez de depender de valores aleatórios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv2k8xabRb8"
      },
      "source": [
        "## Definir o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ylpyqJDQiF"
      },
      "source": [
        "O modelo de segmentação tem doi caminhos:\n",
        "\n",
        "1. **Caminho de Downsampling** - Essa parte da rede extrai as características da imagem. Isso é feito por meio de uma série de camadas de convolução e agrupamento. A saída final é uma imagem reduzida (devido às camadas de pooling) com as características extraídas. Você criará uma CNN personalizada do zero para esse caminho.\n",
        "\n",
        "2. **Caminho de Upsampling** - Esse caminho pega a saída do caminho de downsampling e gera as previsões, além de converter a imagem de volta ao seu tamanho original. Você usará um decodificador FCN-8 para esse caminho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHlBUZvsDybt"
      },
      "source": [
        "### Definir o bloco de convolução básico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-XO9xkN1OR3"
      },
      "source": [
        "<a name='exercise-1'></a>\n",
        "\n",
        "#### **Exercício 1**\n",
        "\n",
        "Preencha a função abaixo para criar o bloco de convolução básico para nossa CNN. Isso terá duas camadas [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/), cada uma seguida por uma [LeakyReLU](https://keras.io/api/layers/activation_layers/leaky_relu/), depois [max pooled](https://keras.io/api/layers/pooling_layers/max_pooling2d/) e [batch-normalized](https://keras.io/api/layers/normalization_layers/batch_normalization/). Use a sintaxe funcional para empilhar essas camadas.\n",
        "\n",
        "$$Input -> Conv2D -> LeakyReLU -> Conv2D -> LeakyReLU -> MaxPooling2D -> BatchNormalization$$\n",
        "\n",
        "Ao definir as camadas Conv2D, observe que nossas entradas de dados terão a dimensão \"channels\" por último. Talvez você queira verificar o argumento `data_format` no [docs](https://keras.io/api/layers/convolution_layers/convolution2d/) com relação a isso. Observe também o argumento `padding` como você fez nos laboratórios não avaliados.\n",
        "\n",
        "Por fim, para usar a ativação `LeakyReLU`, você **não** precisa aninhá-la dentro de uma camada `Activation` (por exemplo, `x = tf.keras.layers.Activation(tf.keras.layers.LeakyReLU()(x)`). Em vez disso, você pode simplesmente empilhar a camada diretamente (por exemplo, `x = tf.keras.layers.LeakyReLU()(x)`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azEEVytHR0Kn"
      },
      "outputs": [],
      "source": [
        "# Parâmetro que descreve onde a dimensão do canal é encontrada em nosso conjunto de dados\n",
        "IMAGE_ORDERING = 'channels_last'\n",
        "\n",
        "def conv_block(input, filters, kernel_size, pooling_size, pool_strides):\n",
        "  '''\n",
        "  Args:\n",
        "    input (tensor) -- lote de imagens ou recursos\n",
        "    filters (int) -- número de filtros das camadas Conv2D\n",
        "    kernel_size (int) -- configuração de kernel_size das camadas Conv2D\n",
        "    pooling_size (int) -- tamanho do pooling das camadas MaxPooling2D\n",
        "    pool_strides (int) -- configuração de strides das camadas MaxPooling2D\n",
        "\n",
        "  Retorna:\n",
        "    (tensor) máximo de recursos agrupados e normalizados por lote da entrada\n",
        "  '''\n",
        "  ### COMECE O CÓDIGO AQUI ###\n",
        "  # use a sintaxe funcional para empilhar as camadas, conforme mostrado no diagrama acima  x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', data_format=IMAGE_ORDERING)(input)\n",
        "  x = None\n",
        "  x = None\n",
        "  x = None\n",
        "  x = None\n",
        "  x = None\n",
        "  ### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGtlHuM6KCRg"
      },
      "outputs": [],
      "source": [
        "# CÓDIGO DE TESTE:\n",
        "\n",
        "test_input = tf.keras.layers.Input(shape=(64,84, 1))\n",
        "test_output = conv_block(test_input, 32, 3, 2, 2)\n",
        "test_model = tf.keras.Model(inputs=test_input, outputs=test_output)\n",
        "\n",
        "print(test_model.summary())\n",
        "\n",
        "# Liberar recursos de teste\n",
        "del test_input, test_output, test_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmf_ZaMP2rV3"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "Preste atenção nas colunas *(type)* e *Output Shape*. O nome *Layer* ao lado do tipo pode ser diferente dependendo de quantas vezes você executou a célula (por exemplo, `input_7` pode ser `input_1`)\n",
        "\n",
        "```txt\n",
        "Model: \"functional_1\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "input_1 (InputLayer)         [(None, 64, 84, 1)]       0\n",
        "_________________________________________________________________\n",
        "conv2d (Conv2D)              (None, 64, 84, 32)        320\n",
        "_________________________________________________________________\n",
        "leaky_re_lu (LeakyReLU)      (None, 64, 84, 32)        0\n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 64, 84, 32)        9248\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_1 (LeakyReLU)    (None, 64, 84, 32)        0\n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 32, 42, 32)        0\n",
        "_________________________________________________________________\n",
        "batch_normalization (BatchNo (None, 32, 42, 32)        128\n",
        "=================================================================\n",
        "Total params: 9,696\n",
        "Trainable params: 9,632\n",
        "Non-trainable params: 64\n",
        "_________________________________________________________________\n",
        "None\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-jJbC91EXTV"
      },
      "source": [
        "### Definir o Caminho de Downsampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ueOTmc3HxJ"
      },
      "source": [
        "<a name='exercise-2'></a>\n",
        "\n",
        "#### **Exercício 2**\n",
        "\n",
        "Agora que definimos o bloco de construção do nosso codificador, você pode criar o caminho de downsampling. Preencha a função abaixo para criar o codificador. Isso deve encadear cinco blocos de construção de convolução para criar uma CNN de extração de características sem as camadas totalmente conectadas.\n",
        "\n",
        "*Observações*:\n",
        "1. Para otimizar o processamento ou para facilitar o trabalho com as dimensões de saída de cada camada, às vezes é aconselhável aplicar um pouco de preenchimento zero à imagem de entrada. Com o código padrão que fornecemos abaixo, preenchemos a largura da entrada com 96 pixels usando a camada [ZeroPadding2D](https://keras.io/api/layers/reshaping_layers/zero_padding2d/).No entanto, isso não é obrigatório. Você pode removê-la mais tarde e ver como isso afetará seus parâmetros. Por exemplo, talvez você precise passar um tamanho de kernel não quadrado para o decodificador no Exercício 3 (por exemplo, `(4,5)`) para corresponder às dimensões de saída do Exercício 2.\n",
        "\n",
        "2. Recomendo manter os parâmetros pool size e stride constantes em 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2VNB99LRwQr"
      },
      "outputs": [],
      "source": [
        "def FCN8(input_height=64, input_width=84):\n",
        "    '''\n",
        "    Define o caminho de downsampling do modelo de segmentação de imagem.\n",
        "\n",
        "    Args:\n",
        "      input_height (int) -- altura das imagens\n",
        "      width (int) -- largura das imagens\n",
        "\n",
        "    Retorna:\n",
        "    (tupla de tensores, tensor)\n",
        "      tupla de tensores -- recursos extraídos nos blocos 3 a 5\n",
        "      tensor -- cópia da entrada\n",
        "    '''\n",
        "    img_input = tf.keras.layers.Input(shape=(input_height,input_width, 1))\n",
        "\n",
        "    ### COMECE O CÓDIGO AQUI ###\n",
        "\n",
        "    # Preencher a largura da imagem de entrada com 96 pixels\n",
        "    x = tf.keras.layers.ZeroPadding2D(((0, 0), (0, 96-input_width)))(img_input)\n",
        "\n",
        "    # Bloco 1\n",
        "    x = None\n",
        "\n",
        "    # Bloco 2\n",
        "    x = None\n",
        "\n",
        "    # Bloco 3\n",
        "    x = None\n",
        "    # salvar o mapa de características nesse estágio\n",
        "    f3 = None\n",
        "\n",
        "    # Bloco 4\n",
        "    x = None\n",
        "    # salvar o mapa de características nesse estágio\n",
        "    f4 = None\n",
        "\n",
        "    # Bloco 5\n",
        "    x = None\n",
        "    # salvar o mapa de características nesse estágio\n",
        "    f5 = None\n",
        "\n",
        "    ### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "    return (f3, f4, f5), img_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVQm1W0CNICS"
      },
      "outputs": [],
      "source": [
        "# CÓDIGO DE TESTE:\n",
        "\n",
        "test_convs, test_img_input = FCN8()\n",
        "test_model = tf.keras.Model(inputs=test_img_input, outputs=[test_convs, test_img_input])\n",
        "\n",
        "print(test_model.summary())\n",
        "\n",
        "del test_convs, test_img_input, test_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxklZe_D3jOI"
      },
      "source": [
        "**Resultado esperado**:\n",
        "\n",
        "Você deverá ver as camadas do seu `conv_block()` sendo repetidas 5 vezes, como na saída abaixo.\n",
        "\n",
        "```txt\n",
        "Model: \"functional_3\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "input_3 (InputLayer)         [(None, 64, 84, 1)]       0\n",
        "_________________________________________________________________\n",
        "zero_padding2d (ZeroPadding2 (None, 64, 96, 1)         0\n",
        "_________________________________________________________________\n",
        "conv2d_2 (Conv2D)            (None, 64, 96, 32)        320\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_2 (LeakyReLU)    (None, 64, 96, 32)        0\n",
        "_________________________________________________________________\n",
        "conv2d_3 (Conv2D)            (None, 64, 96, 32)        9248\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_3 (LeakyReLU)    (None, 64, 96, 32)        0\n",
        "_________________________________________________________________\n",
        "max_pooling2d_1 (MaxPooling2 (None, 32, 48, 32)        0\n",
        "_________________________________________________________________\n",
        "batch_normalization_1 (Batch (None, 32, 48, 32)        128\n",
        "_________________________________________________________________\n",
        "conv2d_4 (Conv2D)            (None, 32, 48, 64)        18496\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_4 (LeakyReLU)    (None, 32, 48, 64)        0\n",
        "_________________________________________________________________\n",
        "conv2d_5 (Conv2D)            (None, 32, 48, 64)        36928\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_5 (LeakyReLU)    (None, 32, 48, 64)        0\n",
        "_________________________________________________________________\n",
        "max_pooling2d_2 (MaxPooling2 (None, 16, 24, 64)        0\n",
        "_________________________________________________________________\n",
        "batch_normalization_2 (Batch (None, 16, 24, 64)        256\n",
        "_________________________________________________________________\n",
        "conv2d_6 (Conv2D)            (None, 16, 24, 128)       73856\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_6 (LeakyReLU)    (None, 16, 24, 128)       0\n",
        "_________________________________________________________________\n",
        "conv2d_7 (Conv2D)            (None, 16, 24, 128)       147584\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_7 (LeakyReLU)    (None, 16, 24, 128)       0\n",
        "_________________________________________________________________\n",
        "max_pooling2d_3 (MaxPooling2 (None, 8, 12, 128)        0\n",
        "_________________________________________________________________\n",
        "batch_normalization_3 (Batch (None, 8, 12, 128)        512\n",
        "_________________________________________________________________\n",
        "conv2d_8 (Conv2D)            (None, 8, 12, 256)        295168\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_8 (LeakyReLU)    (None, 8, 12, 256)        0\n",
        "_________________________________________________________________\n",
        "conv2d_9 (Conv2D)            (None, 8, 12, 256)        590080\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_9 (LeakyReLU)    (None, 8, 12, 256)        0\n",
        "_________________________________________________________________\n",
        "max_pooling2d_4 (MaxPooling2 (None, 4, 6, 256)         0\n",
        "_________________________________________________________________\n",
        "batch_normalization_4 (Batch (None, 4, 6, 256)         1024\n",
        "_________________________________________________________________\n",
        "conv2d_10 (Conv2D)           (None, 4, 6, 256)         590080\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_10 (LeakyReLU)   (None, 4, 6, 256)         0\n",
        "_________________________________________________________________\n",
        "conv2d_11 (Conv2D)           (None, 4, 6, 256)         590080\n",
        "_________________________________________________________________\n",
        "leaky_re_lu_11 (LeakyReLU)   (None, 4, 6, 256)         0\n",
        "_________________________________________________________________\n",
        "max_pooling2d_5 (MaxPooling2 (None, 2, 3, 256)         0\n",
        "_________________________________________________________________\n",
        "batch_normalization_5 (Batch (None, 2, 3, 256)         1024\n",
        "=================================================================\n",
        "Total params: 2,354,784\n",
        "Trainable params: 2,353,312\n",
        "Non-trainable params: 1,472\n",
        "_________________________________________________________________\n",
        "None\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbjYEQU8Eq-T"
      },
      "source": [
        "### Definir o decodificador FCN-8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux5nAU155E5O"
      },
      "source": [
        "<a name='exercise-3'></a>\n",
        "\n",
        "#### **Exercício 3**\n",
        "\n",
        "Agora você pode definir o caminho de upsampling usando as saídas das convoluções em cada estágio como argumentos.\n",
        "* Observação: lembre-se de definir o parâmetro `data_format` para as camadas Conv2D.\n",
        "\n",
        "Aqui está também o diagrama que você viu na aula sobre como isso deve funcionar:\n",
        "\n",
        "<img src='https://github.com/fabiobento/dnn-course-2024-1/raw/main/00_course_folder/adv_cv/class_3/13%20-%20Atividade%20Avalaitiva/images/fcn8.png' alt='fcn-8'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giYEct_Se5Xj"
      },
      "outputs": [],
      "source": [
        "def fcn8_decoder(convs, n_classes):\n",
        "  # características do estágio do codificador\n",
        "  f3, f4, f5 = convs\n",
        "\n",
        "  # Número de filtros\n",
        "  n = 512\n",
        "\n",
        "  # Adicionar camadas convolucionais sobre o extrator CNN.\n",
        "  o = tf.keras.layers.Conv2D(n , (7 , 7) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING)(f5)\n",
        "  o = tf.keras.layers.Dropout(0.5)(o)\n",
        "\n",
        "  o = tf.keras.layers.Conv2D(n , (1 , 1) , activation='relu' , padding='same', name=\"conv7\", data_format=IMAGE_ORDERING)(o)\n",
        "  o = tf.keras.layers.Dropout(0.5)(o)\n",
        "\n",
        "  o = tf.keras.layers.Conv2D(n_classes,  (1, 1), activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o)\n",
        "\n",
        "\n",
        "  ### INICIE O CÓDIGO AQUI\n",
        "\n",
        "  # Faça o upsample do `o` acima e corte os pixels extras introduzidos\n",
        "  o = None\n",
        "  o = None\n",
        "\n",
        "  # carregue a previsão do pool 4 e faça uma convolução 1x1 para remodelá-la para a mesma forma do `o` acima\n",
        "  o2 = None\n",
        "  o2 = None\n",
        "\n",
        "  # adicione os resultados do upsampling e da previsão do pool 4\n",
        "  o = None\n",
        "\n",
        "  # Upsampling do tensor resultante da operação que você acabou de fazer\n",
        "  o = None\n",
        "  o = None\n",
        "\n",
        "  # Carregue a previsão do pool 3 e faça uma convolução 1x1 para remodelá-la para a mesma forma do `o` acima\n",
        "  o2 = None\n",
        "  o2 = tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o2)\n",
        "\n",
        "  # adicione os resultados da do upsampling e da previsão do pool 3\n",
        "  o = None\n",
        "\n",
        "  # Upsampling até o tamanho da imagem original\n",
        "  o = None\n",
        "  o = tf.keras.layers.Cropping2D(((0, 0), (0, 96-84)))(o)\n",
        "\n",
        "  # Acrescentar uma ativação sigmoide\n",
        "  o = (tf.keras.layers.Activation('sigmoid'))(o)\n",
        "  ### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "  return o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQHioDlR5K1_"
      },
      "outputs": [],
      "source": [
        "# CÓDIGO DE TESTE\n",
        "\n",
        "test_convs, test_img_input = FCN8()\n",
        "test_fcn8_decoder = fcn8_decoder(test_convs, 11)\n",
        "\n",
        "print(test_fcn8_decoder.shape)\n",
        "\n",
        "del test_convs, test_img_input, test_fcn8_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1qTwTX-5fwH"
      },
      "source": [
        "**Saída esperada:**\n",
        "\n",
        "```txt\n",
        "(None, 64, 84, 11)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJOhQz86Qk6n"
      },
      "source": [
        "### Definir o modelo completo\n",
        "\n",
        "Os caminhos de downsampling e upsampling agora podem ser combinados conforme mostrado abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EJEf484312h"
      },
      "outputs": [],
      "source": [
        "# iniciar o codificador usando o tamanho de entrada padrão 64 x 84\n",
        "convs, img_input = FCN8()\n",
        "\n",
        "# Passar as convoluções obtidas no codificador para o decodificador\n",
        "dec_op = fcn8_decoder(convs, n_classes)\n",
        "\n",
        "# Definir o modelo especificando a entrada (lote de imagens) e a saída (saída do decodificador)\n",
        "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GAenp1M4gXx"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAAXygZtbZmu"
      },
      "source": [
        "## Compile o Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC-9m1R_5tjV"
      },
      "source": [
        "<a name='exercise-4'></a>\n",
        "\n",
        "### **Exercício 4**\n",
        "\n",
        "Compile o modelo usando uma perda, um otimizador e uma métrica apropriados.\n",
        "\n",
        "Nota:** Atualmente, há um problema com o classificador que aceita determinadas funções de perda. Estaremos atualizando-o, mas, enquanto isso, use esta sintaxe:_\n",
        "\n",
        "```\n",
        "loss='<loss string name>'\n",
        "```\n",
        "\n",
        "*ao invés de:*\n",
        "\n",
        "```\n",
        "loss=tf.keras.losses.<StringCassName>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpWpp8h4g_rE"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "model.compile(loss=None, optimizer=None, metrics=None)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510v0aVDXv1f"
      },
      "source": [
        "## Treino do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1udK8qps6PcG"
      },
      "source": [
        "<a name='exercise-5'></a>\n",
        "\n",
        "### **Exercício 5**\n",
        "\n",
        "Agora você pode treinar o modelo. Defina o número de épocas e observe as métricas retornadas em cada iteração. Você também pode encerrar a execução da célula se achar que o modelo já está funcionando bem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HoZwpGWhMB-"
      },
      "outputs": [],
      "source": [
        "# ALÉM DE DEFINIR O NÚMERO DE ÉPOCAS, NÃO ALTERE NENHUM OUTRO CÓDIGO\n",
        "\n",
        "### INICIE O CÓDIGO AQUI ###\n",
        "EPOCHS = None\n",
        "### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "steps_per_epoch = 4000//BATCH_SIZE\n",
        "validation_steps = 800//BATCH_SIZE\n",
        "test_steps = 200//BATCH_SIZE\n",
        "\n",
        "\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLNzLB3peeNG"
      },
      "source": [
        "**Resultado esperado:**\n",
        "\n",
        "Em geral, as perdas devem estar diminuindo e a precisão deve estar aumentando. Por exemplo, observar as primeiras 4 épocas deve gerar algo semelhante:\n",
        "\n",
        "```txt\n",
        "Epoch 1/70\n",
        "125/125 [==============================] - 6s 50ms/step - loss: 0.5542 - accuracy: 0.8635 - val_loss: 0.5335 - val_accuracy: 0.9427\n",
        "Epoch 2/70\n",
        "125/125 [==============================] - 6s 47ms/step - loss: 0.2315 - accuracy: 0.9425 - val_loss: 0.3362 - val_accuracy: 0.9427\n",
        "Epoch 3/70\n",
        "125/125 [==============================] - 6s 47ms/step - loss: 0.2118 - accuracy: 0.9426 - val_loss: 0.2592 - val_accuracy: 0.9427\n",
        "Epoch 4/70\n",
        "125/125 [==============================] - 6s 47ms/step - loss: 0.1782 - accuracy: 0.9431 - val_loss: 0.1770 - val_accuracy: 0.9432\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eih-Q7GoXzJe"
      },
      "source": [
        "## Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bTkaFM2X1gr"
      },
      "source": [
        "### Fazer previsões\n",
        "\n",
        "Vamos obter as previsões usando nosso conjunto de dados de teste como entrada e imprimir a forma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zENjQuK0luH5"
      },
      "outputs": [],
      "source": [
        "results = model.predict(test_dataset, steps=test_steps)\n",
        "\n",
        "print(results.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IReKPT_DZHjA"
      },
      "source": [
        "Como você pode ver, a forma resultante é `(192, 64, 84, 11)`. Isso significa que, para cada uma das 192 imagens que temos em nosso conjunto de teste, há 11 previsões geradas (ou seja, uma para cada classe: 0 a 1 mais o plano de fundo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBPBqnHyaSaG"
      },
      "source": [
        "Portanto, se você quiser ver a *probabilidade* de o pixel superior esquerdo da 1ª imagem pertencer à classe 0, poderá imprimir algo como `results[0,0,0,0]`. Se quiser ver a probabilidade de o mesmo pixel pertencer à classe 10, então imprima `results[0,0,0,10]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwFiR9WAf0Av"
      },
      "outputs": [],
      "source": [
        "print(results[0,0,0,0])\n",
        "print(results[0,0,0,10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKDsqzKEf28V"
      },
      "source": [
        "What we're interested in is to get the *index* of the highest probability of each of these 11 slices and combine them in a single image. We can do that by getting the [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) at this axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_Uj_uuV9TQt"
      },
      "outputs": [],
      "source": [
        "results = np.argmax(results, axis=3)\n",
        "\n",
        "print(results.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClaGbOLhdKD2"
      },
      "source": [
        "A nova matriz gerada por imagem agora especifica apenas os índices da classe com a maior probabilidade. Vejamos a classe de saída do pixel superior esquerdo. Como você deve ter observado anteriormente quando inspecionou o conjunto de dados, o canto superior esquerdo geralmente é apenas parte do plano de fundo (classe 10). Os dígitos reais estão escritos em algum lugar nas partes centrais da imagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBeBwvHQd2pZ"
      },
      "outputs": [],
      "source": [
        "print(results[0,0,0])\n",
        "\n",
        "# Mapa de previsão para a imagem 0\n",
        "print(results[0,:,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3FxyNgrg7IE"
      },
      "source": [
        "Usaremos esse array `results` quando avaliarmos nossas previsões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpKDUuAWX5Pj"
      },
      "source": [
        "### Métricas\n",
        "\n",
        "Nas aulas, mostramos duas maneiras de avaliar suas previsões. A *intersecção sobre união (IOU)* e a *dice score*. Lembre-se de que:\n",
        "\n",
        "\n",
        "$$IOU = \\frac{area\\_of\\_overlap}{area\\_of\\_union}$$\n",
        "<br>\n",
        "$$Dice Score = 2 * \\frac{area\\_of\\_overlap}{combined\\_area}$$\n",
        "\n",
        "O código abaixo faz isso para você. Um pequeno fator de suavização é introduzido nos denominadores para evitar uma possível divisão por zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKTpLmly_RXb"
      },
      "outputs": [],
      "source": [
        "def class_wise_metrics(y_true, y_pred):\n",
        "  '''\n",
        "  Calcula o IOU e a pontuação de dados por classe.\n",
        "\n",
        "  Args:\n",
        "    y_true (tensor) - mapas de rótulos de grouns-truth\n",
        "    y_pred (tensor) - mapas de rótulos previstos\n",
        "  '''\n",
        "  class_wise_iou = []\n",
        "  class_wise_dice_score = []\n",
        "\n",
        "  smoothing_factor = 0.00001\n",
        "\n",
        "  for i in range(n_classes):\n",
        "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "\n",
        "    iou = (intersection) / (combined_area - intersection + smoothing_factor)\n",
        "    class_wise_iou.append(iou)\n",
        "\n",
        "    dice_score =  2 * ((intersection) / (combined_area + smoothing_factor))\n",
        "    class_wise_dice_score.append(dice_score)\n",
        "\n",
        "  return class_wise_iou, class_wise_dice_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfWPwM4ZhHjE"
      },
      "source": [
        "### Visualize Predictions (Visualizar previsões)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkbsk_P1fpRM"
      },
      "outputs": [],
      "source": [
        "# Coloque um número aqui entre 0 e 191 para escolher uma imagem do conjunto de teste\n",
        "integer_slider = 105\n",
        "\n",
        "ds = test_dataset.unbatch()\n",
        "ds = ds.batch(200)\n",
        "images = []\n",
        "\n",
        "y_true_segments = []\n",
        "for image, annotation in ds.take(2):\n",
        "  y_true_segments = annotation\n",
        "  images = image\n",
        "\n",
        "\n",
        "iou, dice_score = class_wise_metrics(np.argmax(y_true_segments[integer_slider], axis=2), results[integer_slider])\n",
        "show_annotation_and_prediction(image[integer_slider], annotation[integer_slider], results[integer_slider], iou, dice_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiG9K4t6X9iZ"
      },
      "source": [
        "### Calcule a pontuação IOU e Dice Score do seu modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2706boF0CNNS"
      },
      "outputs": [],
      "source": [
        "cls_wise_iou, cls_wise_dice_score = class_wise_metrics(np.argmax(y_true_segments, axis=3), results)\n",
        "\n",
        "average_iou = 0.0\n",
        "for idx, (iou, dice_score) in enumerate(zip(cls_wise_iou[:-1], cls_wise_dice_score[:-1])):\n",
        "  print(\"Dígito {}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score))\n",
        "  average_iou += iou\n",
        "\n",
        "grade = average_iou * 10\n",
        "\n",
        "print(\"\\nPontuação é \" + str(grade))\n",
        "\n",
        "PASSING_GRADE = 60\n",
        "if (grade>PASSING_GRADE):\n",
        "  print(\"Passou!\")\n",
        "else:\n",
        "  print(\"Não Passou.Verifique seu modelo e treine novamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmyvoAynkgVw"
      },
      "source": [
        "**Parabéns por ter concluído este trabalho sobre segmentação de imagens!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "56d44d6a8424451b5ce45d1ae0b0b7865dc60710e7f74571dd51dd80d7829ee9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
