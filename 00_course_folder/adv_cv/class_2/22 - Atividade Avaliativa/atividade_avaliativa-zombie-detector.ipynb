{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/adv_cv/class_2/22%20-%20Atividade%20Avaliativa/atividade_avaliativa-zombie-detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "adaptado de [Visão computacional avançada com TensorFlow](https://www.coursera.org/learn/advanced-computer-vision-with-tensorflow?specialization=tensorflow-advanced-techniques) de [Laurence Moroney](https://laurencemoroney.com/) e [Andrew Ng](https://www.coursera.org/instructor/andrewng) , [DeepLearning.AI](https://www.deeplearning.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Detecção de Zumbis\n",
        "\n",
        "Nessa tarefa avaliativa você usará a API de detecção de objetos e treinará novamente o [RetinaNet](https://arxiv.org/abs/1708.02002) para detectar zumbis usando apenas 5 imagens de treinamento. Você configurará o modelo para restaurar os pesos pré-treinados e ajustará as camadas de classificação.\n",
        "\n",
        "\n",
        "<img src='https://github.com/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/adv_cv/class_2/22%20-%20Atividade%20Avaliativa/images/zombie_detect.jpg?raw=true' alt='zombie'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GKykVXq8elD"
      },
      "source": [
        "## Exercícios\n",
        "\n",
        "* [ Exercício 1 - Importar pacotes da API de detecção de objetos](#exercise-1)\n",
        "* [Exercício 2 - Visualize as imagens de treinamento](#exercise-2)\n",
        "* [Exercício 3 - Definir o dicionário de índice de categoria](#exercise-3)\n",
        "* [Exercício 4 - Baixar pontos de verificação](#exercise-4)\n",
        "* [Exercício 5.1 - Localizar e ler o arquivo de configuração](#exercise-5-1)\n",
        "* [Exercício 5.2 - Modificar a configuração do modelo](#exercise-5-2)\n",
        "* [Exercício 5.3 - Modificar model_config](#exercise-5-3)\n",
        "* [ Exercício 5.4 - Construir o modelo personalizado](#exercise-5-4)\n",
        "* [Exercício 6.1 - Definir pontos de controle para o preditor de caixa](#exercise-6-1)\n",
        "* [Exercício 6.2 - Definir o ponto de verificação do modelo temporário](#exercise-6-2)\n",
        "* [Exercício 6.3 - Restaurar o ponto de verificação](#exercise-6-2)\n",
        "* [Exercício 7 - Execute uma imagem fictícia para gerar as variáveis do modelo](#exercise-7)\n",
        "* [Exercício 8 - Definir hiperparâmetros de treinamento](#exercise-8)\n",
        "* [Exercício 9 - Selecione as variáveis da camada de previsão](#exercise-9)\n",
        "* [Exercício 10 - Definir a etapa de treinamento](#exercise-10)\n",
        "* [Exercício 11 - Pré-processar, prever e pós-processar uma imagem](#exercise-11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Instalação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-J7whqrkIl"
      },
      "source": [
        "Você começará instalando o Tensorflow 2 [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi28cqGGFWnY",
        "outputId": "d08c34b8-b448-400d-fcda-4bffb7dea998"
      },
      "outputs": [],
      "source": [
        "# descomente a próxima linha se quiser excluir um diretório de modelos existente\n",
        "!rm -rf ./models/\n",
        "\n",
        "# clonar o Tensorflow Model Garden\n",
        "!git clone --depth 1 https://github.com/tensorflow/models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "outputs": [],
      "source": [
        "# For compatibility. Pin tf-models-official version so it will use Tensorflow 2.15.\n",
        "!sed -i 's/tf-models-official>=2.5.1/tf-models-official==2.15.0/g' ./models/research/object_detection/packages/tf2/setup.py\n",
        "\n",
        "# Compile the Object Detection API protocol buffers and install the necessary packages\n",
        "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tUtyyVrkIt"
      },
      "source": [
        "## Importações\n",
        "\n",
        "Vamos agora importar os pacotes que você usará nesta tarefa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZcqD4NLdnf4",
        "outputId": "38423394-1b1a-4e17-f92d-97ac3a8a08e8"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version só existe no Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-auwvBQrkIw"
      },
      "source": [
        "<a name='exercise-1'></a>\n",
        "### **Exercício 1**: Importar pacotes da API de detecção de objetos\n",
        "\n",
        "Importe os módulos necessários do pacote `object_detection`.\n",
        "- Do pacote [utils](https://github.com/tensorflow/models/tree/master/research/object_detection/utils):\n",
        "  - [label_map_util](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/label_map_util.py)\n",
        "  - [config_util](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/config_util.py): Você usará para ler as configurações do modelo de um arquivo .config e, em seguida, modificar essa configuração\n",
        "  - [visualization_utils](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py): dê a esse arquivo o alias `viz_utils`, pois ele será usado em alguns códigos de visualização que serão fornecidos a você posteriormente.\n",
        "  - [colab_utils](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/colab_utils.py)\n",
        "- Do pacote [builders](https://github.com/tensorflow/models/tree/master/research/object_detection/builders):\n",
        "  - [model_builder](https://github.com/tensorflow/models/blob/master/research/object_detection/builders/model_builder.py): Constrói o modelo de acordo com a configuração do modelo que você especificar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YihOFxxrkIw"
      },
      "outputs": [],
      "source": [
        "### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)\n",
        "# Importar o módulo utilitário de mapa de rótulos\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "# Importar o módulo para ler e atualizar os arquivos de configuração.\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "# Módulo de importação para visualização. Use o alias `viz_utils`\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "# Módulo de importação para criar o modelo de detecção\n",
        "from object_detection.builders import model_builder\n",
        "### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "# Módulo de importação para utilitários no Colab\n",
        "from object_detection.utils import colab_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## Utilitários\n",
        "\n",
        "Você definirá algumas funções utilitárias para carregar imagens e plotar detecções. Esse código é fornecido para você."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Carrega uma imagem de um arquivo em uma matriz numpy.\n",
        "\n",
        "    Coloca a imagem em uma matriz numpy para alimentar o gráfico do tensorflow.\n",
        "    Observe que, por convenção, coloquei em uma matriz numpy com a forma\n",
        "    (altura, largura, canais), onde canais=3 para RGB.\n",
        "\n",
        "    Args:\n",
        "    path: um caminho de arquivo.\n",
        "\n",
        "    Retorna:\n",
        "    matriz numpy uint8 com formato (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "  \"\"\"Função de wrapper para visualizar as detecções.\n",
        "\n",
        "    Args:\n",
        "    image_np: matriz numérica uint8 com formato (img_height, img_width, 3)\n",
        "    boxes: uma matriz numérica de forma [N, 4]\n",
        "    classes: uma matriz numérica de formato [N]. Observe que os índices de classe são baseados em 1,\n",
        "          e correspondem às chaves no mapa de rótulos.\n",
        "    scores: uma matriz numpy de forma [N] ou None.  Se scores=None, então\n",
        "          essa função pressupõe que as caixas a serem plotadas são caixas de verdade\n",
        "          e plotará todas as caixas como pretas, sem classes ou pontuações.\n",
        "    category_index: um dict contendo dicionários de categorias (cada um contendo\n",
        "          índice de categoria `id` e nome de categoria `name`), codificados por índices de categoria.\n",
        "    figsize: tamanho da figura.\n",
        "    image_name: um nome para o arquivo de imagem.\n",
        "  \"\"\"\n",
        "  image_np_with_annotations = image_np.copy()\n",
        "\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_annotations,\n",
        "      boxes,\n",
        "      classes,\n",
        "      scores,\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      min_score_thresh=0.8)\n",
        "\n",
        "  if image_name:\n",
        "      plt.imsave(image_name, image_np_with_annotations)\n",
        "\n",
        "  else:\n",
        "      plt.imshow(image_np_with_annotations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaXL28TZfk1"
      },
      "source": [
        "## Faça o download dos dados de zumbis\n",
        "\n",
        "Agora você obterá 5 imagens de zumbis que serão usadas para treinamento.\n",
        "- Os zumbis estão hospedados em um bucket do Google.\n",
        "- Você pode fazer download e descompactar as imagens em um diretório local `training/` executando a célula abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqSFoJz2Cgzs",
        "outputId": "6d0f976f-fec4-4bfa-d0e0-71e2f3440c01"
      },
      "outputs": [],
      "source": [
        "# Descomente as próximas 2 linhas se quiser excluir um diretório zip e de treinamento existente\n",
        "!rm training-zombie.zip\n",
        "!rm -rf ./training\n",
        "\n",
        "# Faça o download das imagens\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/tensorflow-3-public/datasets/training-zombie.zip \\\n",
        "    -O ./training-zombie.zip\n",
        "\n",
        "# Descompacte em um diretório local\n",
        "local_zip = './training-zombie.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./training')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyzSGUDsrkI7"
      },
      "source": [
        "<a name='exercise-2'></a>\n",
        "\n",
        "### **Exercício 2**: Visualizar as imagens de treinamento\n",
        "\n",
        "Em seguida, você deverá inspecionar as imagens que acabou de baixar.\n",
        "\n",
        "* Substitua as instâncias de `None` abaixo para carregar e visualizar as 5 imagens de treinamento.\n",
        "* Você pode inspecionar o diretório *training* (usando o botão `Files` no lado esquerdo deste Colab) para ver os nomes dos arquivos das imagens de zumbis. Os caminhos para as imagens terão a seguinte aparência:\n",
        "\n",
        "```\n",
        "./training/training-zombie1.jpg\n",
        "./training/training-zombie2.jpg\n",
        "./training/training-zombie3.jpg\n",
        "./training/training-zombie4.jpg\n",
        "./training/training-zombie5.jpg\n",
        "```\n",
        "- Para definir caminhos de arquivos, você usará [os.path.join](https://www.geeksforgeeks.org/python-os-path-join-method/).  Por exemplo, se você quisesse criar o caminho './pasta_pai/nome_do_arquivo1.txt', poderia escrever:\n",
        "\n",
        "`os.path.join('parent_folder', 'file_name' + str(1) + '.txt')`\n",
        "\n",
        "* Você deverá ver as 5 imagens de treinamento depois de executar essa célula. Caso contrário, inspecione seu código, especialmente o `image_path`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "SQy3ND7EpFQM",
        "outputId": "d9678d1b-5291-45ce-ec1e-6df3d3711529"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)\n",
        "\n",
        "# Atribua o nome (string) do diretório que contém as imagens de treinamento\n",
        "train_image_dir = './training'\n",
        "\n",
        "# declarar uma lista vazia\n",
        "train_images_np = []\n",
        "\n",
        "# Executar um loop for para cada imagem\n",
        "for i in range(1, 6):\n",
        "    # Definir o caminho (string) para cada imagem\n",
        "    image_path = os.path.join(train_image_dir, 'training-zombie' + str(i) + '.jpg')\n",
        "    print(image_path)\n",
        "# carregar imagens em matrizes numpy e anexá-las a uma lista\n",
        "    train_images_np.append(load_image_into_numpy_array(image_path))\n",
        "### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "# Definir as configurações de plotagem via rcParams\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# plotar imagens\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "    plt.subplot(1, 5, idx+1)\n",
        "    plt.imshow(train_image_np)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqb_yjAo3cO_"
      },
      "source": [
        "<a name='gt_boxes_definition'></a>\n",
        "## Preparar dados para treinamento (opcional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdKtOxZGXoy1"
      },
      "source": [
        "Nesta seção, você criará suas caixas de ground-truth.\n",
        "\n",
        "Você pode desenhar suas próprias caixas ou usar uma lista pré-preenchida de coordenadas que forneço abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqDVC6epheJZ"
      },
      "outputs": [],
      "source": [
        "gt_boxes = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNpBspiRheJZ"
      },
      "source": [
        "#### Opção 1: desenhe suas próprias caixas de ground-truth\n",
        "Se quiser desenhar suas próprias caixas, execute a próxima célula e o código de teste a seguir. Caso contrário, pule estas células opcionais.\n",
        "\n",
        "* Desenhe uma caixa ao redor do zumbi em cada imagem.\n",
        "* Clique no botão `next image` para ir para a próxima imagem\n",
        "* Clique em \"enviar\" quando aparecer a mensagem \"Todas as imagens foram concluídas!!!\".\n",
        "\n",
        "- Certifique-se de não deixar a caixa delimitadora muito grande.\n",
        "  - Se a caixa for muito grande, o modelo poderá aprender as características do plano de fundo (por exemplo, porta, estrada, etc.) para determinar se há um zumbi ou não.\n",
        "- Inclua o zumbi inteiro dentro da caixa.\n",
        "- Como exemplo, vá até o início deste notebook para ver a caixa delimitadora em torno do zumbi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "-nEDRoUEcUgL",
        "outputId": "9cb460df-2727-4b50-f75a-0f5bd9d02cf3"
      },
      "outputs": [],
      "source": [
        "# Opção 1: desenhe suas próprias caixas de ground-truth\n",
        "\n",
        "# Anotar as imagens de treinamento\n",
        "colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMeXVKEEmT3B",
        "outputId": "3d2100b6-b8a5-474e-b74d-c161a72213d3"
      },
      "outputs": [],
      "source": [
        "gt_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsSVYhTVZZSP"
      },
      "outputs": [],
      "source": [
        "# Opção 1: desenhe suas próprias caixas de ground-truth\n",
        "# CÓDIGO DE TESTE:\n",
        "try:\n",
        "  assert(len(gt_boxes) == 5), \"Aviso: gt_boxes está vazio. Você clicou em `submit`?\"\n",
        "\n",
        "except AssertionError as e:\n",
        "  print(e)\n",
        "\n",
        "# verifica se há caixas para todas as 5 imagens\n",
        "for gt_box in gt_boxes:\n",
        "    try:\n",
        "      assert(gt_box is not None),\"Há menos de 5 conjuntos de coordenadas de caixa. \" \\\n",
        "                                  \"Execute novamente a célula acima para desenhar as caixas novamente.\\n\" \\\n",
        "                                  \"Como alternativa, você pode executar a próxima célula para carregar caixas pré-determinadas \" \\\n",
        "                                  \"caixas de verdade terrestre.\"\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "        break\n",
        "\n",
        "\n",
        "ref_gt_boxes = [\n",
        "        np.array([[0.27333333, 0.41500586, 0.74333333, 0.57678781]]),\n",
        "        np.array([[0.29833333, 0.45955451, 0.75666667, 0.61078546]]),\n",
        "        np.array([[0.40833333, 0.18288394, 0.945, 0.34818288]]),\n",
        "        np.array([[0.16166667, 0.61899179, 0.8, 0.91910903]]),\n",
        "        np.array([[0.28833333, 0.12543962, 0.835, 0.35052755]]),\n",
        "      ]\n",
        "\n",
        "for gt_box, ref_gt_box in zip(gt_boxes, ref_gt_boxes):\n",
        "    try:\n",
        "      assert(np.allclose(gt_box, ref_gt_box, atol=0.04)),\"Uma das caixas é muito grande ou muito pequena. \" \\\n",
        "                                                         \"Por favor, redesenhe e deixe a caixa mais apertada ao redor do zumbi.\"\n",
        "\n",
        "    except AssertionError as e:\n",
        "      print(e)\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMC13HrOrkI_"
      },
      "source": [
        "<a name='gt-boxes'></a>\n",
        "#### Opção 2: usar as caixas ground_truth fornecidas\n",
        "Você também pode usar essa lista se optar por não desenhar as caixas por conta própria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDJadmvdrkI_"
      },
      "outputs": [],
      "source": [
        "# Opção 2: usar as caixas de ground_truth fornecidas\n",
        "# Defina isso como `True` se quiser substituir as caixas que você desenhou\n",
        "override = False\n",
        "\n",
        "# Caixas delimitadoras para cada um dos 5 zumbis encontrados em cada imagem.\n",
        "# você pode usá-las em vez de desenhar as caixas você mesmo.\n",
        "ref_gt_boxes = [\n",
        "        np.array([[0.27333333, 0.41500586, 0.74333333, 0.57678781]]),\n",
        "        np.array([[0.29833333, 0.45955451, 0.75666667, 0.61078546]]),\n",
        "        np.array([[0.40833333, 0.18288394, 0.945, 0.34818288]]),\n",
        "        np.array([[0.16166667, 0.61899179, 0.8, 0.91910903]]),\n",
        "        np.array([[0.28833333, 0.12543962, 0.835, 0.35052755]]),\n",
        "      ]\n",
        "\n",
        "# se gt_boxes estiver vazio, use a referência\n",
        "if not gt_boxes or override is True:\n",
        "  gt_boxes = ref_gt_boxes\n",
        "\n",
        "# Se gt_boxes não contiver 5 coordenadas de caixa, use a referência\n",
        "for gt_box in gt_boxes:\n",
        "    try:\n",
        "      assert(gt_box is not None)\n",
        "\n",
        "    except:\n",
        "      gt_boxes = ref_gt_boxes\n",
        "\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olhzhCH5heJa"
      },
      "source": [
        "#### Veja as coordenadas de sua caixa de ground-truth\n",
        "Independentemente de você ter optado por desenhar suas próprias caixas ou usar as caixas fornecidas, verifique sua lista de coordenadas da caixa de ground-truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxd66yq-M9Va",
        "outputId": "77926cc5-68e4-46b5-e557-b2aabfdb3c16"
      },
      "outputs": [],
      "source": [
        "# imprima as coordenadas de suas caixas de ground-truth\n",
        "for gt_box in gt_boxes:\n",
        "  print(gt_box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhF7NjtTECqh"
      },
      "source": [
        "Abaixo, adicionamos as anotações de classe. Para simplificar, assumimos apenas uma única classe, embora deva ser fácil estendê-la para lidar com várias classes. Também converteremos tudo para o formato que o loop de treinamento espera (por exemplo, conversão para tensores, representações de um ponto, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeDQaSdrkJC"
      },
      "source": [
        "<a name='exercise-3'></a>\n",
        "\n",
        "### **Exercício 3**: Definir o dicionário de índice de categoria\n",
        "\n",
        "Você precisará informar ao modelo qual ID de classe inteira deve ser atribuído à categoria \"zombie\" e qual \"nome\" deve ser associado a esse ID.\n",
        "\n",
        "- zombie_class_id: Por convenção, os números inteiros de ID de classe começam a ser numerados a partir de 1, 2, 3 e assim por diante.\n",
        "  - Se houver uma classe de \"background\", ela poderá receber o número inteiro 0, mas, nesse caso, você está prevendo apenas uma classe de zumbis.\n",
        "  - Como você está prevendo apenas uma classe (zumbi), atribua `1` ao ID da classe zumbi.\n",
        "\n",
        "- category_index: Defina o dicionário `category_index`, que terá a mesma estrutura que esta:\n",
        "```\n",
        "{human_class_id :\n",
        "  {'id'  : human_class_id,\n",
        "   'name': 'human_so_far'}\n",
        "}\n",
        "```\n",
        "- Defina `category_index` semelhante ao dicionário de exemplo acima, exceto para zumbis.\n",
        "  - Isso será usado pelas funções seguintes para saber a classe `id` e o `nome` das imagens de zumbis.\n",
        "\n",
        "- num_classes: Como você está prevendo uma classe, atribua `1` ao número de classes que o modelo preverá.\n",
        "  - Isso será usado durante o pré-processamento de dados e novamente quando você configurar o modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWBqFVMcweF-"
      },
      "outputs": [],
      "source": [
        "### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)\n",
        "\n",
        "# Atribua o ID da classe zumbi\n",
        "zombie_class_id = 1\n",
        "\n",
        "# Definir um dicionário que descreve a classe zumbi\n",
        "category_index = {zombie_class_id: {'id': zombie_class_id, 'name': 'zombie'}}\n",
        "\n",
        "# Especifique o número de classes que o modelo irá prever\n",
        "num_classes = 1\n",
        "### TERMINE O CÓDIGO AQUI ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPy7vfCCrkJF",
        "outputId": "3a2b4280-8862-49f9-c41f-576e6aa6e148"
      },
      "outputs": [],
      "source": [
        "# CÓDIGO DE TESTE:\n",
        "\n",
        "print(category_index[zombie_class_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL0nj1wvrkJH"
      },
      "source": [
        "**Saída Esperada:**\n",
        "\n",
        "```txt\n",
        "{'id': 1, 'name': 'zombie'}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTvj7Q0oYk75"
      },
      "source": [
        "### Pré-processamento de dados\n",
        "Agora você fará o pré-processamento dos dados para que eles sejam formatados corretamente antes de serem alimentados no modelo:\n",
        "- Converter os rótulos de classe em representações de um único ponto\n",
        "- converter tudo (ou seja, imagens de treinamento, caixas gt e rótulos de classe) em tensores.\n",
        "\n",
        "Esse código é fornecido para você."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H68ot6BkrkJH",
        "outputId": "a0c33e67-b41f-4598-bddd-6bb1761cd867"
      },
      "outputs": [],
      "source": [
        "# O `label_id_offset` desloca todas as classes por um determinado número de índices;\n",
        "# fazemos isso para que o modelo receba rótulos one-hot em que as classes que não são\n",
        "# do plano de fundo começam a contar no índice zero.\n",
        "\n",
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "\n",
        "# Listas que contêm as classes codificadas com one-hot e as caixas de ground-truth\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "\n",
        "for (train_image_np, gt_box_np) in zip(train_images_np, gt_boxes):\n",
        "    # converter a imagem de treinamento em tensor, adicionar dimensão de lote e adicionar à lista\n",
        "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "        train_image_np, dtype=tf.float32), axis=0))\n",
        "    # converter matriz numérica em tensor e, em seguida, adicionar à lista\n",
        "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "    # Aplique o deslocamento para ter classes de ground-truth com índice zero\n",
        "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
        "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "    # Fazer codificação one-hot para classes de ground-truth\n",
        "    gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "        zero_indexed_groundtruth_classes, num_classes))\n",
        "\n",
        "print('Concluída a preparação dos dados.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3_Z3mJWN9KJ"
      },
      "source": [
        "## Visualize os zumbis com suas caixas delimitadoras de ground-truth\n",
        "\n",
        "Você deverá ver as 5 imagens de treinamento com as caixas delimitadoras depois de executar a célula abaixo. Caso contrário, execute novamente a [ferramenta de anotação] (#gt_boxes_definition) ou use a matriz `gt_boxes` pré-preenchida fornecida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "YBD6l-E4N71y",
        "outputId": "ce298337-a8fe-4e2b-e7f0-b194bf3f69d8"
      },
      "outputs": [],
      "source": [
        "# Dar às caixas uma pontuação de 100%\n",
        "dummy_scores = np.array([1.0], dtype=np.float32)\n",
        "\n",
        "# definir o tamanho da figura\n",
        "plt.figure(figsize=(30, 15))\n",
        "\n",
        "# use a função utilitária `plot_detections()` para desenhar as caixas de ground-truth\n",
        "for idx in range(5):\n",
        "  plt.subplot(2, 4, idx+1)\n",
        "  plot_detections(train_images_np[idx],\n",
        "    gt_boxes[idx],\n",
        "    np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "    dummy_scores, category_index)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghDAsqfoZvPh"
      },
      "source": [
        "## Faça o download do ponto de verificação que contém os pesos pré-treinados\n",
        "\n",
        "Em seguida, você fará o download do [RetinaNet](https://arxiv.org/abs/1708.02002) e o copiará dentro do diretório de detecção de objetos.\n",
        "\n",
        "Ao trabalhar com modelos que estão nas fronteiras da pesquisa, os modelos e os pontos de verificação podem ainda não estar organizados em um local central como o TensorFlow Garden (https://github.com/tensorflow/models).\n",
        "- Geralmente, você lerá uma postagem no blog dos pesquisadores, que normalmente fornecerão informações sobre:\n",
        "  - como usar o modelo\n",
        "  - onde baixar os modelos e os pontos de verificação pré-treinados.\n",
        "\n",
        "É uma boa prática fazer um pouco desse \"trabalho de detetive\", para que você se sinta mais confortável ao explorar novos modelos!  Portanto, tente as seguintes etapas:\n",
        "\n",
        "- Acesse o [TensorFlow Blog](https://blog.tensorflow.org/), onde os pesquisadores anunciam novas descobertas.\n",
        "- Na caixa de pesquisa na parte superior da página, procure por \"retinanet\".\n",
        "- Nos resultados da pesquisa, clique na postagem do blog intitulada \"TensorFlow 2 meets the Object Detection API\" (pode ser o primeiro resultado da pesquisa).\n",
        "- Dê uma olhada nesse blog e procure links para os pontos de verificação ou para os Colabs que mostrarão como usar os pontos de verificação.\n",
        "- Tente preencher a célula de código abaixo, que fazem o seguinte:\n",
        "  - Baixar o ponto de verificação compactado do SSD Resnet 50 versão 1, 640 x 640.\n",
        "  - Descompactar o arquivo tar\n",
        "  - Mova o ponto de verificação descompactado para `models/research/object_detection/test_data/`\n",
        "\n",
        "Se você quiser ajuda para começar, clique na célula \"Dicas iniciais\" para obter algumas dicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaKPLEUSheJa"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Dicas Iniciais</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "Dicas gerais para começar\n",
        "<ul>\n",
        "    <li>O link para o blog é <a href=\"https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html\">TensorFlow 2 meets the Object Detection API</a> </li>\n",
        "    <li>No blog, você encontrará o texto \"COCO pre-trained weights\" (Pesos pré-treinados do COCO), onde você encontra a uma lista de pontos de verificação no GitHub intitulada\n",
        "      <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\">TensorFlow 2 Detection Model Zoo<a>.\n",
        "    </li>\n",
        "    <li>\n",
        "    Se você ler o nome de cada ponto de verificação, encontrará o do SSD Resnet 50 versão 1, 640 por 640.\n",
        "    </li>\n",
        "    <li>\n",
        "    Se você clicar com o botão direito do mouse no link do ponto de verificação desejado, poderá salvar o endereço do link e usá-lo na célula de código abaixo para obter o ponto de verificação.\n",
        "    </li>\n",
        "    <li>Para obter mais dicas, clique na célula \"Mais dicas\"</li>\n",
        "</ul>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tBRiJjUheJa"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Mais Dicas</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "\n",
        "Mais Dicas\n",
        "<ul>\n",
        "    <li> Para saber como fazer o download do ponto de verificação, procure no blog os links para os tutoriais do Colab.\n",
        "    </li>\n",
        "    <li>\n",
        "        Por exemplo, o blog está vinculado a um \"Colab demo\" \"for inference\" intitulado <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\">Intro to Object Detection Colab</a>\n",
        "    </li>\n",
        "    <li>\n",
        "        No Colab, você verá a seção intitulada \"Build a detection model and load pre-trained model weights\" (Criar um modelo de detecção e carregar pesos de modelos pré-treinados), que é seguida por uma célula de código que mostra como fazer download, descompactar e realocar um ponto de verificação.  Use sintaxe semelhante, mas use o URL do ponto de verificação ssd resnet50 versão 1 640x640.\n",
        "    </li>\n",
        "    <li> Se estiver com dúvidas, clique na célula \"Ainda mais dicas\".\n",
        "    </li>\n",
        "</ul>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv9hTf4bheJa"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Ainda Mais Dicas</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "\n",
        "Ainda Mais Dicas\n",
        "<ul>\n",
        "    <li> A postagem do blog também tem um link para um notebook intitulado\n",
        "    <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb\">\n",
        "    Eager Few Shot Object Detection Colab</a> na parte em que fala sobre \"Colab demonstrations of eager mode compatible few-shot training\"\n",
        "    </li>\n",
        "    <li> Nesse notebook, procure a seção intitulada \"Create model and restore weights for all but last layer\" (Criar modelo e restaurar pesos para todas as camadas, exceto a última).\n",
        "    A célula de código abaixo mostra como fazer o download do ponto de verificação exato em que você está interessado.\n",
        "    </li>\n",
        "</ul>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn21-EelheJa"
      },
      "source": [
        "<a name='exercise-4'></a>\n",
        "### Exercício 4: Baixar pontos de verificação\n",
        "\n",
        "  - Faça o download do ponto de verificação compactado do SSD Resnet 50 versão 1, 640 x 640.\n",
        "  - Descompacte o arquivo tar\n",
        "  - Mova o ponto de verificação descompactado para `models/research/object_detection/test_data/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J16r3NChD-7",
        "outputId": "c732f669-ad0e-466f-841a-4651bc3b2b20"
      },
      "outputs": [],
      "source": [
        "\n",
        "### INICIE O CÓDIGO AQUI\n",
        "# Faça o download do ponto de verificação do SSD Resnet 50 versão 1, 640x640\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "\n",
        "\n",
        "# Descompactar o arquivo tar\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "\n",
        "# Copie o ponto de verificação para a pasta test_data models/research/object_detection/test_data/\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/\n",
        "### TERMINE O CÓDIGO AQUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgG_YT7UrkJQ"
      },
      "source": [
        "## Configurar o modelo\n",
        "Aqui, você configurará o modelo para esse caso de uso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3eHyNmxheJa"
      },
      "source": [
        "<a name='exercise-5-1'></a>\n",
        "\n",
        "### **Exercício 5.1**: Localizar e ler o arquivo de configuração\n",
        "\n",
        "#### pipeline_config\n",
        "- No Colab, no índice do lado esquerdo, clique no ícone de pasta para exibir o navegador de arquivos do espaço de trabalho atual.\n",
        "- Navegue até `models/research/object_detection/configs/tf2`.  A pasta tem vários arquivos .config.\n",
        "- Procure o arquivo correspondente ao ssd resnet 50 versão 1 640x640.\n",
        "- Você pode clicar duas vezes no arquivo de configuração para visualizar seu conteúdo. Isso pode ajudá-lo ao completar as próximas células de código para configurar o modelo.\n",
        "- Defina `pipeline_config` como uma string que contenha o caminho completo para o arquivo de configuração do resnet, em outras palavras: `models/research/.../... .config`\n",
        "\n",
        "\n",
        "#### configs\n",
        "Se você observar o módulo [config_util](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/config_util.py) que importou, ele contém a seguinte função:\n",
        "\n",
        "```\n",
        "def get_configs_from_pipeline_file(pipeline_config_path, config_override=None):\n",
        "```\n",
        "- Utilize essa função para carregar a configuração de seu `pipeline_config`.\n",
        "  - `configs` agora conterá um dicionário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59sEM6wXheJa",
        "outputId": "badaa8d6-cf2c-4fd8-a082-91ecd681f83b"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print('Criação de modelo e restauração de pesos para ajuste fino...', flush=True)\n",
        "num_classes = 1\n",
        "\n",
        "### COMECE O CÓDIGO AQUI ###\n",
        "# Defina o caminho para o arquivo .config do ssd resnet 50 v1 640x640\n",
        "#pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "pipeline_config = \"/content/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\"\n",
        "\n",
        "# Carregar o arquivo de configuração em um dicionário\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "### TERMINE O CÓDIGO AQUI ###\n",
        "# Veja como são as configurações\n",
        "configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAkgGnMcheJa"
      },
      "source": [
        "<a name='exercise-5-2'></a>\n",
        "\n",
        "### **Exercício 5.2**: Obter a configuração do modelo\n",
        "\n",
        "#### model_config\n",
        "- No dicionário `configs`, acesse o objeto associado à chave 'model'.\n",
        "- O `model_config` agora contém um objeto do tipo `object_detection.protos.model_pb2.DetectionModel`.\n",
        "- Se você imprimir `model_config`, verá algo parecido com isto:\n",
        "\n",
        "```\n",
        "ssd {\n",
        "  num_classes: 90\n",
        "  image_resizer {\n",
        "    fixed_shape_resizer {\n",
        "      height: 640\n",
        "      width: 640\n",
        "    }\n",
        "  }\n",
        "  feature_extractor {\n",
        "...\n",
        "...\n",
        "  freeze_batchnorm: false\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps3AzqBvheJa",
        "outputId": "28330868-4155-4966-ac3b-9bc2709937e7"
      },
      "outputs": [],
      "source": [
        "### COMECE O CÓDIGO AQUI ###\n",
        "# Leia o objeto armazenado na chave 'model' do dicionário configs\n",
        "model_config = configs['model']\n",
        "\n",
        "### FINALIZE O CÓDIGO AQUI\n",
        "# veja como é o model_config\n",
        "model_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxHqQK1eheJa"
      },
      "source": [
        "<a name='exercise-5-3'></a>\n",
        "\n",
        "### **Exercício 5.3**: Modificar model_config\n",
        "- Modifique o num_classes do padrão `90` para o `num_classes` que você definiu anteriormente neste notebook.\n",
        "  - O num_classes está aninhado no ssd.  Você precisará usar a notação de ponto 'obj.x' e NÃO a notação de colchete obj['x']` para acessar o num_classes.\n",
        "- Congelar a normalização em lote\n",
        "  - A normalização em lote não está congelada na configuração padrão.\n",
        "  - Se você inspecionar o objeto `model_config`, verá que `freeze_batchnorm` está aninhado em `ssd`, assim como `num_classes`.\n",
        "  - Congele a normalização de lote definindo o campo relevante como `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyT4BUbaMeG-",
        "outputId": "8f5ca65b-7158-4b3d-c5fb-87a0e2a9672c"
      },
      "outputs": [],
      "source": [
        "### COMECE O CÓDIGO AQUI ###\n",
        "# Modifique o número de classes de seu padrão de 90\n",
        "model_config.ssd.num_classes = num_classes\n",
        "\n",
        "# Congelar a normalização do lote\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "\n",
        "### FINALIZE O CÓDIGO AQUI\n",
        "\n",
        "# Veja como o model_config fica agora depois que você o personalizou!\n",
        "model_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFnuT2rfheJa"
      },
      "source": [
        "## Construir o modelo\n",
        "\n",
        "Lembre-se de que você importou o [model_builder](https://github.com/tensorflow/models/blob/master/research/object_detection/builders/model_builder.py).\n",
        "- Você usará o `model_builder` para criar o modelo de acordo com as configurações que você acabou de baixar e personalizar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvtrqsJIrkJT"
      },
      "source": [
        "<a name='exercise-5.4'></a>\n",
        "\n",
        "### **Exercício 5.4**: Criar o modelo personalizado\n",
        "\n",
        "#### model_builder\n",
        "O model_builder tem uma função `build`:\n",
        "\n",
        "```\n",
        "def build(model_config, is_training, add_summaries=True):\n",
        "\n",
        "```\n",
        "- model_config: Defina isso como a configuração do modelo que você acabou de personalizar.\n",
        "- is_training: Defina esse valor como `True`.\n",
        "- Você pode manter o valor padrão para o parâmetro restante.\n",
        "- Observe que levará algum tempo para criar o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfoRTeV_rkJT",
        "outputId": "77f01d60-88d7-41a7-c8cb-46093b90048e"
      },
      "outputs": [],
      "source": [
        "### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)\n",
        "### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "print(type(detection_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuUBhoegrkJY"
      },
      "source": [
        "**Saída esperada**:\n",
        "\n",
        "```txt\n",
        "<class 'object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch'>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C97Zo0OJnOf"
      },
      "source": [
        "## Restaurar pesos do ponto de verificação\n",
        "\n",
        "Agora, você restaurará seletivamente os pesos do seu ponto de verificação.\n",
        "- Seu objetivo final é criar um modelo personalizado que reutilize partes, mas não todas, das camadas do RetinaNet (atualmente armazenadas na variável `detection_model`).\n",
        "  - As partes do RetinaNet que você deseja reutilizar são:\n",
        "    - Camadas de extração de características\n",
        "    - Camada de previsão de regressão de caixa delimitadora\n",
        "  - A parte do RetinaNet que você não deseja reutilizar é a camada de previsão de classificação (já que você definirá e treinará sua própria camada de classificação específica para zumbis).\n",
        "  - Para as partes do RetinaNet que deseja reutilizar, você também restaurará os pesos do ponto de verificação que selecionou."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISYvzjTcheJa"
      },
      "source": [
        "#### Inspecione o detection_model\n",
        "Primeiro, dê uma olhada no tipo do detection_model e em sua classe Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M81_BgsuheJa",
        "outputId": "bf12a06e-0f36-46ba-d901-f403329beea3"
      },
      "outputs": [],
      "source": [
        "# Execute isso para verificar o tipo de detection_model\n",
        "detection_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfxROGsrheJa"
      },
      "source": [
        "#### Localize o código-fonte de detection_model\n",
        "\n",
        "Você verá que o tipo do modelo é `object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch`.\n",
        "Faça um trabalho de detetive e abra o código-fonte dessa classe no repositório do GitHub.  Lembre-se de que, no início desta tarefa, você clonou a partir deste repositório:  [TensorFlow Models](https://github.com/tensorflow/models).\n",
        "- Navegue por estas subpastas: models -> research -> object_detection.\n",
        "  - Se não conseguir achar, confira este link: [object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)\n",
        "- Dê uma olhada nessa pasta \"object_detection\" e procure as pastas restantes para navegar com base no tipo de classe do detection_model: object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch\n",
        "  - Com sorte, você encontrará a pasta meta_architectures e, dentro dela, notará um arquivo chamado `ssd_meta_arch.py`.\n",
        "  - Abra e visualize esse arquivo [ssd_meta_arch.py](https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py).\n",
        "\n",
        "#### Exibir as variáveis em detection_model\n",
        "Agora, verifique as variáveis de classe que estão em `detection_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6McB9xTheJa",
        "outputId": "ae889827-fe4c-4cd1-f0da-f38a9194daed"
      },
      "outputs": [],
      "source": [
        "vars(detection_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvLX9gheheJa"
      },
      "source": [
        "Você verá que detection_model contém diversas variáveis:\n",
        "\n",
        "Duas delas serão relevantes para você:\n",
        "```\n",
        "...\n",
        "_box_predictor': <object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x7f5205eeb1d0>,\n",
        "...\n",
        "_feature_extractor': <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f52040f1ef0>,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE907TPoheJa"
      },
      "source": [
        "#### Inspecione o `_feature_extractor`\n",
        "\n",
        "Dê uma olhada no código [ssd_meta_arch.py](https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py).\n",
        "```\n",
        "# Linha 303\n",
        "feature_extractor: a SSDFeatureExtractor object.\n",
        "```\n",
        "Também\n",
        "```\n",
        "# Linha 381\n",
        "self._feature_extractor = feature_extractor\n",
        "```\n",
        "Portanto, `detection_model._feature_extractor` é um extrator de características que você desejará reutilizar no seu modelo de detector de zumbis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKgYA0oheJa"
      },
      "source": [
        "#### Inspecione o `_box_predictor`\n",
        "\n",
        "- Visualize o arquivo [ssd_meta_arch.py](https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py) (que é o código-fonte do detection_model)\n",
        "- Observe que no construtor __init__ da classe SSDMetaArch(model.DetectionModel),\n",
        "```\n",
        "...\n",
        "box_predictor: a box_predictor.BoxPredictor object\n",
        "...\n",
        "self._box_predictor = box_predictor\n",
        "```\n",
        "#### Inspecionar _box_predictor\n",
        "Dê uma olhada no tipo de classe de`detection_model._box_predictor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToM0KgeMheJa",
        "outputId": "c6af89f5-0640-4bf1-ddfa-05197c9eeb8d"
      },
      "outputs": [],
      "source": [
        "# visualizar o tipo de _box_predictor\n",
        "detection_model._box_predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAxQGzZ1heJa"
      },
      "source": [
        "Você verá que o tipo de classe de _box_predictor é\n",
        "```\n",
        "object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor\n",
        "```\n",
        "Você pode navegar pelo repositório do GitHub até este caminho:\n",
        "- [objection_detection/predictors](https://github.com/tensorflow/models/tree/master/research/object_detection/predictors)\n",
        "- Observe que há um arquivo chamado convolutional_keras_box_predictor.py.  Abra esse arquivo.\n",
        "\n",
        "#### Exibir variáveis em `_box_predictor`\n",
        "Visualize também as variáveis contidas em _box_predictor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od3eQek-heJa",
        "outputId": "f002f6e1-3a48-4d2c-9bbe-01ccfbbb0ee2"
      },
      "outputs": [],
      "source": [
        "vars(detection_model._box_predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ79nKuoheJa"
      },
      "source": [
        "Entre as variáveis listadas, algumas serão relevantes para você:\n",
        "\n",
        "```\n",
        "...\n",
        "_base_tower_layers_for_heads\n",
        "...\n",
        "_box_prediction_head\n",
        "...\n",
        "_prediction_heads\n",
        "```\n",
        "\n",
        "No código-fonte do [convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py) que você acabou de abrir, examine o código-fonte para ter uma ideia do que essas três variáveis representam.\n",
        "\n",
        "#### Inspecionar `base_tower_layers_for_heads`\n",
        "Se você observar o arquivo [convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py), verá o seguinte:\n",
        "```\n",
        "# linha 307\n",
        "self._base_tower_layers_for_heads = {\n",
        "        BOX_ENCODINGS: [],\n",
        "        CLASS_PREDICTIONS_WITH_BACKGROUND: [],\n",
        "    }\n",
        "```\n",
        "- `base_tower_layers_for_heads` é um dicionário com dois pares de valores-chave.\n",
        "  - `BOX_ENCODINGS`: aponta para uma lista de camadas\n",
        "  - `CLASS_PREDICTIONS_WITH_BACKGROUND`: aponta para uma lista de camadas\n",
        "  - Se você examinar o código, verá que, em ambos os casos, as listas são preenchidas com todas as camadas que aparecem ANTES da camada de previsão.\n",
        "```\n",
        "# Linha 385\n",
        "# Stack the base_tower_layers in the order of conv_layer, batch_norm_layer\n",
        "    # and activation_layer\n",
        "    base_tower_layers = []\n",
        "    for i in range(self._num_layers_before_predictor):\n",
        "```\n",
        "Portanto, `detection_model.box_predictor._base_tower_layers_for_heads` contém:\n",
        "- As camadas para a previsão antes da previsão final da caixa delimitadora\n",
        "- As camadas para a previsão antes da previsão final da classe.\n",
        "\n",
        "Você deverá usá-las em seu modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4lxibHheJa"
      },
      "source": [
        "#### Inspecione `_box_prediction_head`\n",
        "Se você examinar novamente o arquivo [convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py), verá o seguinte\n",
        "\n",
        "```\n",
        "# Line 248\n",
        "box_prediction_head: The head that predicts the boxes.\n",
        "```\n",
        "Portanto, `detection_model.box_predictor._box_prediction_head` aponta para a camada de previsão de caixa delimitadora, que você desejará usar em seu modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGEGC-d-heJa"
      },
      "source": [
        "#### Inspecione `_prediction_heads`\n",
        "\n",
        "Se você examinar novamente o arquivo [convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py), verá o seguinte\n",
        "```\n",
        "# Line 121\n",
        "self._prediction_heads = {\n",
        "        BOX_ENCODINGS: box_prediction_heads,\n",
        "        CLASS_PREDICTIONS_WITH_BACKGROUND: class_prediction_heads,\n",
        "    }\n",
        "```\n",
        "Você também verá esta documentação\n",
        "```\n",
        "# Line 83\n",
        "class_prediction_heads: A list of heads that predict the classes.\n",
        "```\n",
        "\n",
        "Portanto, `detection_model.box_predictor._prediction_heads` é um dicionário que aponta para as duas camadas de previsão:\n",
        "- A camada que prevê as caixas delimitadoras\n",
        "- A camada que prevê a classe (categoria).\n",
        "\n",
        "#### Quais camadas você reutilizará?\n",
        "Lembre-se de que você está reutilizando o modelo para extração de recursos e detecção de caixas delimitadoras.\n",
        "- Você criará sua própria camada de classificação e a treinará em imagens de zumbis.\n",
        "- Portanto, você não precisará reutilizar a camada de previsão de classe do `detection_model`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGtYFYkQheJa"
      },
      "source": [
        "## Definir pontos de verificação para as camadas desejadas\n",
        "Agora você isolará as camadas do `detection_model` que deseja reutilizar para que possa restaurar os pesos apenas para essas camadas.\n",
        "- Primeiro, defina os pontos de verificação para o preditor de caixa\n",
        "- Em seguida, defina pontos de verificação para o modelo, que apontará para esse ponto de verificação do preditor de caixa, bem como para as camadas de extração de recursos.\n",
        "\n",
        "Use [tf.train.Checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint).\n",
        "\n",
        "Como um lembrete de como usar o tf.train.Checkpoint:\n",
        "\n",
        "```\n",
        "tf.train.Checkpoint(\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "Faça de conta que o `detection_model` contém essas variáveis para as quais você deseja restaurar os pesos:\n",
        "- `detection_model._ice_cream_sundae`\n",
        "- `detection_model._pies._apple_pie`\n",
        "- `detection_model._pies._pecan_pie`\n",
        "\n",
        "Observe que as tortas estão aninhadas dentro de `._pies`.\n",
        "\n",
        "Se quiser apenas as variáveis de sundae de sorvete e torta de maçã (e não a torta de nozes), faça o seguinte:\n",
        "\n",
        "```\n",
        "tmp_pies_checkpoint = tf.train.Checkpoint(\n",
        "  _apple_pie = detection_model._pies._apple_pie\n",
        ")\n",
        "```\n",
        "\n",
        "Em seguida, para conectá-los em um grafo de nós, faça o seguinte:\n",
        "```\n",
        "tmp_model_checkpoint = tf.train.Checkpoint(\n",
        "  _pies = tmp_pies_checkpoint,\n",
        "  _ice_cream_sundae = detection_model._ice_cream_sundae\n",
        ")\n",
        "```\n",
        "\n",
        "Por fim, defina um ponto de verificação que use a chave `model` e receba o ponto de verificação tmp_model_checkpoint.\n",
        "\n",
        "```\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "  model = tmp_model_checkpoint\n",
        ")\n",
        "```\n",
        "\n",
        "Em seguida, você estará pronto para restaurar os pesos do ponto de verificação que baixou.\n",
        "\n",
        "Experimente isso passo a passo!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BK06DyFheJa"
      },
      "source": [
        "<a name='exercise-6-1'></a>\n",
        "### Exercício 6.1: Definir pontos de verificação para o preditor de caixa\n",
        "\n",
        "- Defina `box_predictor_checkpoint` como ponto de verificação para essas duas camadas do preditor de caixa do `detection_model`:\n",
        "  - A camada da torre de base (as camadas que precedem as camadas de previsão de classe e de previsão de caixa delimitadora).\n",
        "  - A cabeça de previsão de caixa (a camada de previsão para caixas delimitadoras).\n",
        "- Observe que você não incluirá a camada de previsão de classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KPWWYbyrkJY"
      },
      "outputs": [],
      "source": [
        "### COMECE O CÓDIGO AQUI ###\n",
        "\n",
        "tmp_box_predictor_checkpoint = tf.compat.v2.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
        "# (ou seja, a cabeça de classificação que *não* restauraremos)\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "\n",
        "### TERMINE O CÓDIGO AQUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlefNC_XheJa",
        "outputId": "7791c724-1e8f-426b-aafc-1507d681e9df"
      },
      "outputs": [],
      "source": [
        "# Verificar o tipo de dados desse ponto de verificação\n",
        "type(tmp_box_predictor_checkpoint)\n",
        "\n",
        "# Saída esperada:\n",
        "# tensorflow.python.training.tracking.util.Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9X0BfufheJa",
        "outputId": "580fca97-5d25-46a7-d187-4972076a5633"
      },
      "outputs": [],
      "source": [
        "# Verificar as variáveis desse ponto de verificação\n",
        "vars(tmp_box_predictor_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORTI9PU6heJa"
      },
      "source": [
        "#### Saída esperada\n",
        "Você deve esperar ver uma lista de variáveis que incluem o seguinte:\n",
        "```\n",
        "```\n",
        " ...\n",
        " '_base_tower_layers_for_heads': {'box_encodings': ListWrapper([]),\n",
        "  'class_predictions_with_background': ListWrapper([])},\n",
        " '_box_prediction_head': <object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7f49d0234450>,\n",
        " ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqMyPTU1rkJa"
      },
      "source": [
        "<a name='exercise-6-2'></a>\n",
        "### Exercício 6.2: Definir o ponto de verificação do modelo temporário**\n",
        "\n",
        "Agora, defina `tmp_model_checkpoint` de modo que ele aponte para essas duas camadas:\n",
        "- O extrator de características do modelo de detecção.\n",
        "- O ponto de verificação do preditor de caixa temporário que você acabou de definir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCqGfrawrkJa"
      },
      "outputs": [],
      "source": [
        "### COMECE O CÓDIGO AQUI ###\n",
        "\n",
        "tmp_model_checkpoint =  tf.compat.v2.train.Checkpoint(\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          _box_predictor=tmp_box_predictor_checkpoint)\n",
        "\n",
        "\n",
        "### TERMINE O CÓDIGO AQUI ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QXZBgRtheJa",
        "outputId": "385cc2c7-6655-4c58-e327-6ba4301f5bc7"
      },
      "outputs": [],
      "source": [
        "# Verificar o tipo de dados desse ponto de verificação\n",
        "type(tmp_model_checkpoint)\n",
        "\n",
        "# Saída esperada\n",
        "# Tensorflow.python.training.tracking.util.Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSxZqzU4heJa",
        "outputId": "bce80ae5-0f0b-4b3c-e20a-ca4c80cdbf49"
      },
      "outputs": [],
      "source": [
        "# Verificar as vars desse ponto de verificação\n",
        "vars(tmp_model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sJzbL4iheJa"
      },
      "source": [
        "#### Saída esperada\n",
        "Entre as variáveis desse ponto de verificação, você deve ver:\n",
        "```\n",
        "'_box_predictor': <tensorflow.python.training.tracking.util.Checkpoint at 0x7fefac044a20>,\n",
        " '_feature_extractor': <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7fefac0240b8>,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_BtL4ZZKVvV"
      },
      "source": [
        "<a name='exercise-6-3'></a>\n",
        "### Exercício 6.3: Restaurar o ponto de verificação\n",
        "\n",
        "Agora você pode restaurar o ponto de verificação.\n",
        "\n",
        "Primeiro, localize e defina o `checkpoint_path`\n",
        "\n",
        "- checkpoint_path:\n",
        "  - Usando o navegador \"files\" no lado esquerdo do Colab, navegue até `models -> research -> object_detection -> test_data`.\n",
        "  - Se você tiver concluído a célula de código anterior que faz download e move o ponto de verificação, verá uma subpasta chamada \"checkpoint\".\n",
        "    - A pasta \"checkpoint\" contém três arquivos:\n",
        "      - checkpoint\n",
        "      - ckpt-0.data-00000-of-00001\n",
        "      - ckpt-0.index\n",
        "    - Defina checkpoint_path como o caminho para o caminho completo `models/.../ckpt-0`\n",
        "      - Observe que você não deseja incluir uma extensão de arquivo após `ckpt-0`.\n",
        "    - **IMPORTANTE**: Não defina o caminho para incluir a extensão `.index` no nome do arquivo de ponto de verificação.\n",
        "      - Se você o definir como `ckpt-0.index`, não haverá nenhuma mensagem de erro imediata, mas, mais tarde, durante o treinamento, você perceberá que a perda do modelo não melhora, o que significa que os pesos pré-treinados não foram restaurados corretamente.\n",
        "\n",
        "Em seguida, defina um último ponto de verificação usando `tf.train.Checkpoint()`.\n",
        "- Para o argumento de palavra-chave única,\n",
        "  - Defina a chave como `model=`\n",
        "  - Defina o valor como o ponto de verificação do modelo temporário que você acabou de definir.\n",
        "- **IMPORTANTE**: Você precisará definir o argumento da palavra-chave como `model=` e não como `detection_model=`.\n",
        "- Se você definir esse argumento de palavra-chave como qualquer outra coisa, ele não mostrará um erro imediato, mas quando você treinar o modelo nas imagens de zumbis, a perda do modelo não diminuirá (o modelo não aprenderá).\n",
        "\n",
        "Por fim, chame a função `.restore()` desse ponto de verificação, passando o caminho para o ponto de verificação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elkvDUUarkJg",
        "outputId": "82e169e6-5223-408a-97f4-d75f651220ff"
      },
      "outputs": [],
      "source": [
        "### COMECE O CÓDIGO AQUI ###\n",
        "\n",
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "# Definir um ponto de verificação que define `model` como o ponto de verificação do modelo temporário\n",
        "checkpoint = tf.compat.v2.train.Checkpoint(model=tmp_model_checkpoint)\n",
        "\n",
        "# Restaurar o ponto de verificação no caminho do ponto de verificação\n",
        "checkpoint.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "### TERMINE O CÓDIGO AQUI ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k6oFoxTrkJi"
      },
      "source": [
        "<a name='exercise-7'></a>\n",
        "### **Exercício 7**: Executar uma imagem fictícia para gerar as variáveis do modelo\n",
        "\n",
        "Execute uma imagem fictícia no modelo para que as variáveis sejam criadas. Precisamos selecionar as variáveis treináveis posteriormente no Exercício 9 e, no momento, elas ainda estão vazias. Tente executar `len(detection_model.trainable_variables)` em uma célula de código e você obterá `0`. Passaremos uma imagem fictícia por meio da passagem direta para criar essas variáveis.\n",
        "\n",
        "Lembre-se de que `detection_model` é um objeto do tipo [object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch]()\n",
        "\n",
        "Os métodos importantes que estão disponíveis no objeto `detection_model` são:\n",
        "- [preprocess()](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L459):\n",
        "    - recebe um tensor que representa uma imagem e retorna\n",
        "    - retorna `imagem, formas`\n",
        "    - Para a imagem fictícia, você pode declarar um [tensor of zeros](https://www.tensorflow.org/api_docs/python/tf/zeros) que tenha uma forma que o método `preprocess()` possa aceitar (ou seja, [batch, height, width, channels]).\n",
        "    - Lembre-se de que suas imagens têm dimensões de 640 x 640 x 3.\n",
        "    - Você pode passar um lote de 1 ao criar a imagem fictícia.\n",
        "\n",
        "- [predict()](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L525)\n",
        "  - recebe `image, shapes` que são criados pela chamada da função `preprocess()`.\n",
        "  - retorna uma previsão em um dicionário Python\n",
        "  - isso passará a imagem fictícia pela passagem direta da rede e criará as variáveis do modelo\n",
        "\n",
        "- [postprocess()](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L655)\n",
        "  - Recebe o prediction_dict e as formas\n",
        "  - retorna um dicionário de previsões pós-processadas de objetos detectados (\"detections\").\n",
        "\n",
        "\n",
        "**Nota**: Use os nomes de variáveis recomendados, que incluem o prefixo `tmp_`, uma vez que essas variáveis não serão usadas posteriormente, mas você definirá variáveis com nomes semelhantes mais tarde para fazer previsões em imagens reais de zumbis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MANTSP9LrkJi",
        "outputId": "df77f49d-eaf1-431f-b0eb-cb63bc782e66"
      },
      "outputs": [],
      "source": [
        "### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)###\n",
        "\n",
        "# use o método `preprocess()` do modelo de detecção e passe uma imagem fictícia\n",
        "tmp_image, tmp_shapes =  detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "\n",
        "# Executar uma previsão com a imagem e as formas pré-processadas\n",
        "tmp_prediction_dict =  detection_model.predict(tmp_image, tmp_shapes)\n",
        "\n",
        "# Pós-processar as previsões em detecções finais\n",
        "tmp_detections = detection_model.postprocess(tmp_prediction_dict, tmp_shapes)\n",
        "\n",
        "### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "print('Pesos restaurados!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0KvPP-krkJn",
        "outputId": "59b7f1ad-eea8-4f75-b8a2-0b1bd6345788"
      },
      "outputs": [],
      "source": [
        "# Código de Teste:\n",
        "assert len(detection_model.trainable_variables) > 0, \"Passe uma imagem fictícia para criar as variáveis treináveis.\"\n",
        "\n",
        "print(detection_model.weights[0].shape)\n",
        "print(detection_model.weights[231].shape)\n",
        "print(detection_model.weights[462].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCJXBqROrkJp"
      },
      "source": [
        "**Saída esperada**:\n",
        "\n",
        "```txt\n",
        "(3, 3, 256, 24)\n",
        "(512,)\n",
        "(256,)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCkWmdoZZ0zJ"
      },
      "source": [
        "## Loop de treinamento personalizado no modo Eager\n",
        "\n",
        "Com os dados e o modelo já configurados, você pode prosseguir com a configuração do treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njlV4PoPrkJq"
      },
      "source": [
        "<a name='exercise-8'></a>\n",
        "### **Exercício 8**: Definir hiperparâmetros de treinamento\n",
        "\n",
        "Defina uma taxa de aprendizado e um otimizador apropriados para o treinamento.\n",
        "\n",
        "- batch_size: você pode usar 4\n",
        "  - É possível aumentar o tamanho do lote para até 5, já que você tem apenas 5 imagens para treinamento.\n",
        "- num_batches: Você pode usar 100\n",
        "  - Você pode aumentar o número de lotes, mas o treinamento levará mais tempo para ser concluído.\n",
        "- learning_rate: Você pode usar 0,01\n",
        "  - Quando você executar o loop de treinamento mais tarde, observe como a perda inicial AUMENTA` antes de diminuir.\n",
        "  - Você pode tentar uma taxa de aprendizado menor para ver se consegue evitar esse aumento de perda.\n",
        "- otimizador: você pode usar [tf.keras.optimizers.SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD)\n",
        "  - Definir a taxa de aprendizado\n",
        "  - Defina o momentum como 0,9\n",
        "\n",
        "O treinamento será bastante rápido, portanto, recomendamos que você faça alguns experimentos com esses hiperparâmetros!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyHoF4mUrv5-",
        "outputId": "c305969d-bdea-4b72-8df5-d1230e2edf4a"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)\n",
        "\n",
        "# Definir o batch_size\n",
        "batch_size = 4\n",
        "\n",
        "# Definir o número de lotes\n",
        "num_batches = 100\n",
        "\n",
        "# Definir a taxa de aprendizado\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Definir o otimizador e passar o learning_rate\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "### TERMINE O CÓDIGO AQUI ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-MaJmHmrkJs"
      },
      "source": [
        "## Escolha as camadas para fazer o ajuste fino\n",
        "\n",
        "Para usar a aprendizagem por transferência e os pesos pré-treinados, você treinará apenas algumas partes do modelo de detecção, ou seja, as últimas camadas de previsão.\n",
        "- Reserve um minuto para inspecionar as camadas do `detection_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91r2dk-7heJb",
        "outputId": "b038d941-d44f-41d2-f6c4-846b4e3aba50"
      },
      "outputs": [],
      "source": [
        "# Inspecionar as camadas do detection_model\n",
        "for i,v in enumerate(detection_model.trainable_variables):\n",
        "    print(f\"i: {i} \\t name: {v.name} \\t shape:{v.shape} \\t dtype={v.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wtU67JZheJb"
      },
      "source": [
        "Observe que há algumas camadas cujos nomes têm o seguinte prefixo:\n",
        "```\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/BoxPredictionTower\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/ClassPredictionTower\n",
        "...\n",
        "```\n",
        "\n",
        "Entre elas, quais você acha que são as camadas de previsão no \"final\" do modelo?\n",
        "- Lembre-se de que, ao inspecionar o código-fonte para restaurar os pontos de verificação ([convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py)), você notou que:\n",
        "  - `_base_tower_layers_for_heads`: refere-se às camadas que são colocadas logo antes da camada de previsão\n",
        "  - `_box_prediction_head`: refere-se à camada de previsão para as caixas delimitadoras\n",
        "  - `_prediction_heads`: refere-se ao conjunto de camadas de previsão (tanto para classificação quanto para caixas delimitadoras)\n",
        "\n",
        "\n",
        "Assim, você pode ver que, no código-fonte desse modelo, \"tower\" refere-se às camadas que estão antes da camada de previsão, e \"head\" refere-se às camadas de previsão."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6S9QjSWheJb"
      },
      "source": [
        "<a name='exercise-9'></a>\n",
        "\n",
        "### **Exercício 9**: Selecione as variáveis da camada de previsão\n",
        "\n",
        "Com base na inspeção do `detection_model.trainable_variables`, selecione as variáveis da camada de previsão que você ajustará:\n",
        "- As variáveis da cabeça da caixa delimitadora (que preveem as coordenadas da caixa delimitadora)\n",
        "- As variáveis do cabeçalho da classe (que preveem a classe/categoria)\n",
        "\n",
        "Você tem algumas opções para fazer isso:\n",
        "- Você pode acessá-las pelo índice da lista:\n",
        "```\n",
        "detection_model.trainable_variables[92]\n",
        "```\n",
        "\n",
        "- Como alternativa, você pode usar a correspondência de strings para selecionar as variáveis:\n",
        "```\n",
        "tmp_list = []\n",
        "for v in detection_model.trainable_variables:\n",
        "  if v.name.startswith('ResNet50V1_FPN/bottom_up_block5'):\n",
        "    tmp_list.append(v)\n",
        "```\n",
        "\n",
        "**Dica**: Há um total de quatro variáveis que você deseja ajustar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDdvGRCYrkJt"
      },
      "outputs": [],
      "source": [
        "### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)\n",
        "\n",
        "# Defina uma lista que contenha as camadas que você deseja ajustar\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "to_fine_tune = []\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "for var in trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)\n",
        "\n",
        "### TERMINE O CÓDIGO AQUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwzzGkzyrkJv",
        "outputId": "a9715adc-cf14-46d3-e810-3e406d64abed"
      },
      "outputs": [],
      "source": [
        "# Código de Teste:\n",
        "\n",
        "print(to_fine_tune[0].name)\n",
        "print(to_fine_tune[2].name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-VK-GB2rkJy"
      },
      "source": [
        "**Saída Esperada**:\n",
        "\n",
        "```txt\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel:0\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel:0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffvhZZu2rkJy"
      },
      "source": [
        "## Treine seu modelo\n",
        "\n",
        "Você definirá uma função que lida com o treinamento de um lote, que será usado posteriormente em seu loop de treinamento.\n",
        "\n",
        "Primeiro, percorra essas células de código para saber como você realizará o treinamento usando esse modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qJcTJTvheJb"
      },
      "outputs": [],
      "source": [
        "# Obtenha um lote de suas imagens de treinamento\n",
        "g_images_list = train_image_tensors[0:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw44cJLcheJb"
      },
      "source": [
        "O `detection_model` é da classe [SSDMetaArch](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L655), e seu código-fonte mostra que ele tem esta função [preprocess](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L459).\n",
        "- Essa função pré-processa as imagens para que elas possam ser passadas para o modelo (para treinamento ou previsão):\n",
        "```\n",
        "  def preprocess(self, inputs):\n",
        "    \"\"\"Feature-extractor specific preprocessing.\n",
        "    ...\n",
        "    Args:\n",
        "      inputs: a [batch, height_in, width_in, channels] float tensor representing\n",
        "        a batch of images with values between 0 and 255.0.\n",
        "    Returns:\n",
        "      preprocessed_inputs: a [batch, height_out, width_out, channels] float\n",
        "        tensor representing a batch of images.\n",
        "\n",
        "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
        "        of the form [height, width, channels] indicating the shapes\n",
        "        of true images in the resized images, as resized images can be padded\n",
        "        with zeros.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XifEma9DheJb",
        "outputId": "911b5e32-d8a0-433b-b0be-74bbe22e98b9"
      },
      "outputs": [],
      "source": [
        "# Use .preprocess para pré-processar uma imagem\n",
        "g_preprocessed_image = detection_model.preprocess(g_images_list[0])\n",
        "print(f\"g_preprocessed_image type: {type(g_preprocessed_image)}\")\n",
        "print(f\"g_preprocessed_image length: {len(g_preprocessed_image)}\")\n",
        "print(f\"O índice 0 tem a imagem pré-processada da forma {g_preprocessed_image[0].shape}\")\n",
        "print(f\"o índice 1 tem informações sobre a forma real da imagem, excluindo o preenchimento: {g_preprocessed_image[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kQq7BrMheJb"
      },
      "source": [
        "Você pode pré-processar cada imagem e salvar seus resultados em duas listas separadas\n",
        "- Uma lista das imagens pré-processadas\n",
        "- Uma lista da forma real para cada imagem pré-processada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXKZR5rOheJb",
        "outputId": "bdaf1cb9-e2df-4483-af9e-996ffc746147"
      },
      "outputs": [],
      "source": [
        "preprocessed_image_list = []\n",
        "true_shape_list = []\n",
        "\n",
        "for img in g_images_list:\n",
        "    processed_img, true_shape = detection_model.preprocess(img)\n",
        "    preprocessed_image_list.append(processed_img)\n",
        "    true_shape_list.append(true_shape)\n",
        "\n",
        "print(f\"preprocessed_image_list é do tipo {type(preprocessed_image_list)}\")\n",
        "print(f\"preprocessed_image_list tem comprimento {len(preprocessed_image_list)}\")\n",
        "print()\n",
        "print(f\"true_shape_list é do tipo {type(true_shape_list)}\")\n",
        "print(f\"true_shape_list tem comprimento {len(true_shape_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyb8CyKheJb"
      },
      "source": [
        "## Fazer uma previsão\n",
        "O `detection_model` também tem uma função `.predict`.  De acordo com o código-fonte de [predict](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L525)\n",
        "\n",
        "```\n",
        "  def predict(self, preprocessed_inputs, true_image_shapes):\n",
        "    \"\"\"Prediz tensores não pós-processados a partir do tensor de entrada.\n",
        "    Essa função recebe um lote de imagens de entrada e o executa por meio da passagem\n",
        "    da rede para produzir previsões não pós-processadas.\n",
        "...\n",
        "    Args:\n",
        "      preprocessed_inputs: um tensor de imagens [lote, altura, largura, canais].\n",
        "\n",
        "      true_image_shapes: tensor int32 da forma [batch, 3] em que cada linha é\n",
        "        da forma [altura, largura, canais] indicando as formas\n",
        "        das imagens verdadeiras nas imagens redimensionadas, pois as imagens redimensionadas podem ser preenchidas\n",
        "        com zeros.\n",
        "\n",
        "    Retorna:\n",
        "      prediction_dict: um dicionário que contém tensores de previsão \"brutos\":\n",
        "        1) preprocessed_inputs: o tensor de imagem [batch, height, width, channels].\n",
        "          imagem.\n",
        "        2) box_encodings: Tensor flutuante 4-D de forma [batch_size, num_anchors,\n",
        "          box_code_dimension] contendo caixas previstas.\n",
        "        3) class_predictions_with_background: Tensor de forma flutuante 3-D\n",
        "          [batch_size, num_anchors, num_classes+1] contendo previsões de classe\n",
        "          (logits) para cada uma das âncoras.  Observe que esse tensor *inclui*\n",
        "          previsões de classe de fundo (no índice de classe 0).\n",
        "        4) feature_maps: uma lista de tensores em que o i-ésimo tensor tem a forma\n",
        "          [batch, height_i, width_i, depth_i].\n",
        "        5) anchors: Tensor flutuante 2-D de forma [num_anchors, 4] contendo\n",
        "          as âncoras geradas em coordenadas normalizadas.\n",
        "        6) final_anchors: Tensor flutuante 3-D de forma [batch_size, num_anchors, 4]\n",
        "          contendo as âncoras geradas em coordenadas normalizadas.\n",
        "        Se self._return_raw_detections_during_predict for True, o dicionário\n",
        "        também conterá:\n",
        "        7) raw_detection_boxes: um tensor 4-D float32 com forma\n",
        "          [batch_size, self.max_num_proposals, 4] em coordenadas normalizadas.\n",
        "        8) raw_detection_feature_map_indices: um tensor 3-D int32 com forma\n",
        "          [batch_size, self.max_num_proposals].\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "Observe que o `.predict` recebe suas entradas como tensores.  Se você tentar passar as imagens pré-processadas e as formas verdadeiras, receberá um erro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oAoT6q0heJb",
        "lines_to_next_cell": 2,
        "outputId": "92d2373e-83c6-41d5-a104-86f0806c692b"
      },
      "outputs": [],
      "source": [
        "# Tente chamar `predict` e passe as listas; veja a mensagem de erro\n",
        "try:\n",
        "    detection_model.predict(preprocessed_image_list, true_shape_list)\n",
        "except AttributeError as e:\n",
        "    print(\"Mensagem de erro:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Bs4lfPheJb"
      },
      "source": [
        "Mas não se preocupe! Você pode verificar como usar corretamente o `predict`:\n",
        "- Observe que a documentação do código-fonte diz que `preprocessed_inputs` e `true_image_shapes` devem ser tensores e não listas de tensores.\n",
        "- Uma maneira de transformar uma lista de tensores em um tensor é usar [tf.concat](https://www.tensorflow.org/api_docs/python/tf/concat)\n",
        "\n",
        "```\n",
        "tf.concat(\n",
        "    values, axis, name='concat'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He2IbVteheJb",
        "outputId": "760b46f2-0b55-4157-fb2b-1142dd4a33db"
      },
      "outputs": [],
      "source": [
        "# Transforme uma lista de tensores em um tensor\n",
        "preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
        "true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
        "\n",
        "print(f\"preprocessed_image_tensor shape: {preprocessed_image_tensor.shape}\")\n",
        "print(f\"true_shape_tensor shape: {true_shape_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCkoOWXdheJb"
      },
      "source": [
        "Agora você pode fazer previsões para as imagens.\n",
        "De acordo com o código-fonte, `predict` retorna um dicionário que contém as informações de previsão, incluindo:\n",
        "- As previsões da caixa delimitadora\n",
        "- As previsões de classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSiOe-FFheJb",
        "outputId": "3ad7dd8e-4f51-48b9-f92f-eba0e7f7b67b"
      },
      "outputs": [],
      "source": [
        "# Faça previsões sobre as imagens\n",
        "prediction_dict = detection_model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
        "\n",
        "print(\"chaves em prediction_dict\")\n",
        "for key in prediction_dict.keys():\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amiRcKs8heJb"
      },
      "source": [
        "#### Calcular a perda\n",
        "\n",
        "Agora que seu modelo fez a previsão, você deseja compará-la com a verdade terrestre para calcular uma perda.\n",
        "- O `detection_model` tem uma função [loss](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L807).\n",
        "\n",
        "```Python\n",
        "  def loss(self, prediction_dict, true_image_shapes, scope=None):\n",
        "    \"\"\"Compute scalar loss tensors with respect to provided groundtruth.\n",
        "    A chamada dessa função requer que os tensores da verdade básica tenham sido\n",
        "    fornecidos por meio da função provide_groundtruth.\n",
        "    Args:\n",
        "      prediction_dict: um dicionário que contém tensores de previsão com\n",
        "        1) box_encodings: tensor flutuante 3-D de forma [batch_size, num_anchors,\n",
        "          box_code_dimension] contendo caixas previstas.\n",
        "        2) class_predictions_with_background: Tensor de forma flutuante 3-D\n",
        "          [batch_size, num_anchors, num_classes+1] contendo previsões de classe\n",
        "          (logits) para cada uma das âncoras. Observe que esse tensor *inclui*\n",
        "          as previsões de classe em segundo plano.\n",
        "      true_image_shapes: tensor int32 de forma [batch, 3] em que cada linha é\n",
        "        da forma [altura, largura, canais] indicando as formas\n",
        "        das imagens verdadeiras nas imagens redimensionadas, pois as imagens redimensionadas podem ser preenchidas\n",
        "        com zeros.\n",
        "      escopo: Nome opcional do escopo.\n",
        "    Retorna:\n",
        "      Um dicionário que mapeia as chaves de perda (`localization_loss` e\n",
        "        `classification_loss`) para tensores escalares que representam os valores de perda correspondentes\n",
        "        correspondentes.\n",
        "    \"\"\"\n",
        "```\n",
        "Ele recebe:\n",
        "- O dicionário de previsão que vem de sua chamada para `.predict()`.\n",
        "- A forma das imagens verdadeiras que vem de sua chamada para `.preprocess()` seguida pela conversão de uma lista para um tensor.\n",
        "\n",
        "Tente chamar `.loss`.  Você verá uma mensagem de erro que deverá ser corrigida para que a função `.loss` seja executada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPeDlc6VheJb",
        "outputId": "53b01b7d-0408-4051-cc7e-bc090c70740f"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "except RuntimeError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMeqXdjSheJb"
      },
      "source": [
        "Isso está gerando um erro sobre groundtruth_classes_list:\n",
        "```\n",
        "O tensor gráfico tem o nome: groundtruth_classes_list:0\n",
        "```\n",
        "\n",
        "Observe que na documentação de `loss` (mostrada acima), está escrito:\n",
        "```\n",
        "A chamada dessa função requer que os tensores de verdade terrestre tenham sido\n",
        "    fornecidos por meio da função provide_groundtruth.\n",
        "```\n",
        "\n",
        "Portanto, primeiro você deve definir a verdade básica (rótulos e caixas delimitadoras verdadeiras) antes de calcular a perda.\n",
        "- Isso faz sentido, pois a perda está comparando a previsão com a verdade básica e, portanto, a função de perda precisa conhecer a verdade básica.\n",
        "#### Fornecer a verdade básica\n",
        "O código-fonte para fornecer a verdade fundamental está localizado na classe principal do `SSDMetaArch`, `model.DetectionModel`.\n",
        "- Aqui está o link para o código de [provide_ground_truth](https://github.com/tensorflow/models/blob/fd6b24c19c68af026bb0a9efc9f7b1719c231d3d/research/object_detection/core/model.py#L297)\n",
        "\n",
        "```Python\n",
        "def provide_groundtruth(\n",
        "      self,\n",
        "      groundtruth_boxes_list,\n",
        "      groundtruth_classes_list,\n",
        "... # mais parâmetros não mostrados aqui\n",
        "\"\"\"\n",
        "    Args:\n",
        "      groundtruth_boxes_list: uma lista de tensores 2-D tf.float32 de forma\n",
        "        [num_boxes, 4] contendo as coordenadas das caixas de verdade.\n",
        "          As caixas da verdade terrestre são fornecidas no formato [y_min, x_min, y_max, x_max]\n",
        "          e supõe-se que sejam normalizadas e recortadas\n",
        "          em relação à janela da imagem com y_min <= y_max e x_min <= x_max.\n",
        "      groundtruth_classes_list: uma lista de tensores 2-D tf.float32 one-hot (ou k-hot)\n",
        "        tensores de forma [num_boxes, num_classes] contendo os alvos de classe\n",
        "        com o índice 0 assumido para mapear a primeira classe sem fundo.\n",
        "\"\"\"\n",
        "```\n",
        "Você definirá dois parâmetros em `provide_ground_truth`:\n",
        "- As caixas delimitadoras verdadeiras\n",
        "- As classes verdadeiras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XWjDnRxheJb"
      },
      "outputs": [],
      "source": [
        "# Obter as caixas delimitadoras da ground-truth\n",
        "gt_boxes_list = gt_box_tensors[0:2]\n",
        "\n",
        "# Obter os rótulos de classe da ground-truth\n",
        "gt_classes_list = gt_classes_one_hot_tensors[0:2]\n",
        "\n",
        "# Fornecer a ground-truth ao modelo\n",
        "detection_model.provide_groundtruth(\n",
        "            groundtruth_boxes_list=gt_boxes_list,\n",
        "            groundtruth_classes_list=gt_classes_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gjgxv9ZheJb"
      },
      "source": [
        "Agora você pode calcular a perda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-bztzeNheJb",
        "outputId": "d65e1773-ea32-45bf-eb36-80c5a35f0027"
      },
      "outputs": [],
      "source": [
        "# Calcule a perda depois de ter fornecido a ground-truth\n",
        "losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "\n",
        "# Exibir o dicionário de perdas\n",
        "losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "print(f\"chaves do dicionário de perdas: {losses_dict.keys()}\")\n",
        "print(f\"perda de localização {losses_dict['Loss/localization_loss']:.8f}\")\n",
        "print(f\"perda de classificação {losses_dict['Loss/classification_loss']:.8f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHC1vJsLheJb"
      },
      "source": [
        "Agora você pode calcular o gradiente e otimizar as variáveis que selecionou para fazer o ajuste fino.\n",
        "- Use tf.GradientTape\n",
        "\n",
        "```Python\n",
        "with tf.GradientTape() as tape:\n",
        "    # Faça a previsão\n",
        "\n",
        "    # calcular a perda\n",
        "\n",
        "    # Calcule o gradiente de cada variável do modelo com relação a cada perda\n",
        "    gradients = tape.gradient([some loss], variables to fine tune)\n",
        "\n",
        "    # Aplicar os gradientes para atualizar essas variáveis do modelo\n",
        "    optimizer.apply_gradients(zip(gradients, variables to fine tune))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdY8qHSLheJb"
      },
      "outputs": [],
      "source": [
        "# Vamos apenas redefinir o modelo para que você possa praticar a configuração por conta própria!\n",
        "detection_model.provide_groundtruth(groundtruth_boxes_list=[], groundtruth_classes_list=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIUKsYPLheJb"
      },
      "source": [
        "<a name='exercise-10'></a>\n",
        "\n",
        "### **Exercício 10**: Definir a etapa de treinamento\n",
        "Preencha a função abaixo para configurar uma etapa de treinamento.\n",
        "- Pré-processar as imagens\n",
        "- Fazer uma previsão\n",
        "- Calcule a perda (e certifique-se de que a função de perda tenha a verdade básica para comparar com a previsão)\n",
        "- Calcule a perda total:\n",
        "  - `total_loss` = `localization_loss + classification_loss`\n",
        "  - Observação: isso é diferente do código de exemplo que você viu acima\n",
        "- Calcule os gradientes com relação às variáveis que você selecionou para treinar.\n",
        "- Otimize as variáveis do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nttIf_ZgheJb",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# decore com @tf.function para um treinamento mais rápido (lembre-se, modo graph!)\n",
        "# Configure a passagem para frente + para trás para uma única etapa do treino.\n",
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "  \"\"\"Get a tf.function for training step.\"\"\"\n",
        "\n",
        "  # Use o tf.function para ter um pouco mais de velocidade.\n",
        "  # Comente o decorador tf.function se você quiser que o interior da função\n",
        "  # da função seja executado no modo eager.\n",
        "@tf.function\n",
        "def train_step_fn(image_list,\n",
        "                groundtruth_boxes_list,\n",
        "                groundtruth_classes_list,\n",
        "                model,\n",
        "                optimizer,\n",
        "                vars_to_fine_tune):\n",
        "  \"\"\"Uma única iteração de treinamento.\n",
        "\n",
        "    Args:\n",
        "      image_list: Uma lista de [1, altura, largura, 3] Tensor do tipo tf.float32.\n",
        "        Observe que a altura e a largura podem variar entre as imagens, pois elas são\n",
        "        remodeladas dentro dessa função para serem 640x640.\n",
        "      groundtruth_boxes_list: Uma lista de tensores de forma [N_i, 4] com o tipo\n",
        "        tf.float32 representando as caixas de groundtruth para cada imagem no lote.\n",
        "      groundtruth_classes_list: Uma lista de tensores de forma [N_i, num_classes]\n",
        "        com o tipo tf.float32 representando caixas de verdade para cada imagem no\n",
        "        do lote.\n",
        "\n",
        "    Retorna:\n",
        "      Um tensor escalar que representa a perda total para o lote de entrada.\n",
        "  \"\"\"\n",
        "  shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
        "  model.provide_groundtruth(\n",
        "      groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "      groundtruth_classes_list=groundtruth_classes_list)\n",
        "  with tf.GradientTape() as tape:\n",
        "    ### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)\n",
        "    # Pré-processar as imagens\n",
        "    preprocessed_image_tensor =  tf.concat(\n",
        "          [detection_model.preprocess(image_tensor)[0]\n",
        "           for image_tensor in image_tensors], axis=0)\n",
        "    #true_shape_tensor = None\n",
        "    # Faça uma previsão\n",
        "    prediction_dict = model.predict(preprocessed_image_tensor, shapes)\n",
        "    # Calcule a perda total (soma das duas perdas)\n",
        "    losses_dict = model.loss(prediction_dict, shapes)\n",
        "    total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "    # Calcular os gradientes\n",
        "    gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "    # Otimizar as variáveis selecionadas do modelo\n",
        "    optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "    ### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n7imaJRrkJ1"
      },
      "source": [
        "## Executar o loop de treinamento\n",
        "\n",
        "Execute o loop de treinamento usando a função de etapa de treinamento que você acabou de definir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UUBdFJKIgMY",
        "outputId": "209c001f-8d82-4658-fec8-75571b94396d"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# Esses parâmetros podem ser ajustados; como nosso conjunto de treinamento tem 5 imagens,\n",
        "# não faz sentido ter um tamanho de lote muito maior, embora possamos colocar mais exemplos\n",
        "# na memória se quisermos.\n",
        "batch_size = 4\n",
        "learning_rate = 0.01\n",
        "num_batches = 100\n",
        "\n",
        "# Selecione as variáveis nas camadas superiores para fazer o ajuste fino.\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "to_fine_tune = []\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "for var in trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)\n",
        "\n",
        "# Configure a passagem para frente + para trás para uma única etapa do treino.\n",
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "  \"\"\"Get a tf.function for training step.\"\"\"\n",
        "\n",
        "  # Use o tf.function para ter um pouco mais de velocidade.\n",
        "  # Comente o decorador tf.function se você quiser que o interior da função\n",
        "  # da função seja executado no modo eager.\n",
        "  @tf.function\n",
        "  def train_step_fn(image_tensors,\n",
        "                    groundtruth_boxes_list,\n",
        "                    groundtruth_classes_list):\n",
        "    \"\"\"Uma única iteração de treinamento.\n",
        "\n",
        "    Args:\n",
        "      image_tensors: Uma lista de [1, height, width, 3] Tensor do tipo tf.float32.\n",
        "        Observe que a altura e a largura podem variar entre as imagens, pois elas são\n",
        "        remodeladas dentro dessa função para serem 640x640.\n",
        "      groundtruth_boxes_list: Uma lista de tensores de forma [N_i, 4] com o tipo\n",
        "        tf.float32 representando as caixas de groundtruth para cada imagem no lote.\n",
        "      groundtruth_classes_list: Uma lista de tensores de forma [N_i, num_classes]\n",
        "        com o tipo tf.float32 representando caixas de verdade para cada imagem no\n",
        "        do lote.\n",
        "\n",
        "    Retorna:\n",
        "      Um tensor escalar que representa a perda total para o lote de entrada.\n",
        "    \"\"\"\n",
        "    shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
        "    model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list)\n",
        "    with tf.GradientTape() as tape:\n",
        "      preprocessed_images = tf.concat(\n",
        "          [detection_model.preprocess(image_tensor)[0]\n",
        "           for image_tensor in image_tensors], axis=0)\n",
        "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
        "      losses_dict = model.loss(prediction_dict, shapes)\n",
        "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "    return total_loss\n",
        "\n",
        "  return train_step_fn\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "train_step_fn = get_model_train_step_function(\n",
        "    detection_model, optimizer, to_fine_tune)\n",
        "\n",
        "print('Iniciar o fine-tuning!', flush=True)\n",
        "for idx in range(num_batches):\n",
        "  # Obter chaves para um subconjunto aleatório de exemplos\n",
        "  all_keys = list(range(len(train_images_np)))\n",
        "  random.shuffle(all_keys)\n",
        "  example_keys = all_keys[:batch_size]\n",
        "\n",
        "  # Observe que não aumentamos os dados nesta demonstração.  Se quiser fazer um exercício divertido,\n",
        "  # recomendamos experimentar a inversão horizontal aleatória e o corte aleatório :)\n",
        "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "  # Training step (forward pass + backwards pass)\n",
        "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "\n",
        "  if idx % 10 == 0:\n",
        "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "\n",
        "print('Done fine-tuning!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx8KefSZrkJ5"
      },
      "source": [
        "**Saída esperada:**\n",
        "\n",
        "A perda total deve estar diminuindo e deve ser menor que 1 após o ajuste fino. Por exemplo:\n",
        "\n",
        "```txt\n",
        "Iniciar o ajuste fino!\n",
        "lote 0 de 100, perda=1,2559178\n",
        "lote 10 de 100, perda=16,067217\n",
        "lote 20 de 100, perda=8,094654\n",
        "lote 30 de 100, perda=0,34514275\n",
        "lote 40 de 100, perda = 0,033170983\n",
        "lote 50 de 100, perda=0,0024622646\n",
        "lote 60 de 100, perda = 0,00074224477\n",
        "lote 70 de 100, perda = 0,0006149876\n",
        "lote 80 de 100, perda = 0,00046916265\n",
        "lote 90 de 100, perda=0,0004159231\n",
        "Ajuste fino concluído!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHlXL1x_Z3tc"
      },
      "source": [
        "## Carregue as imagens de teste e execute a inferência com o novo modelo!\n",
        "\n",
        "Agora você pode testar seu modelo em um novo conjunto de imagens. A célula abaixo faz o download de 237 imagens de um zumbi \"caminhando\" e as armazena em um diretório `results/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL697_MnHcJF",
        "outputId": "1fcba5a8-f2b6-42a8-b9bb-cbf1e49ea837"
      },
      "outputs": [],
      "source": [
        "# descomente se quiser excluir os arquivos existentes\n",
        "!rm zombie-walk-frames.zip\n",
        "!rm -rf ./zombie-walk\n",
        "!rm -rf ./results\n",
        "\n",
        "# Faça o download das imagens de teste\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/tensorflow-3-public/datasets/zombie-walk-frames.zip \\\n",
        "    -O zombie-walk-frames.zip\n",
        "\n",
        "# Descompactar as imagens de teste\n",
        "local_zip = './zombie-walk-frames.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./results')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_LJJ8wWhzlZ"
      },
      "source": [
        "Você carregará essas imagens em matrizes numpy para prepará-las para a inferência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcE6OwrHQJya",
        "outputId": "da95ea8d-8d09-4dad-b07a-85ea20117797"
      },
      "outputs": [],
      "source": [
        "test_image_dir = './results/'\n",
        "test_images_np = []\n",
        "\n",
        "# Carregue as imagens em uma matriz numérica. Isso levará alguns minutos para ser concluído.\n",
        "for i in range(0, 237):\n",
        "    image_path = os.path.join(test_image_dir, 'zombie-walk' + \"{0:04}\".format(i) + '.jpg')\n",
        "    print(image_path)\n",
        "    test_images_np.append(np.expand_dims(\n",
        "      load_image_into_numpy_array(image_path), axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-GIbqkzrkJ_"
      },
      "source": [
        "<a name='exercise-11'></a>\n",
        "\n",
        "### **Exercício 11**: Pré-processar, prever e pós-processar uma imagem\n",
        "\n",
        "Defina uma função que retorna as caixas de detecção, as classes e as pontuações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aXt-bodrkKA"
      },
      "outputs": [],
      "source": [
        "# Novamente, descomente esse decorador se quiser executar a inferência no modo eager\n",
        "@tf.function\n",
        "def detect(input_tensor):\n",
        "    \"\"\"Executa a detecção em uma imagem de entrada.\n",
        "\n",
        "    Args:\n",
        "    input_tensor: Um [1, altura, largura, 3] Tensor do tipo tf.float32.\n",
        "      Observe que a altura e a largura podem ser qualquer coisa, pois a imagem será\n",
        "      imediatamente redimensionada de acordo com as necessidades do modelo dentro dessa\n",
        "      função.\n",
        "\n",
        "    Retorna:\n",
        "    Um dict contendo 3 Tensores (`detection_boxes`, `detection_classes`,\n",
        "      e `detection_scores`).\n",
        "    \"\"\"\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "\n",
        "    ### INICIE O CÓDIGO AQUI (Substitua as instâncias de `None` pelo seu código)\n",
        "    # Use o método postprocess() do modelo de detecção para obter as detecções finais\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    ### TERMINE O CÓDIGO AQUI ###\n",
        "\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzvMrohqiyZD"
      },
      "source": [
        "Agora você pode percorrer as imagens de teste e obter as pontuações de detecção e as caixas delimitadoras para sobrepor na imagem original. Salvaremos cada resultado em um dicionário `results` e o autograder o usará para avaliar seus resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhXwX7O8rkKC"
      },
      "outputs": [],
      "source": [
        "# Observe que o primeiro frame acionará o rastreamento da função tf.function, o que levará algum tempo.\n",
        "# levará algum tempo, após o qual a inferência deverá ser rápida.\n",
        "\n",
        "label_id_offset = 1\n",
        "results = {'boxes': [], 'scores': []}\n",
        "\n",
        "for i in range(len(test_images_np)):\n",
        "  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "  detections = detect(input_tensor)\n",
        "  plot_detections(\n",
        "    test_images_np[i][0],\n",
        "    detections['detection_boxes'][0].numpy(),\n",
        "    detections['detection_classes'][0].numpy().astype(np.uint32)\n",
        "    + label_id_offset,\n",
        "    detections['detection_scores'][0].numpy(),\n",
        "    category_index, figsize=(15, 20), image_name=\"./results/gif_frame_\" + ('%03d' % i) + \".jpg\")\n",
        "  results['boxes'].append(detections['detection_boxes'][0][0].numpy())\n",
        "  results['scores'].append(detections['detection_scores'][0][0].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNV5uwN7rkKE",
        "outputId": "ba5b67a5-42b9-4d72-a5c2-81a06eac29a2"
      },
      "outputs": [],
      "source": [
        "# Código de Teste\n",
        "\n",
        "print(len(results['boxes']))\n",
        "print(results['boxes'][0].shape)\n",
        "print()\n",
        "\n",
        "# Comparar com as caixas delimitadoras esperadas\n",
        "print(np.allclose(results['boxes'][0], [0.28838485, 0.06830047, 0.7213766 , 0.19833465], rtol=0.18))\n",
        "print(np.allclose(results['boxes'][5], [0.29168868, 0.07529271, 0.72504973, 0.20099735], rtol=0.18))\n",
        "print(np.allclose(results['boxes'][10], [0.29548776, 0.07994056, 0.7238164 , 0.20778716], rtol=0.18))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRbmLv_UrkKG"
      },
      "source": [
        "**Saída esperada:** O ideal é que os três valores booleanos na parte inferior sejam `True`. Mas se você obtiver apenas dois, ainda poderá tentar enviar. Isso compara suas caixas delimitadoras resultantes para cada imagem de zumbi com algumas coordenadas pré-carregadas (ou seja, os valores codificados na célula de teste acima). Dependendo de como você anotou as imagens de treinamento, é possível que alguns dos seus resultados sejam diferentes para esses três quadros, mas ainda assim obtenha bons resultados gerais quando todas as imagens forem examinadas pelo avaliador. Se duas ou todas forem falsas, tente anotar as imagens novamente com uma caixa delimitadora mais estreita ou use a lista [predefinida `gt_boxes`] (#gt-boxes).\n",
        "\n",
        "```txt\n",
        "237\n",
        "(4,)\n",
        "\n",
        "Verdadeiro\n",
        "Verdadeiro\n",
        "Verdadeiro\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Rh0oy7l7Aj"
      },
      "source": [
        "Você também pode verificar se o modelo detecta uma classe de zumbis nas imagens examinando a chave `scores` do dicionário `results`. Você deve obter um valor superior a 88,0 aqui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDEYAkg3l4lh",
        "outputId": "993feba3-5bfa-4b36-93fa-74fa54ec8da1"
      },
      "outputs": [],
      "source": [
        "x = np.array(results['scores'])\n",
        "\n",
        "# Porcentagem de quadros em que um zumbi é detectado\n",
        "zombie_detected = (np.where(x > 0.9, 1, 0).sum())/237*100\n",
        "print(zombie_detected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ3JQiRWj3FH"
      },
      "source": [
        "Você também pode exibir alguns quadros estáticos e inspecionar visualmente. Se você não vir uma caixa delimitadora ao redor do zumbi, considere a possibilidade de reanotar o ground_truth ou usar as `gt_boxes` predefinidas [aqui](#gt-boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ww0hLDI4fB1f",
        "outputId": "17bc8d71-301a-474a-fa6c-03f965ac0c98"
      },
      "outputs": [],
      "source": [
        "print('Frame 0')\n",
        "display(IPyImage('./results/gif_frame_000.jpg'))\n",
        "print()\n",
        "print('Frame 5')\n",
        "display(IPyImage('./results/gif_frame_005.jpg'))\n",
        "print()\n",
        "print('Frame 10')\n",
        "display(IPyImage('./results/gif_frame_010.jpg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrjHwbkVXqLG"
      },
      "source": [
        "## Crie um zip com as imagens do zombie-walk.\n",
        "Você pode fazer o download se quiser criar suas próprias animações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeKSNOG7BVd4"
      },
      "outputs": [],
      "source": [
        "zipf = zipfile.ZipFile('./zombie.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "\n",
        "filenames = glob.glob('./results/gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "\n",
        "for filename in filenames:\n",
        "    zipf.write(filename)\n",
        "\n",
        "zipf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTDH45KYX1Iv"
      },
      "source": [
        "## Criar animação de zumbis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW1FrT2iNnpy",
        "outputId": "8ab70deb-0b52-4459-a5af-e1989325ea64"
      },
      "outputs": [],
      "source": [
        "imageio.plugins.freeimage.download()\n",
        "\n",
        "!rm -rf ./results/zombie-anim.gif\n",
        "\n",
        "anim_file = './zombie-anim.gif'\n",
        "\n",
        "filenames = glob.glob('./results/gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "last = -1\n",
        "images = []\n",
        "\n",
        "for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    images.append(image)\n",
        "\n",
        "imageio.mimsave(anim_file, images, 'GIF-FI', fps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JqLoPaigdl4"
      },
      "source": [
        "Infelizmente, usar o `IPyImage` no notebook  para o grande `gif` gerado desconectará o tempo de execução. Para visualizar a animação, você pode usar o painel `Files` à esquerda e clicar duas vezes em `zombie-anim.gif`. Isso abrirá uma página de visualização à direita. Levará de 2 a 3 minutos para carregar e ver o zumbi andando."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
