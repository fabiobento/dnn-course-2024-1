{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/adv_cv/class_2/21%20-%20%20Laborat%C3%B3rio/C3_W2_Lab_3_Interactive_Eager_Few_Shot_OD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Esta é uma cópia deste [tutorial oficial](https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb) com revisões mínimas na seção `Imports` para fazê-lo funcionar com versões mais recentes do Tensorflow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Colab _Eager Few Shot Object Detection_ (Detecção de objetos com poucos disparos)\n",
    "\n",
    "Neste colab, demonstramos o ajuste fino de uma arquitetura RetinaNet (compatível com TF2) em pouquíssimos exemplos de uma nova classe após a inicialização de um ponto de verificação COCO pré-treinado.\n",
    "\n",
    "O treinamento é executado no modo eager.\n",
    "\n",
    "Tempo estimado para executar este laboratório (com GPU): < 5 minutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi28cqGGFWnY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Clonar o repositório de modelos do Tensorflow se ele ainda não existir\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para compatibilidade. Fixe a versão oficial do tf-models para que ele use o Tensorflow 2.15.\n",
    "!sed -i 's/tf-models-official>=2.5.1/tf-models-official==2.15.0/g' ./models/research/object_detection/packages/tf2/setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar a API de detecção de objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwdsBdGhFanc"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Nota: No Google Colab, você precisa reiniciar a runtime para finalizar a instalação dos pacotes. Você pode fazer isso selecionando Runtime > Restart Runtime na barra de menus. **Não prossiga para a próxima seção sem reiniciar.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZcqD4NLdnf4"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  # Testar se estamos no Google Colab\n",
    "  IN_COLAB = True\n",
    "  from object_detection.utils import colab_utils\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IogyryF2lFBL"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y9R0Xllefec"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Carrega uma imagem de um arquivo em uma matriz numpy.\n",
    "\n",
    "  Coloca a imagem em uma matriz numpy para alimentar o gráfico do tensorflow.\n",
    "  Observe que, por convenção, nós a colocamos em uma matriz numpy com a forma\n",
    "  (altura, largura, canais), onde canais=3 para RGB.\n",
    "\n",
    "  Args:\n",
    "    path: um caminho de arquivo.\n",
    "\n",
    "  Retorna:\n",
    "    matriz numpy uint8 com formato (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "  image = Image.open(BytesIO(img_data))\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    category_index,\n",
    "                    figsize=(12, 16),\n",
    "                    image_name=None):\n",
    "  \"\"\"Função de wrapper para visualizar as detecções.\n",
    "\n",
    "  Args:\n",
    "    image_np: matriz numérica uint8 com formato (img_height, img_width, 3)\n",
    "    boxes: uma matriz numérica de forma [N, 4]\n",
    "    classes: uma matriz numérica de formato [N]. Observe que os índices de classe são baseados em 1,\n",
    "      e correspondem às chaves no mapa de rótulos.\n",
    "    scores: uma matriz numpy de forma [N] ou None.  Se scores=None, então\n",
    "      essa função pressupõe que as caixas a serem plotadas são caixas de verdade\n",
    "      e plotará todas as caixas como pretas, sem classes ou pontuações.\n",
    "    category_index: um dict contendo dicionários de categorias (cada um contendo\n",
    "      índice de categoria `id` e nome de categoria `name`), codificados por índices de categoria.\n",
    "    figsize: tamanho da figura.\n",
    "    image_name: um nome para o arquivo de imagem.\n",
    "  \"\"\"\n",
    "  image_np_with_annotations = image_np.copy()\n",
    "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_annotations,\n",
    "      boxes,\n",
    "      classes,\n",
    "      scores,\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      min_score_thresh=0.8)\n",
    "  if image_name:\n",
    "    plt.imsave(image_name, image_np_with_annotations)\n",
    "  else:\n",
    "    plt.imshow(image_np_with_annotations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSaXL28TZfk1"
   },
   "source": [
    "# Dados do \"_Rubber Ducky_\"\n",
    "\n",
    "Começaremos com alguns dados que consistem em 5 imagens de um pato de borracha.\n",
    "\n",
    "Observe que o conjunto de dados [coco](https://cocodataset.org/#explore) contém vários animais, mas, notavelmente, ele *não* contém patos de borracha (ou mesmo patos de verdade), portanto, essa é uma classe nova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQy3ND7EpFQM"
   },
   "outputs": [],
   "source": [
    "# Carregar imagens e visualizar\n",
    "train_image_dir = 'models/research/object_detection/test_images/ducky/train/'\n",
    "train_images_np = []\n",
    "for i in range(1, 6):\n",
    "  image_path = os.path.join(train_image_dir, 'robertducky' + str(i) + '.jpg')\n",
    "  train_images_np.append(load_image_into_numpy_array(image_path))\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['xtick.labelsize'] = False\n",
    "plt.rcParams['ytick.labelsize'] = False\n",
    "plt.rcParams['xtick.top'] = False\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.right'] = False\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "\n",
    "for idx, train_image_np in enumerate(train_images_np):\n",
    "  plt.subplot(2, 3, idx+1)\n",
    "  plt.imshow(train_image_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbKXmQoxcUgE"
   },
   "source": [
    "# Anotar imagens com caixas delimitadoras\n",
    "\n",
    "Nesta célula, você anotará os patinhos de borracha - desenhe uma caixa ao redor do patinho de borracha em cada imagem; clique em \"próxima imagem\" para ir para a próxima imagem e em \"enviar\" quando não houver mais imagens.\n",
    "\n",
    "Se você quiser pular a etapa de anotação manual, nós entendemos perfeitamente.  Nesse caso, basta ignorar essa célula e executar a próxima célula, na qual preenchemos previamente o groundtruth com caixas delimitadoras pré-anotadas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nEDRoUEcUgL"
   },
   "outputs": [],
   "source": [
    "gt_boxes = []\n",
    "colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTP9AFqecUgS"
   },
   "source": [
    "# Caso você não queira rotular...\n",
    "\n",
    "Execute essa célula somente se você não tiver anotado nada acima e\n",
    "preferir usar apenas nossas caixas pré-anotadas.  Não se esqueça de descomentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIAT6ZUmdHOC"
   },
   "outputs": [],
   "source": [
    "gt_boxes = [\n",
    "            np.array([[0.436, 0.591, 0.629, 0.712]], dtype=np.float32),\n",
    "            np.array([[0.539, 0.583, 0.73, 0.71]], dtype=np.float32),\n",
    "            np.array([[0.464, 0.414, 0.626, 0.548]], dtype=np.float32),\n",
    "            np.array([[0.313, 0.308, 0.648, 0.526]], dtype=np.float32),\n",
    "            np.array([[0.256, 0.444, 0.484, 0.629]], dtype=np.float32)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dqb_yjAo3cO_"
   },
   "source": [
    "# Preparar dados para treinamento\n",
    "\n",
    "Abaixo, adicionamos as anotações de classe (para simplificar, considerei uma única classe neste colab, embora deva ser fácil estender isso para lidar com várias classes).\n",
    "\n",
    "Também foi tudo convertido para o formato que o loop de treinamento abaixo espera (por exemplo, tudo convertido em tensores, classes convertidas em representações de um único ponto etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWBqFVMcweF-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Por convenção, nossas classes sem histórico começam a contar em 1.\n",
    "# Já que estaremos prevendo apenas uma classe, atribuiremos a ela uma\n",
    "# classe de 1.\n",
    "duck_class_id = 1\n",
    "num_classes = 1\n",
    "\n",
    "category_index = {duck_class_id: {'id': duck_class_id, 'name': 'rubber_ducky'}}\n",
    "\n",
    "# Converta rótulos de classe em um único disparo; converta tudo em tensores.\n",
    "# O `label_id_offset` aqui desloca todas as classes em um determinado número de índices;\n",
    "# Fazemos isso aqui para que o modelo receba rótulos de um único instante em que as classes que não são de fundo\n",
    "# classes começam a contar no índice zero.  Normalmente, isso é tratado\n",
    "# automaticamente em nossos binários de treinamento, mas precisamos reproduzi-lo aqui.\n",
    "label_id_offset = 1\n",
    "train_image_tensors = []\n",
    "gt_classes_one_hot_tensors = []\n",
    "gt_box_tensors = []\n",
    "for (train_image_np, gt_box_np) in zip(\n",
    "    train_images_np, gt_boxes):\n",
    "  train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
    "      train_image_np, dtype=tf.float32), axis=0))\n",
    "  gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
    "  zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
    "      np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
    "  gt_classes_one_hot_tensors.append(tf.one_hot(\n",
    "      zero_indexed_groundtruth_classes, num_classes))\n",
    "print('Dados de preparação concluídos.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3_Z3mJWN9KJ"
   },
   "source": [
    "# Vamos apenas visualizar os patos de borracha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBD6l-E4N71y"
   },
   "outputs": [],
   "source": [
    "dummy_scores = np.array([1.0], dtype=np.float32)   # Dar às caixas uma pontuação de 100%\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "for idx in range(5):\n",
    "  plt.subplot(2, 3, idx+1)\n",
    "  plot_detections(\n",
    "      train_images_np[idx],\n",
    "      gt_boxes[idx],\n",
    "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
    "      dummy_scores, category_index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghDAsqfoZvPh"
   },
   "source": [
    "# Criar modelo e restaurar pesos para todas as camadas, exceto a última\n",
    "\n",
    "Nesta célula, criamos uma arquitetura de detecção de estágio único (RetinaNet) e restauramos tudo, exceto a camada de classificação no topo (que será automaticamente inicializada de forma aleatória).\n",
    "\n",
    "Para simplificar, codificamos várias coisas nesta célula para a arquitetura RetinaNet específica que temos em mãos (incluindo a suposição de que o tamanho da imagem será sempre 640x640), mas não é difícil generalizar para outras configurações de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J16r3NChD-7"
   },
   "outputs": [],
   "source": [
    "# Faça o download do ponto de verificação e coloque-o em models/research/object_detection/test_data/\n",
    "\n",
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyT4BUbaMeG-"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print('Criação de modelo e restauração de pesos para ajuste fino...', flush=True)\n",
    "num_classes = 1\n",
    "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "\n",
    "# Carregue a configuração do pipeline e crie um modelo de detecção.\n",
    "#\n",
    "# Como estamos trabalhando com uma arquitetura COCO que prevê 90\n",
    "# classes por padrão, substituímos o campo `num_classes` aqui para que seja apenas\n",
    "# apenas um (para a nossa nova classe de pato de borracha).\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "model_config.ssd.num_classes = num_classes\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=True)\n",
    "\n",
    "# Configure a restauração de pontos de verificação baseada em objetos --- O RetinaNet tem duas previsões\n",
    "# uma para classificação e a outra para regressão de caixa.  Vamos\n",
    "# restaurar a cabeça de regressão de caixa, mas inicializar a cabeça de classificação\n",
    "# do zero (mostramos a omissão abaixo comentando a linha que\n",
    "# adicionaríamos se quiséssemos restaurar os dois cabeçotes)\n",
    "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
    "    #    (i.e., the classification head that we *will not* restore)\n",
    "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
    "    )\n",
    "fake_model = tf.compat.v2.train.Checkpoint(\n",
    "          _feature_extractor=detection_model._feature_extractor,\n",
    "          _box_predictor=fake_box_predictor)\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
    "ckpt.restore(checkpoint_path).expect_partial()\n",
    "\n",
    "# Execute o modelo por meio de uma imagem fictícia para que as variáveis sejam criadas\n",
    "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "prediction_dict = detection_model.predict(image, shapes)\n",
    "_ = detection_model.postprocess(prediction_dict, shapes)\n",
    "print('Weights restored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCkWmdoZZ0zJ"
   },
   "source": [
    "# Loop de treinamento personalizado do modo Eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyHoF4mUrv5-"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "# Esses parâmetros podem ser ajustados; como nosso conjunto de treinamento tem 5 imagens\n",
    "# não faz sentido ter um tamanho de lote muito maior, embora pudéssemos\n",
    "# caber mais exemplos na memória, se quisermos.\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "num_batches = 100\n",
    "\n",
    "# Selecione as variáveis nas camadas superiores para fazer o ajuste fino.\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "to_fine_tune = []\n",
    "prefixes_to_train = [\n",
    "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
    "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
    "for var in trainable_variables:\n",
    "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
    "    to_fine_tune.append(var)\n",
    "\n",
    "# Configure a propagação forward + backward para uma única etapa de treino.\n",
    "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
    "  \"\"\"Get a tf.function for training step.\"\"\"\n",
    "\n",
    "  # Use o tf.function para ter um pouco mais de velocidade.\n",
    "  # Comente o decorador tf.function se você quiser que o interior da função\n",
    "  # da função seja executado com ansiedade.\n",
    "\n",
    "  @tf.function\n",
    "  def train_step_fn(image_tensors,\n",
    "                    groundtruth_boxes_list,\n",
    "                    groundtruth_classes_list):\n",
    "    \"\"\"Uma única iteração de treinamento.\n",
    "\n",
    "    Args:\n",
    "      image_tensors: Uma lista de [1, height, width, 3] Tensor do tipo tf.float32.\n",
    "        Observe que a altura e a largura podem variar entre as imagens, pois elas são\n",
    "        remodeladas dentro dessa função para serem 640x640.\n",
    "      groundtruth_boxes_list: Uma lista de tensores de forma [N_i, 4] com o tipo\n",
    "        tf.float32 representando caixas de verdade para cada imagem no lote.\n",
    "      groundtruth_classes_list: Uma lista de tensores de forma [N_i, num_classes]\n",
    "        com o tipo tf.float32 representando caixas de verdade para cada imagem no\n",
    "        do lote.\n",
    "\n",
    "    Retorna:\n",
    "      Um tensor escalar que representa a perda total para o lote de entrada.\n",
    "    \"\"\"\n",
    "    shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
    "    model.provide_groundtruth(\n",
    "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
    "        groundtruth_classes_list=groundtruth_classes_list)\n",
    "    with tf.GradientTape() as tape:\n",
    "      preprocessed_images = tf.concat(\n",
    "          [detection_model.preprocess(image_tensor)[0]\n",
    "           for image_tensor in image_tensors], axis=0)\n",
    "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "      losses_dict = model.loss(prediction_dict, shapes)\n",
    "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
    "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "    return total_loss\n",
    "\n",
    "  return train_step_fn\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "train_step_fn = get_model_train_step_function(\n",
    "    detection_model, optimizer, to_fine_tune)\n",
    "\n",
    "print('Começar o fine-tuning!', flush=True)\n",
    "for idx in range(num_batches):\n",
    "  # Obter chaves para um subconjunto aleatório de exemplos\n",
    "  all_keys = list(range(len(train_images_np)))\n",
    "  random.shuffle(all_keys)\n",
    "  example_keys = all_keys[:batch_size]\n",
    "\n",
    "  # Observe que não aumentamos os dados nesta demonstração.  Se você quiser um\n",
    "  # um exercício divertido, recomendamos fazer experiências com inversão horizontal aleatória\n",
    "  # e cortes aleatórios :)\n",
    "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "\n",
    "  # Etapa de treinamento (passe para frente + passe para trás)\n",
    "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
    "\n",
    "  if idx % 10 == 0:\n",
    "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
    "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
    "\n",
    "print('Fine-tuning concluído!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHlXL1x_Z3tc"
   },
   "source": [
    "# Carregue as imagens de teste e execute a inferência com o novo modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcE6OwrHQJya"
   },
   "outputs": [],
   "source": [
    "test_image_dir = 'models/research/object_detection/test_images/ducky/test/'\n",
    "test_images_np = []\n",
    "for i in range(1, 50):\n",
    "  image_path = os.path.join(test_image_dir, 'out' + str(i) + '.jpg')\n",
    "  test_images_np.append(np.expand_dims(\n",
    "      load_image_into_numpy_array(image_path), axis=0))\n",
    "\n",
    "# Descomente esse decorador se quiser executar a inferência eager\n",
    "@tf.function\n",
    "def detect(input_tensor):\n",
    "  \"\"\"Executa a detecção em uma imagem de entrada.\n",
    "\n",
    "  Args:\n",
    "    input_tensor: Um [1, altura, largura, 3] Tensor do tipo tf.float32.\n",
    "      Observe que a altura e a largura podem ser qualquer coisa, pois a imagem será\n",
    "      imediatamente redimensionada de acordo com as necessidades do modelo dentro dessa\n",
    "      função.\n",
    "\n",
    "  Retorna:\n",
    "    Um dict contendo 3 Tensores (`detection_boxes`, `detection_classes`,\n",
    "      e `detection_scores`).\n",
    "  \"\"\"\n",
    "  preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
    "  prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
    "  return detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "# Observe que o primeiro quadro acionará o rastreamento da função tf.function, o que levará algum tempo.\n",
    "# levará algum tempo, após o qual a inferência deverá ser rápida.\n",
    "\n",
    "label_id_offset = 1\n",
    "for i in range(len(test_images_np)):\n",
    "  input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
    "  detections = detect(input_tensor)\n",
    "\n",
    "  plot_detections(\n",
    "      test_images_np[i][0],\n",
    "      detections['detection_boxes'][0].numpy(),\n",
    "      detections['detection_classes'][0].numpy().astype(np.uint32)\n",
    "      + label_id_offset,\n",
    "      detections['detection_scores'][0].numpy(),\n",
    "      category_index, figsize=(15, 20), image_name=\"gif_frame_\" + ('%02d' % i) + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW1FrT2iNnpy"
   },
   "outputs": [],
   "source": [
    "imageio.plugins.freeimage.download()\n",
    "\n",
    "anim_file = 'duckies_test.gif'\n",
    "\n",
    "filenames = glob.glob('gif_frame_*.jpg')\n",
    "filenames = sorted(filenames)\n",
    "last = -1\n",
    "images = []\n",
    "for filename in filenames:\n",
    "  image = imageio.imread(filename)\n",
    "  images.append(image)\n",
    "\n",
    "imageio.mimsave(anim_file, images, 'GIF-FI', fps=5)\n",
    "\n",
    "display(IPyImage(open(anim_file, 'rb').read()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "interactive_eager_few_shot_od_training_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
