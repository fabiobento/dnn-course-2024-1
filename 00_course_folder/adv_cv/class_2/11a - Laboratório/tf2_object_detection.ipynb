{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/adv_cv/class_2/11a%20-%20Laborat%C3%B3rio/tf2_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*Adaptado deste [tutorial oficial](https://github.com/tensorflow/docs/blob/master/site/en/hub/tutorials/tf2_object_detection.ipynb) disponível em [tensorflow.org](https://www.tensorflow.org/hub/tutorials/tf2_object_detection)*]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Colab de detecção de objetos do Hub do TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRImnk_7WOq1"
   },
   "source": [
    "### Mais modelos\n",
    "[Na página do Tensorflow no Kaggle](https://www.kaggle.com/models?task=17074&framework=tensorFlow2&publisher=tensorflow) você encontra modelos de detecção de objetos TF2 que foram treinados no conjunto de dados COCO 2017.\n",
    "\n",
    "[Aqui](https://www.kaggle.com/models?task=17074&framework=tensorFlow2&publisher=tensorflow) você pode encontrar todos os modelos de detecção de objetos do Tensorflow 2 que estão atualmente hospedados em [www.kaggle.com](https://www.kaggle.com/models?framework=tensorFlow2&publisher=tensorflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Importações e configuração\n",
    "\n",
    "Vamos começar com as importações básicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:20:26.559965Z",
     "iopub.status.busy": "2024-03-09T13:20:26.559435Z",
     "iopub.status.idle": "2024-03-09T13:20:33.873706Z",
     "shell.execute_reply": "2024-03-09T13:20:33.872633Z"
    },
    "id": "Xk4FU-jx9kc3"
   },
   "outputs": [],
   "source": [
    "# Este Colab requer uma versão recente do numpy.\n",
    "!pip install numpy==1.24.3\n",
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Nota: No Google Colab, você precisa reiniciar a runtime para finalizar a instalação dos pacotes. Você pode fazer isso selecionando Runtime > Restart Runtime na barra de menus. **Não prossiga para a próxima seção sem reiniciar.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:20:33.878594Z",
     "iopub.status.busy": "2024-03-09T13:20:33.877814Z",
     "iopub.status.idle": "2024-03-09T13:20:37.050057Z",
     "shell.execute_reply": "2024-03-09T13:20:37.049187Z"
    },
    "id": "yn5_uV1HLvaz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IogyryF2lFBL"
   },
   "source": [
    "## Utilitários\n",
    "\n",
    "Execute a célula a seguir para criar alguns utilitários que serão necessários posteriormente:\n",
    "\n",
    "- Método auxiliar para carregar uma imagem\n",
    "- Mapa do nome do modelo para o identificador do hub TF\n",
    "- Lista de tuplas com pontos-chave humanos para o conjunto de dados COCO 2017. Isso é necessário para modelos com pontos-chave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-03-09T13:20:37.055145Z",
     "iopub.status.busy": "2024-03-09T13:20:37.054652Z",
     "iopub.status.idle": "2024-03-09T13:20:37.067532Z",
     "shell.execute_reply": "2024-03-09T13:20:37.066791Z"
    },
    "id": "-y9R0Xllefec"
   },
   "outputs": [],
   "source": [
    "# @title Run this!!\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Carrega uma imagem de um arquivo em uma matriz numpy.\n",
    "\n",
    "  Coloca a imagem em uma matriz numpy para alimentar o gráfico do tensorflow.\n",
    "  Observe que, por convenção, nós a colocamos em uma matriz numpy com a forma\n",
    "  (altura, largura, canais), onde canais=3 para RGB.\n",
    "\n",
    "  Args:\n",
    "    path: o caminho do arquivo para a imagem\n",
    "\n",
    "  Retorna:\n",
    "    matriz numpy uint8 com formato (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  image = None\n",
    "  if(path.startswith('http')):\n",
    "    response = urlopen(path)\n",
    "    image_data = response.read()\n",
    "    image_data = BytesIO(image_data)\n",
    "    image = Image.open(image_data)\n",
    "  else:\n",
    "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "ALL_MODELS = {\n",
    "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
    "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
    "'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
    "'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
    "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
    "'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
    "'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
    "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
    "'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
    "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
    "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
    "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
    "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
    "'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
    "'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
    "'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
    "'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
    "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
    "'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
    "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
    "'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
    "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
    "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
    "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
    "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
    "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
    "'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
    "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
    "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
    "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
    "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
    "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
    "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
    "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
    "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
    "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
    "}\n",
    "\n",
    "IMAGES_FOR_TEST = {\n",
    "  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n",
    "  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n",
    "  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
    "  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n",
    "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
    "  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n",
    "  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
    "  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
    "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
    "  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
    "}\n",
    "\n",
    "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
    " (0, 2),\n",
    " (1, 3),\n",
    " (2, 4),\n",
    " (0, 5),\n",
    " (0, 6),\n",
    " (5, 7),\n",
    " (7, 9),\n",
    " (6, 8),\n",
    " (8, 10),\n",
    " (5, 6),\n",
    " (5, 11),\n",
    " (6, 12),\n",
    " (11, 12),\n",
    " (11, 13),\n",
    " (13, 15),\n",
    " (12, 14),\n",
    " (14, 16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14bNk1gzh0TN"
   },
   "source": [
    "## Ferramentas de visualização\n",
    "\n",
    "Para visualizar as imagens com as caixas, os pontos-chave e a segmentação detectados corretamente, usaremos a API de detecção de objetos do TensorFlow. Para instalá-la, vamos clonar o repositório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:20:37.071182Z",
     "iopub.status.busy": "2024-03-09T13:20:37.070563Z",
     "iopub.status.idle": "2024-03-09T13:20:39.807233Z",
     "shell.execute_reply": "2024-03-09T13:20:39.805891Z"
    },
    "id": "oi28cqGGFWnY"
   },
   "outputs": [],
   "source": [
    "# Clonar o repositório de modelos do Tensorflow se ele ainda não existir\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para compatibilidade. Fixe a versão oficial do tf-models para que ele use o Tensorflow 2.15.\n",
    "!sed -i 's/tf-models-official>=2.5.1/tf-models-official==2.15.0/g' ./models/research/object_detection/packages/tf2/setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yX3pb_pXDjYA"
   },
   "source": [
    "Instalação da API de detecção de objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:20:39.811798Z",
     "iopub.status.busy": "2024-03-09T13:20:39.811452Z",
     "iopub.status.idle": "2024-03-09T13:21:55.505801Z",
     "shell.execute_reply": "2024-03-09T13:21:55.504672Z"
    },
    "id": "NwdsBdGhFanc"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt install -y protobuf-compiler\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yDNgIx-kV7X"
   },
   "source": [
    "Agora podemos importar as dependências de que precisaremos mais tarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:21:55.511178Z",
     "iopub.status.busy": "2024-03-09T13:21:55.510407Z",
     "iopub.status.idle": "2024-03-09T13:21:56.071101Z",
     "shell.execute_reply": "2024-03-09T13:21:56.070303Z"
    },
    "id": "2JCeQU3fkayh"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKtD0IeclbL5"
   },
   "source": [
    "### Carregar dados do mapa de rótulos (para plotagem).\n",
    "\n",
    "Os mapas de rótulos correspondem a números de índice a nomes de categorias, de modo que, quando nossa rede de convolução prevê `5`, sabemos que isso corresponde a `airplane`.\n",
    "\n",
    " Aqui, usamos funções utilitárias internos, mas qualquer coisa que retorne um dicionário mapeando números inteiros para rótulos de cadeia de caracteres apropriados seria adequada.\n",
    "\n",
    "Para simplificar, vamos carregar do repositório que carregamos o código da API de detecção de objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:21:56.075965Z",
     "iopub.status.busy": "2024-03-09T13:21:56.075141Z",
     "iopub.status.idle": "2024-03-09T13:21:56.085924Z",
     "shell.execute_reply": "2024-03-09T13:21:56.085248Z"
    },
    "id": "5mucYUS6exUJ"
   },
   "outputs": [],
   "source": [
    "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6917xnUSlp9x"
   },
   "source": [
    "## Crie um modelo de detecção e carregue os pesos do modelo pré-treinado\n",
    "\n",
    "Aqui, escolheremos o modelo de detecção de objetos que usaremos.\n",
    "Selecione a arquitetura e ela será carregada automaticamente.\n",
    "Se você quiser alterar o modelo para tentar outras arquiteturas posteriormente, basta alterar a próxima célula e executar as seguintes.\n",
    "\n",
    "**Dica**: se quiser ler mais detalhes sobre o modelo selecionado, você pode seguir o link (identificador do modelo) e ler a documentação adicional. Depois de selecionar um modelo, imprimiremos o identificador para facilitar o processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:21:56.089635Z",
     "iopub.status.busy": "2024-03-09T13:21:56.089013Z",
     "iopub.status.idle": "2024-03-09T13:21:56.094023Z",
     "shell.execute_reply": "2024-03-09T13:21:56.093304Z"
    },
    "id": "HtwrSqvakTNn"
   },
   "outputs": [],
   "source": [
    "#@title Seleção de Modelo { display-mode: \"form\", run: \"auto\" }\n",
    "model_display_name = 'CenterNet HourGlass104 Keypoints 512x512' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
    "model_handle = ALL_MODELS[model_display_name]\n",
    "\n",
    "print('Modelo Selecionado:'+ model_display_name)\n",
    "print('Model Handle no TensorFlow Hub: {}'.format(model_handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muhUt-wWL582"
   },
   "source": [
    "## Carregando o modelo selecionado do TensorFlow Hub\n",
    "\n",
    "Aqui, precisamos apenas do identificador do modelo que foi selecionado e usamos a biblioteca do Tensorflow Hub para carregá-lo na memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:21:56.097525Z",
     "iopub.status.busy": "2024-03-09T13:21:56.096993Z",
     "iopub.status.idle": "2024-03-09T13:22:45.723723Z",
     "shell.execute_reply": "2024-03-09T13:22:45.722842Z"
    },
    "id": "rBuD07fLlcEO"
   },
   "outputs": [],
   "source": [
    "print('carregando modelo...')\n",
    "hub_model = hub.load(model_handle)\n",
    "print('modelo carregado!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIawRDKPPnd4"
   },
   "source": [
    "## Carregando uma imagem\n",
    "\n",
    "Vamos testar o modelo em uma imagem simples. Para ajudar nisso, fornecemos uma lista de imagens de teste.\n",
    "\n",
    "Aqui estão algumas coisas simples que você pode experimentar se estiver curioso:\n",
    "* Tente executar a inferência em suas próprias imagens, basta carregá-las no colab e carregá-las da mesma forma que é feito na célula abaixo.\n",
    "* Modifique algumas das imagens de entrada e veja se a detecção ainda funciona.  Algumas coisas simples a serem testadas aqui incluem inverter a imagem horizontalmente ou convertê-la em escala de cinza (observe que ainda esperamos que a imagem de entrada tenha 3 canais).\n",
    "\n",
    "**Cuidado:** ao usar imagens com um canal alfa, o modelo espera imagens com 3 canais e o alfa será contado como o quarto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:22:45.727619Z",
     "iopub.status.busy": "2024-03-09T13:22:45.727316Z",
     "iopub.status.idle": "2024-03-09T13:22:48.716029Z",
     "shell.execute_reply": "2024-03-09T13:22:48.715128Z"
    },
    "id": "hX-AWUQ1wIEr"
   },
   "outputs": [],
   "source": [
    "#@title Image Selection (don't forget to execute the cell!) { display-mode: \"form\"}\n",
    "selected_image = 'Beach' # @param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n",
    "flip_image_horizontally = False #@param {type:\"boolean\"}\n",
    "convert_image_to_grayscale = False #@param {type:\"boolean\"}\n",
    "\n",
    "image_path = IMAGES_FOR_TEST[selected_image]\n",
    "image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "# Flip horizontally\n",
    "if(flip_image_horizontally):\n",
    "  image_np[0] = np.fliplr(image_np[0]).copy()\n",
    "\n",
    "# Convert image to grayscale\n",
    "if(convert_image_to_grayscale):\n",
    "  image_np[0] = np.tile(\n",
    "    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(24,32))\n",
    "plt.imshow(image_np[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTHsFjR6HNwb"
   },
   "source": [
    "## Fazendo a inferência\n",
    "\n",
    "Para fazer a inferência, precisamos apenas chamar nosso modelo carregado no TF Hub.\n",
    "\n",
    "Coisas que você pode tentar:\n",
    "* Imprimir o `result['detection_boxes']` e tentar corresponder os locais das caixas às caixas na imagem.  Observe que as coordenadas são fornecidas em formato normalizado (ou seja, no intervalo [0, 1]).\n",
    "* Inspecione outras chaves de saída presentes no resultado. Uma documentação completa pode ser vista na página de documentação dos modelos (apontando seu navegador para o identificador do modelo impresso anteriormente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:22:48.773296Z",
     "iopub.status.busy": "2024-03-09T13:22:48.772935Z",
     "iopub.status.idle": "2024-03-09T13:22:51.675720Z",
     "shell.execute_reply": "2024-03-09T13:22:51.674910Z"
    },
    "id": "Gb_siXKcnnGC"
   },
   "outputs": [],
   "source": [
    "# Executando a inferência\n",
    "results = hub_model(image_np)\n",
    "\n",
    "# Diferentes modelos de detecção de objetos têm resultados adicionais\n",
    "# Todos eles são explicados na documentação\n",
    "result = {key:value.numpy() for key,value in results.items()}\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ5VYaBoeeFM"
   },
   "source": [
    "## Visualização dos resultados\n",
    "\n",
    "É aqui que precisaremos da API de detecção de objetos do TensorFlow para mostrar os quadrados da etapa de inferência (e os pontos-chave, quando disponíveis).\n",
    "\n",
    "A documentação completa desse método pode ser vista [aqui](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py)\n",
    "\n",
    "Aqui você pode, por exemplo, definir `min_score_thresh` para outros valores (entre 0 e 1) para permitir mais detecções ou para filtrar mais detecções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:22:51.679501Z",
     "iopub.status.busy": "2024-03-09T13:22:51.679181Z",
     "iopub.status.idle": "2024-03-09T13:22:54.104627Z",
     "shell.execute_reply": "2024-03-09T13:22:54.103771Z"
    },
    "id": "2O7rV8g9s8Bz"
   },
   "outputs": [],
   "source": [
    "label_id_offset = 0\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "# Use pontos-chave se disponíveis nas detecções\n",
    "keypoints, keypoint_scores = None, None\n",
    "if 'detection_keypoints' in result:\n",
    "  keypoints = result['detection_keypoints'][0]\n",
    "  keypoint_scores = result['detection_keypoint_scores'][0]\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_detections[0],\n",
    "      result['detection_boxes'][0],\n",
    "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
    "      result['detection_scores'][0],\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=.30,\n",
    "      agnostic_mode=False,\n",
    "      keypoints=keypoints,\n",
    "      keypoint_scores=keypoint_scores,\n",
    "      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(24,32))\n",
    "plt.imshow(image_np_with_detections[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qaw6Xi08NpEP"
   },
   "source": [
    "## [Opcional]\n",
    "\n",
    "Entre os modelos de detecção de objetos disponíveis, há o Mask R-CNN e a saída desse modelo permite a segmentação da instância.\n",
    "\n",
    "Para visualizá-la, usaremos o mesmo método que usamos antes, mas adicionando um parâmetro adicional: `instance_masks=output_dict.get('detection_masks_reframed', None)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T13:22:54.156015Z",
     "iopub.status.busy": "2024-03-09T13:22:54.155156Z",
     "iopub.status.idle": "2024-03-09T13:22:56.475955Z",
     "shell.execute_reply": "2024-03-09T13:22:56.475069Z"
    },
    "id": "zl3qdtR1OvM_"
   },
   "outputs": [],
   "source": [
    "# Manipular modelos com máscaras:\n",
    "image_np_with_mask = image_np.copy()\n",
    "\n",
    "if 'detection_masks' in result:\n",
    "  # precisamos converter np.arrays em tensores\n",
    "  detection_masks = tf.convert_to_tensor(result['detection_masks'][0])\n",
    "  detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])\n",
    "\n",
    "  # Reformule a máscara da caixa de seleção para o tamanho da imagem.\n",
    "  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes,\n",
    "              image_np.shape[1], image_np.shape[2])\n",
    "  detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                      tf.uint8)\n",
    "  result['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_mask[0],\n",
    "      result['detection_boxes'][0],\n",
    "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
    "      result['detection_scores'][0],\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=.30,\n",
    "      agnostic_mode=False,\n",
    "      instance_masks=result.get('detection_masks_reframed', None),\n",
    "      line_thickness=8)\n",
    "\n",
    "plt.figure(figsize=(24,32))\n",
    "plt.imshow(image_np_with_mask[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tf2_object_detection.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
