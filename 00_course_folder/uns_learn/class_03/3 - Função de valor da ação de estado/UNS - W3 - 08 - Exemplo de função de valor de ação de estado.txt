Usando o exemplo da função de valor de ação de estado. Você está vendo como são os valores do QSA. Para manter nossa intuição sobre os problemas de aprendizado por reforço e como os valores do QSA mudam dependendo do problema, forneceremos um laboratório opcional. Isso permite que você modifique o exemplo [inaudível] e veja por si mesmo como o QSA mudará. Vamos dar uma olhada. Aqui está um notebook Jupyter com o qual espero que você jogue depois de assistir a este vídeo. Vou executar essas funções auxiliares, agora observe aqui que isso especifica o número seis das duas ações, então eu não as alteraria. E isso especifica as recompensas do terminal à esquerda no terminal à direita, que foram 140 e, em seguida, zero foram as recompensas dos estados intermediários. O fator de desconto aposta 0,5. E vamos ignorar a probabilidade de erro. Por enquanto, falaremos sobre isso em um vídeo posterior. E com esses valores, se você executar esse código, ele calculará e visualizará a política ideal, bem como a função Q Q de SA. Você aprenderá mais tarde sobre como desenvolver um algoritmo de aprendizado para estimar ou calcular você mesmo Q de SA. Então, por enquanto, não se preocupe com o código que escrevemos para calcular Q of SA. Mas você vê que os valores aqui Q of SA são os valores que vimos na palestra. Agora é aqui que a diversão começa. Vamos mudar alguns dos valores e ver como essas coisas mudam. Vou atualizar a recompensa certa do terminal para um valor muito menor, digamos apenas 10. Se eu agora executar o código novamente, veja como Q of SA muda e agora pense que se você estiver no estado 5. Então, se você for para a esquerda e se comportar de maneira ideal, obterá 6,25. Por outro lado, se você for para a direita e se comportar também depois, receberá um retorno de apenas cinco. Agora, quando a recompensa à direita é tão pequena, são apenas 10. Mesmo quando você está tão perto de você, prefira ir para a esquerda até o fim. E, de fato, a política automotiva agora deve sair de cada estado. Vamos fazer algumas outras mudanças. Vou mudar a recompensa certa do terminal de volta para 40. Mas deixe-me mudar o fator de desconto para 0,9, com um fator de desconto mais próximo de um. Isso faz com que o Mars Rover seja menos impaciente e esteja disposto a levar mais tempo para esperar por uma recompensa maior, porque as recompensas no futuro não são multiplicadas por 0,5, mas alguma alta potência é multiplicada por 0,9 para alguma alta potência. E por isso está disposto a ser mais paciente, porque as recompensas no futuro não são descontadas ou multiplicadas por um número tão pequeno quanto quando o desconto era de 0,5. Então, vamos executar o código novamente. E agora você vê que isso é Q de SA para os diferentes estados e agora, para o estado 5, ir para a esquerda, na verdade, oferece uma recompensa maior de 65,61 em comparação com 36. Observe, a propósito, que 36 é 0,9 vezes essa recompensa terminal de 40. Então, esses números fazem sentido. Mas quando um paciente pequeno está disposto a ir para a esquerda, mesmo quando você está no estado 5. Agora vamos mudar a gama para um número muito menor, como 0,3. Portanto, isso desconta muito as recompensas no futuro. Isso o torna incrivelmente impaciente. Então, deixe-me executar esse código novamente e agora o comportamento mudou. Percebi que agora no estado 4 não vou ter paciência para escolher a recompensa maior de 100, porque o fator de desconto gama agora é tão pequeno é 0,3. Prefiro receber a recompensa de 40, embora seja uma recompensa muito menor, esteja mais próxima, e isso é o que escolhermos fazer. Então, espero que você possa ter uma ideia brincando com esses números sozinho e executando esse código. Como os valores de Q de SA mudam, assim como o retorno ideal que você nota é o maior desses dois números QSA. Como isso muda e como a política ideal também muda. Então, espero que você brinque com o laboratório opcional e altere a função de recompensa, altere a gama do fator de desconto e experimente valores diferentes. E veja você mesmo como os valores de Q of SA mudam, como o retorno ideal de diferentes estados muda e como a política automática muda dependendo desses valores diferentes. E, ao fazer isso, espero que isso aprimore sua intuição sobre como essas diferentes quantidades são afetadas, dependendo das recompensas e assim por diante no aplicativo de aprendizado por reforço. Depois de ir para o laboratório, estaremos prontos para voltar e conversar sobre o que provavelmente é a equação mais importante no aprendizado por reforço, que é algo chamado equação de Bellman. Então, espero que você se divirta brincando com o laboratório opcional e, depois disso, voltemos a falar sobre as equações de Bellman.
Obrigatória
pt-BR
​

