Deixe-me resumir onde estamos. Se você puder calcular a função de valor da ação de estado Q de S, A , ela fornecerá uma maneira de escolher uma boa ação de cada cena. Basta escolher a ação A, que fornece o maior valor de Q de S, A. A questão é: como você calcula esses valores Q de S, A? No aprendizado por reforço, há uma equação chave chamada equação de Bellman que nos ajudará a calcular a função de valor da ação estadual. Vamos dar uma olhada no que é essa equação. Como um lembrete, esta é a definição de Q de S, A. É retorno se começarmos no estado S, tomarmos a ação de uma vez e eles se comportarem de maneira ideal depois disso. Para descrever a equação de Bellman, vou usar a seguinte notação. Vou usar S para indicar o estado atual. Em seguida, vou usar R de S para indicar as recompensas do estado atual. Para nosso pequeno exemplo de MDP, teremos que r de um Estado1 é 100. A recompensa do Estado 2 é 0 e assim por diante. A recompensa do Estado 6 é 40. Vou usar o alfabeto A para indicar a ação atual, a ação que você executa no estado S. Depois de realizar a ação a, você chega a um novo estado. Por exemplo, se você estiver no Estado 4 e fizer a ação para a esquerda , você chegará ao Estado 3. Vou usar S primo para indicar o estado que você alcança depois de realizar aquela ação a do estado atual S. Eu também vou usar A primo para indicar a ação que você pode realizar no estado S primo, o novo estável que você alcançou. A convenção de notação, aliás, é que S, A correspondem ao estado e à ação atuais. Quando adicionamos o primo, esse é o próximo estado e, em seguida, a próxima ação. A equação de Bellman é a seguinte. Diz que Q de S, A, que é o retorno sob esse conjunto de suposições que é igual a r de S, diz recompensa que você recebe por estar nesse estado mais o fator de desconto Gamma vezes max sobre todas as ações possíveis, um primo de q de S primo, o novo estado em que você acabou de chegar e, em seguida, um primo. Há muita coisa acontecendo nessa equação. Vamos primeiro dar uma olhada em alguns exemplos. Voltaremos para ver por que essa equação pode fazer sentido. Vejamos um exemplo. Vamos dar uma olhada no Q of State 2 e na ação. Aplique a Equação de Bellman a isso para ver qual valor ela nos dá. Se o estado atual for o estado dois e a ação for para a direita, no dia seguinte você chegará, depois de escrever S prime, seria o Estado 3. A equação de Bellman diz que Q de 2, à direita é R de S. Esse R Estado 2, que é apenas a recompensa zero mais o fator de desconto Gamma, que definimos como 0,5 neste exemplo, vezes o máximo dos valores de Q no estado S primo no Estado 3. Isso vai ser o máximo de 25 e 6,25, já que isso é máximo sobre um primo de q de S primo vírgula um primo. Isso é pegar o maior de 25 ou 6,25 porque essas são as duas opções para o Estado 3. Isso acaba sendo igual a zero mais 0,5 vezes 25, o que é igual a 12,5, o que, felizmente, é Q de dois e, em seguida, a ação correta. Vejamos apenas mais um exemplo. Deixe-me pegar o Estado 4 e ver o que é Q do Estado 4 se você decidir ir para a esquerda. Nesse caso, o estado atual é quatro. A ação atual é ir para a esquerda. O próximo estado, se você puder começar das quatro, indo para a esquerda. Você acaba também no Estado 3. Vamos primar esses três novamente, a Equação de Bellman, diremos que isso é igual a R de S. Nosso estado quatro, que é zero mais 0,5 do fator de desconto Gama de max sobre um primo de q de S primo. Esse é o Estado 3 novamente, vírgula a primo. Mais uma vez, os valores de Q para o Estado 3 são 25 e 6,25 e o maior deles é 25. Isso resulta em nossos 40 mais 0,5 vezes 25, o que é novamente igual a 12,5. É por isso que q de quatro com a ação restante também é igual a 12,5, apenas uma nota. Se você estiver em um estado terminal, a Equação de Bellman simplifica para q de SA igual a r de S porque não há estado S primo e, portanto, esse segundo termo desapareceria. É por isso que Q de S, A nos estados terminais é apenas 100, 100 ou 40 40. Se desejar, sinta-se à vontade para pausar o vídeo e aplicar a Equação de Bellman a qualquer outra ação de estado neste MDP e verificar por si mesmo se essa matemática funciona. Apenas para recapitular, foi assim que definimos Q de S, A. Vimos anteriormente que o melhor retorno possível de qualquer estado S é máximo sobre um Q de S, A. Na verdade, apenas para renomear SNA, descobrimos que o melhor retorno possível de um estado S primo é máximo sobre S primo de um primo. Eu realmente não fiz nada além de renomear S, S prime e a para prime. Mas isso facilitará um pouco mais tarde algumas das intuições. Mas para qualquer estado S primo, como o Estado 3, o melhor retorno possível, digamos, do Estado 3 é o máximo sobre todas as ações possíveis de Q de S primo E primo. Aqui está novamente a equação de Bellman. A intuição que isso captura é que, se você está partindo do estado s e vai agir de forma ideal depois disso, verá uma sequência de recompensas ao longo do tempo. Em particular, o retorno será calculado a partir da recompensa na primeira etapa, mais Gama vezes recompensa na segunda etapa mais Gama ao quadrado vezes recompensa na terceira etapa, e assim por diante. Mais ponto, ponto, ponto até chegar ao estado terminal. O que a equação de Bellman diz é que essa sequência de recompensas, o que é o fator de desconto, pode ser dividida em dois componentes. Primeiro, esse R de s, é a recompensa que você recebe imediatamente. Na literatura de aprendizado por reforço, isso às vezes também é chamado de recompensa imediata, mas é isso que R_1 é. É a recompensa que você recebe por começar em alguns estados . O segundo termo, então, é o seguinte; depois de começar no estado s e realizar a ação a, você chega a algum novo estado s prime. A definição de Q de s, a pressupõe que vamos nos comportar de maneira ideal depois disso. Depois de chegarmos ao s prime, vamos nos comportar de maneira ideal e obter o melhor retorno possível do estado s prime. O que é isso, máximo de um primo de Q de s primo a primo, esse é o retorno de se comportar de maneira ideal, partindo do estado s primo. Isso é exatamente o que escrevemos aqui, é o melhor retorno possível para quando você começa do estado s prime. Outra forma de expressar isso é que esse retorno total aqui embaixo também é igual a R_1 mais, e então vamos fatorar Gama no mapa, é Gama vezes R_2 mais, e em vez de Gama ao quadrado é apenas Gama vezes R_3 mais Gama ao quadrado vezes R_4 mais ponto ponto ponto ponto ponto ponto. Observe que se você estivesse começando do estado s primo, a sequência de recompensas que você receberá será R_2, R_3, depois R_4 e assim por diante. É por isso que essa expressão aqui é o retorno total se você começar do estado s primo. Se você se comportasse de maneira ideal , essa expressão deveria ser o melhor retorno possível para começar do estado s primo, e é por isso que essa sequência de recompensas de desconto é igual ao máximo de um primo de Q de s primo a primo e também sobrou esse fator de desconto extra Gamma ali, e é por isso que Q de s, a também é igual a essa expressão aqui. Caso você ache que isso é muito complicado e não esteja acompanhando todos os detalhes, não se preocupe. Contanto que você aplique essa equação, você conseguirá obter os resultados certos. Mas a intuição de alto nível que espero que você aprenda é que o retorno total que você obtém no problema de aprendizado por reforço tem duas partes. A primeira parte é essa recompensa que você recebe imediatamente , e a segunda parte é Gamma vezes o retorno que você obtém a partir do próximo estado s prime. Juntando esses dois componentes, R de s mais Gamma vezes o retorno do próximo estado, que é igual ao retorno total do estado atual s. Essa é a essência da equação de Bellman. Apenas para relacionar isso com nosso exemplo anterior, Q de 4, à esquerda. Esse é o retorno total para começar o Estado 4 e ir para a esquerda. Se você fosse para a esquerda no Estado 4, as recompensas que você ganharia seriam 0 no Estado 4, 0 no Estado 3, 0 no Estado 2 e depois 100, e é por isso que o retorno total é esse; 0,5 ao quadrado mais 0,5 cúbico, ou seja, 12,5. O que a equação de Bellman está dizendo é que podemos dividir isso em duas partes. Existe esse zero, que é R do estado quatro, e depois mais 0,5 vezes essa outra sequência, 0 mais 0,50 mais 0,5 ao quadrado vezes 100. Mas se você observar o que é essa sequência, esse é realmente o retorno ideal do próximo estado s primo que você obteve após realizar a ação à esquerda do estado quatro. É por isso que isso é igual à recompensa 4 mais 0,5 vezes o retorno ideal do Estado 3. Porque se você começasse do Estado 3, as recompensas que você receberia seriam zero seguido por zero seguido por 100, então esse é o retorno ideal do Estado 3 e é por isso que é apenas R de 4 mais 0,5 máximo sobre um Q primo do Estado 3, um primo. Eu conheço a equação de Bellman, essa é uma equação um tanto complicada que divide seus retornos totais na recompensa que você está recebendo imediatamente. A recompensa imediata mais Gamma vezes os retornos do próximo estado s prime. Se fizer sentido para você, mas não totalmente, tudo bem. Não se preocupe com isso. Você ainda pode aplicar as equações de Bellman para fazer com que um algoritmo de aprendizado por reforço funcione corretamente, mas espero que seja pelo menos a intuição de alto nível de por que dividir as recompensas entre o que você recebe imediatamente e o que receberá no futuro. Espero que isso faça sentido. Antes de prosseguir com o desenvolvimento de um algoritmo de aprendizado por reforço, apresentaremos a seguir um vídeo opcional sobre processos de decisão de Markov estocástico ou sobre aplicativos de aprendizado por reforço em que as ações, se você tomar, podem ter um efeito ligeiramente aleatório. Dê uma olhada no vídeo opcional, se desejar. Depois disso, começaremos a desenvolver um algoritmo de aprendizado por reforço.
