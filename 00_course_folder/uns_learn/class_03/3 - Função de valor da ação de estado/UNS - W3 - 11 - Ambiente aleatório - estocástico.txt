Em alguns aplicativos, quando você executa uma ação, o resultado nem sempre é totalmente confiável. Por exemplo, se você ordenar que seu rover de Marte vá para a esquerda, talvez haja um pequeno deslizamento de rocha, ou talvez o chão esteja muito escorregadio e, portanto, ele deslize e vá na direção errada. Na prática, muitos robôs nem sempre conseguem fazer exatamente o que você manda por causa do vento soprando e saindo do curso, da roda escorregando ou de qualquer outra coisa. Há uma generalização da estrutura de aprendizado por reforço sobre a qual falamos até agora, que modela ambientes aleatórios ou estocásticos. Neste vídeo opcional, falaremos sobre como esses problemas de aprendizado por reforço funcionam. Continuando com nosso exemplo simplificado do Mars Rover, digamos que você execute a ação e ordene que ela vá para a esquerda. Na maioria das vezes, você terá sucesso, mas e se 10% das vezes ou 0,1 das vezes, ela realmente acabar escorregando acidentalmente e indo na direção oposta? Se você ordenar que ele vá para a esquerda, ele tem 90 por cento de chance ou 0,9 de chance de ir corretamente na direção esquerda. Mas a chance de 0,1 de realmente ir para a direita tem 9% de chance de terminar em, digamos, três neste exemplo e 10% de chance de terminar no estado cinco. Por outro lado, se você ordenar que ele vá para a direita e realize a ação, certo, ele tem 0,9 chance de terminar no estado cinco e 0,1 chance de terminar no estado três. Esse seria um exemplo de ambiente estocástico. Vamos ver o que acontece nesse problema de aprendizado por reforço. Digamos que você use a política mostrada aqui, na qual você vai para a esquerda nos estágios 2 3 4 e vai para a direita ou tenta ir para a direita no estado cinco. Se você começasse no estado quatro e seguisse esta política, a sequência real de estados que você visita pode ser aleatória. Por exemplo, no estado quatro, você vai para a esquerda, e talvez seu circuito tenha sorte, e ele realmente chegue ao estado três, e então você tenta ir para a esquerda novamente, e talvez ele realmente chegue lá. Você diz para ele ir para a esquerda novamente, e ele chega a esse estado. Se é isso que acontece, você acaba com a sequência de recompensas 000100. Mas se você tentar exatamente essa mesma política pela segunda vez, talvez tenha um pouco menos sorte, na segunda vez que começar aqui. Tente ir para a esquerda e ver o sucesso, então um zero do estado quatro, zero do estado três, ouça você mandar ele ir para a esquerda, mas desta vez você não teve sorte e o robô escorrega e acaba voltando para o estado quatro. Então você é ensinado a ligar para a esquerda, para a esquerda e para a esquerda e, eventualmente, chegar à recompensa de 100. Nesse caso, essa será a sequência de recompensas que você observará. Este de quatro para três para quatro, três, dois, depois um, ou até mesmo é possível, se você disser do estado quatro para ir para a esquerda seguindo a política, você pode ter azar mesmo na primeira etapa e acabar indo para o estado cinco porque ela caiu. Em seguida, diga cinco, você ordena que vá para a direita, e ele terá sucesso quando você chegar aqui. Nesse caso, a sequência de recompensas que você vê será 0040, porque passou de quatro para cinco e, em seguida, afirma seis. Anteriormente, escrevemos o retorno como essa soma das recompensas com desconto. Mas quando o problema do aprendizado por reforço é estocástico, não há uma sequência de recompensas que você veja com certeza, em vez disso, você vê essa sequência de recompensas diferentes. Em um problema de aprendizado por reforço estocástico, o que nos interessa não é maximizar o retorno porque esse é um número aleatório. O que nos interessa é maximizar o valor médio da soma das recompensas com desconto. Por valor médio, quero dizer, se você pegasse sua apólice e a testasse mil vezes, 100.000 vezes ou um milhão de vezes, obteria muitas sequências de recompensas diferentes como essa e se obtivesse a média de todas essas sequências diferentes da soma das recompensas com desconto , isso é o que chamamos de retorno esperado. Nas estatísticas, o termo esperado é apenas outra forma de dizer média. Mas isso significa que queremos maximizar o que esperamos obter em média em termos da soma das recompensas com desconto. A notação matemática para isso é escrever isso como E. E representa o valor esperado de R1 mais Gamma R2 plus, e assim por diante. O trabalho do algoritmo de aprendizado por reforço é escolher uma política Pi para maximizar a média ou a soma esperada das recompensas com desconto. Resumindo, quando você tem um problema de aprendizado por reforço estocástico ou um processo de decisão estocástico de Markov, o objetivo é escolher uma política que nos diga qual ação tomar no estado S para maximizar o retorno esperado. A última maneira pela qual isso muda, falamos sobre isso, é que modifica um pouco a equação de Bellman. Aqui está a equação de Bellman exatamente como escrevemos. Mas a diferença agora é que quando você executa a ação a no estado s, o próximo estado s primo a que você chega é aleatório. Quando você está no estado 3 e diz para ele ir para a esquerda no próximo estado s primo, pode ser o estado 2 ou pode ser o estado 4. O S prime agora é aleatório, e é por isso que também colocamos um operador médio ou um operador inesperado aqui. Dizemos que o retorno total do estado s, tomando a ação a, uma vez em um comportamento ideal, é igual à recompensa que você recebe imediatamente, também chamada de recompensa imediata mais o fator de desconto, Gamma mais o que você espera obter em média dos retornos futuros. Se você quiser aprimorar sua intuição sobre o que acontece com esses problemas de aprendizado por reforço estocástico. Você voltaria ao laboratório opcional que mostrei há pouco, onde esse problema de erro de parâmetro é a probabilidade de seu Mars Rover ir na direção oposta à que você ordenou. Se dissermos que o erro dois é 0,1 e reexecutamos o Notebook e, portanto, esses números aqui são o retorno ideal se você tomasse as melhores ações possíveis, adote essa política ideal, mas o robô pisaria na direção errada 10 por cento das vezes e esses são os valores q desse NTP estocástico. Observe que esses valores agora estão um pouco mais baixos porque você não pode controlar o robô tão bem quanto antes. Os valores de q, assim como os retornos ideais, caíram um pouco. Na verdade, se você aumentasse a probabilidade de erro, digamos, em 40% das vezes, o robô nem mesmo segue direções. Você está comandando isso apenas 60 por cento das vezes. Ele vai para onde você disse, então esses valores acabam ficando ainda mais baixos porque seu grau de controle sobre o robô diminuiu. Eu encorajo você a usar o laboratório opcional e alterar o valor da probabilidade de erro e ver como isso afeta o retorno esperado ou o retorno automático esperado, bem como os valores Q, q de s a. Agora, em tudo o que fizemos até agora, temos usado esse processo de decisão de Markov, esse rover de Marte com apenas seis estados. Para muitas aplicações práticas, o número de estados será muito maior. No próximo vídeo, pegaremos a estrutura de aprendizado por reforço ou processo de decisão de Markov sobre a qual falamos até agora e a generalizaremos para esse conjunto de problemas muito mais rico e talvez ainda mais interessante, com espaços de estados muito maiores e, em particular, contínuos. Vamos dar uma olhada nisso no próximo vídeo.
