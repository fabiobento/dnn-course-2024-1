Para finalizar o formalismo do aprendizado por reforço, em vez de olhar para algo tão complicado quanto um helicóptero ou um cão-robô, podemos usar um exemplo simplificado que é vagamente inspirado no rover de Marte.
Reproduza o vídeo começando em ::18 e siga a transcrição0:18
Isso foi adaptado do exemplo devido à professora de Stanford Emma Branskill e a uma de minhas colaboradoras, Jagriti Agrawal, que na verdade escreveram um código que está realmente controlando o rover de Marte no momento, o que também me ajudou a conversar e a desenvolver esse exemplo. Vamos dar uma olhada. Desenvolveremos o aprendizado por reforço usando um exemplo simplificado inspirado no rover de Marte. Nesta aplicação, o rover pode estar em qualquer uma das seis posições, conforme mostrado nas seis caixas aqui. O rover, pode começar, digamos, na quarta caixa mostrada aqui. A posição do rover de Marte é chamada de estado no aprendizado por reforço, e vou chamar esses seis estados de estado 1, estado 2, estado 3, estado 4, estado 5 e estado 6, então o rover está começando no estado 4. Agora, o rover foi enviado a Marte para tentar realizar diferentes missões científicas. Ele pode ir a lugares diferentes para usar seus sensores, como uma furadeira, um radar ou um espectrômetro para analisar a rocha em diferentes lugares do planeta, ou ir a lugares diferentes para tirar fotos interessantes para os cientistas da Terra observarem. Neste exemplo, o estado 1 aqui à esquerda tem uma superfície muito interessante que os cientistas adorariam que o rover coletasse amostras. O estado 6 também tem uma superfície bastante interessante que os cientistas gostariam muito que o rover amostrasse, mas não tão interessante quanto o estado 1. É mais provável que realizemos a missão científica no estado 1 do que no estado 6, mas o estado 1 está mais distante. A maneira como refletiremos que o estado 1 é potencialmente mais valioso é por meio da função de recompensa. A recompensa no estado 1 é 100, e a recompensa no estágio 6 é 40, e as recompensas em todos os outros estados intermediários, vou escrever como uma recompensa de zero, porque não há muita ciência interessante a ser feita nesses estados 2, 3, 4 e 5. Em cada etapa, o rover pode escolher uma das duas ações. Ele pode ir para a esquerda ou para a direita. A questão é: o que o rover deve fazer? No aprendizado por reforço, prestamos muita atenção às recompensas porque é assim que sabemos se o robô está indo bem ou mal. Vejamos alguns exemplos do que poderia acontecer se o robô fosse para a esquerda, partindo do estado 4. Então, inicialmente a partir do estado 4, ele receberá uma recompensa de zero e, depois de ir para a esquerda, chegará ao estado 3, onde receberá novamente uma recompensa de zero. Em seguida, ele chega ao estado 2, recebe a recompensa 0 e, finalmente, apenas ao estado 1, onde recebe uma recompensa de 100. Para este aplicativo, presumo que, quando ele atingir o estado 1 ou o estado 6, o dia terminará. No aprendizado por reforço, às vezes chamamos isso de estado terminal, e isso significa que, depois de chegar a um desses estados terminais, recebe uma recompensa nesse estado, mas nada acontece depois disso. Talvez os robôs tenham ficado sem combustível ou sem tempo para o dia, e é por isso que eles só conseguem aproveitar a recompensa de 100 ou 40 , mas só isso por dia. Depois disso, ele não ganha recompensas adicionais. Agora, em vez de ir para a esquerda, o robô também poderia escolher ir para a direita. Nesse caso, do estado 4, ele primeiro teria uma recompensa de zero e, em seguida, se moverá para a direita e chegará ao estado 5, terá outra recompensa de zero e, em seguida, chegará a esse outro estado terminal à direita, estado 6 e receberá uma recompensa de 40. Mas ir para a esquerda e para a direita são as únicas opções. Uma coisa que o robô pode fazer é partir do estado 4 e decidir se mover para a direita. Ele vai do estado 4-5, recebe uma recompensa de zero no estado 4 e uma recompensa de zero no estado 5, e então talvez ele mude de ideia e decida começar a ir para a esquerda da seguinte forma. Nesse caso, ele receberá uma recompensa de zero no estado 4, no estado 3, no estado 2 e, em seguida, a recompensa de 100 quando chegar ao estado 1. Nessa sequência de ações e estados, o robô está perdendo mais tempo. Então, talvez essa não seja uma ótima maneira de agir, mas é uma opção que o algoritmo poderia escolher, mas espero que você não escolha essa. Resumindo, em cada etapa de tempo, o robô está em algum estado, que chamarei de S, e pode escolher uma ação, além de desfrutar de algumas recompensas, R de S, que recebe desse estado. Como resultado dessa ação, chegou a algum novo estado S prime. Como um exemplo concreto, quando o robô estava no estado 4 e executou a ação, vá para a esquerda, talvez não tenha gostado da recompensa de zero associada a esse estado 4 e não tenha nenhum novo estado 3. Quando você aprende sobre algoritmos específicos de aprendizado por reforço, você vê essas quatro coisas, o estado, a ação, a recompensa e o próximo estado, que é o que acontece basicamente toda vez que você realiza uma ação que é apenas um elemento central do que os algoritmos de aprendizado por reforço analisarão ao decidir como realizar ações. Só para esclarecer, a recompensa aqui, R de S, é a recompensa associada a esse estado. Essa recompensa de zero está associada ao estado 4 e não ao estado 3. Esse é o formalismo de como um aplicativo de aprendizado por reforço funciona. No próximo vídeo, vamos dar uma olhada em como especificamos exatamente o que queremos que o algoritmo de aprendizado por reforço faça. Em particular, falaremos sobre uma ideia importante no aprendizado por reforço chamada retorno. Vamos ao próximo vídeo para ver o que isso significa.
