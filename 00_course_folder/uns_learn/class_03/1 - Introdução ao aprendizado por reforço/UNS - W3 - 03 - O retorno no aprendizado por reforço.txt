Você viu no último vídeo quais são os estados da aplicação do aprendizado por reforço e como, dependendo das ações que você realiza, você passa por diferentes estados e também pode desfrutar de recompensas diferentes. Mas como saber se um conjunto específico de recompensas é melhor ou pior do que um conjunto diferente de recompensas? O retorno do aprendizado por reforço, que definiremos neste vídeo, nos permite capturar isso. À medida que analisamos isso, uma analogia que você pode achar útil é se você imaginar que tem uma nota de cinco dólares a seus pés, você pode estender a mão e pegar, ou meia hora do outro lado da cidade, você pode caminhar meia hora e pegar uma nota de 10 dólares. Qual deles você prefere ir atrás? Dez dólares é muito melhor do que cinco dólares, mas se você precisar caminhar por meia hora para pegar aquela nota de 10 dólares, talvez seja mais conveniente simplesmente pegar a nota de cinco dólares. O conceito de retorno mostra que as recompensas que você pode obter mais rápido talvez sejam mais atraentes do que as recompensas que levam muito tempo para receber. Vamos dar uma olhada exatamente em como isso funciona. Aqui está um exemplo do Mars Rover. Se, partindo do estado 4, você for para a esquerda, vimos que as recompensas que você recebe seriam zero na primeira etapa do estado 4, zero no estado 3, zero no estado 2 e, em seguida, 100 no estado 1, o estado terminal. O retorno é definido como a soma dessas recompensas, mas ponderado por um fator adicional , chamado de fator de desconto. O fator de desconto é um número um pouco menor que 1. Deixe-me escolher 0,9 como fator de desconto. Vou ponderar que a recompensa na primeira etapa é apenas zero, a recompensa na segunda etapa é um fator de desconto, 0,9 vezes essa recompensa e, em seguida, mais o fator de desconto ^ 2 vezes a recompensa e, em seguida, mais o fator de desconto ^ 3 vezes essa recompensa. Se você calcular isso, isso resulta em 0,729 vezes 100, que é 72,9.
Reproduza o vídeo começando em :2:13 e siga a transcrição2:13
A fórmula mais geral para o retorno é que se seu robô passar por alguma sequência de estados e receber a recompensa R_1 na primeira etapa e R_2 na segunda etapa e R_3 na terceira etapa, e assim por diante, o retorno será R_1 mais o fator de desconto Gamma, esse alfabeto grego Gamma que eu configurei como 0,9 neste exemplo, o Gamma vezes R_2 mais Gamma ^ 2 vezes R_3 mais Gamma^3 vezes R_4 e assim por diante, até chegar ao estado terminal. O que o fator de desconto Gamma faz é tornar o algoritmo de aprendizado por reforço um pouco impaciente. Como o retorno dá crédito total à primeira recompensa, 100% é 1 vezes R_1, mas dá um pouco menos crédito à recompensa que você recebe na segunda etapa é multiplicada por 0,9 e, em seguida, ainda menos crédito à recompensa que você recebe na próxima etapa R_3, e assim por diante, e assim por diante, e assim por diante, obtendo recompensas mais cedo resulta em um valor maior para o retorno total. Em muitos algoritmos de aprendizado por reforço, uma escolha comum para o fator de desconto será um número bem próximo de 1, como 0,9, 0,99 ou mesmo 0,999. Mas, para fins ilustrativos, no exemplo em execução que vou usar, na verdade vou usar um fator de desconto de 0,5. Isso diminui muito ou, dizemos, reduz muito as recompensas no futuro, porque a cada data e hora de análise adicional, você recebe apenas metade do crédito das recompensas que teria recebido um passo antes. Se Gamma fosse igual a 0,5, o retorno no exemplo acima teria sido 0 mais 0,5 vezes 0, substituindo essa equação na parte superior, mais 0,5 ^ 2 0 mais 0,5 ^ 3 vezes 100. Isso é recompensa perdida porque o estado 1 chegou ao estado terminal, e isso acaba sendo um retorno de 12,5. Em aplicações financeiras, o fator de desconto também tem uma interpretação muito natural como a taxa de juros ou o valor temporal do dinheiro. Se você puder ter um dólar hoje, isso pode valer um pouco mais do que se você pudesse obter apenas um dólar no futuro. Porque até mesmo um dólar hoje você pode colocar no banco, ganhar juros e acabar com um pouco mais de dinheiro daqui a um ano. Para aplicações financeiras, geralmente, esse fator de desconto representa quanto menos é um dólar no futuro, em comparação com um dólar hoje. Vejamos alguns exemplos concretos de devoluções. O retorno que você recebe depende das recompensas, e as recompensas dependem das ações que você realiza e, portanto, o retorno depende das ações que você realiza. Vamos usar nosso exemplo usual e dizer que, nesse exemplo, eu sempre vou para a esquerda. Já vimos anteriormente que, se o robô começasse no estado 4, o retorno seria de 12,5, conforme calculamos no slide anterior. Acontece que, se começasse em, digamos, três, o retorno seria de 25%, porque ele recebe a recompensa de 100 um passo antes e, portanto, tem um desconto menor. Se começasse no estado 2, o retorno seria 50. Se fosse apenas começar e declarar 1, bem, ele receberá a recompensa de 100 imediatamente, então não tem um desconto mínimo. O retorno se começarmos no estado 1 será 100 e, em seguida, o retorno nesses dois estados será 6,25. Acontece que se você começar no estado 6, que é o estado terminal, você só recebe a recompensa e, portanto, o retorno de 40. Agora, se você tomasse um conjunto diferente de ações, os retornos seriam, na verdade, diferentes. Por exemplo, se fôssemos sempre para a direita, se essas fossem nossas ações , se você começasse no estado 4, ganharia uma recompensa de 0. Então você chega ao estado 5, recebe uma recompensa de 0, ele chega ao estado 6 e recebe uma recompensa de 40. Nesse caso, o retorno seria 0 mais 0,5, o fator de desconto multiplicado por 0 mais 0,5 quadrado vezes 40, e isso seria igual a 0,5 quadrado é 1/4, então 1/4 de 40 é 10. O retorno desse estado, do estado 4, é 10. Se você fosse agir, sempre vá para a direita. Por meio de raciocínio semelhante, o retorno desse estado é 20, o retorno desse estado é cinco, o retorno desse estado é 2,5 e, em seguida, o retorno, o estado determinante é 140. A propósito, se esses números não fizerem todo o sentido, fique à vontade para pausar o vídeo e verificar a matemática e ver se você consegue se convencer de que esses são os valores apropriados para o retorno. Pois se você partir de estados diferentes e se você sempre fosse para a direita. Vemos que sempre iria para a direita. O retorno que você espera obter é menor para a maioria dos estados. Talvez ir sempre para a direita não seja uma ideia tão boa quanto sempre ir para a esquerda. Mas acontece que nem sempre precisamos ir para a esquerda, sempre para a direita. Também podemos decidir se você está no estado 2, vá para a esquerda. Se você estiver no estado 3, vá para a esquerda. Se você estiver no estado 4, vá para a esquerda. Mas se você está no estado 5, então você está muito perto dessa recompensa. Vamos para a direita. Essa será uma maneira diferente de escolher ações a serem tomadas com base no estado em que você está. Acontece que o retorno que você receberá dos diferentes estados será 100, 50, 25, 12,5, 20 e 40. Só para ilustrar um caso. Se você começasse no estado 5, aqui você iria para a direita e, portanto, as recompensas que você obteria seriam zero primeiro no estado 5 e depois 4. O retorno é zero, a primeira recompensa, mais o fator de desconto, é 0,5 vezes 40, ou seja, 20, e é por isso que o retorno desse status é 20 se você realizar as ações mostradas aqui. Resumindo, o retorno no aprendizado por reforço é a soma das recompensas que o sistema recebe, ponderada pelo fator de desconto, enquanto as recompensas em um futuro distante são ponderadas pelo fator de desconto elevado a uma potência maior. Agora, isso realmente tem um efeito interessante quando você tem sistemas com recompensas negativas. No exemplo que examinamos, todas as recompensas foram zero ou positivas. Mas se houver alguma recompensa negativa , o fator de desconto na verdade incentiva o sistema a enviar as recompensas negativas o mais longe possível no futuro. Tomando um exemplo financeiro, se você tivesse que pagar $10 a alguém, talvez isso fosse uma recompensa negativa de menos 10. Mas se você pudesse adiar o pagamento por alguns anos , na verdade estaria melhor porque $10 daqui a alguns anos, porque a taxa de juros na verdade vale menos do que $10 que você teve que pagar hoje. Para sistemas com recompensas negativas, isso faz com que o algoritmo tente fazer as recompensas o mais longe possível no futuro. Para aplicativos financeiros e para outros aplicativos, isso realmente é a coisa certa para o sistema fazer. Agora você sabe qual é o retorno do aprendizado por reforço. Vamos para o próximo vídeo para formalizar a meta do algoritmo de aprendizado por reforço.
