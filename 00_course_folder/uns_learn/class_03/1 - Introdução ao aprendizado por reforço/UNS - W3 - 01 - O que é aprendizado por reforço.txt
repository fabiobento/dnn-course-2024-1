Bem-vindo à última semana da especialização em aprendizado de máquina. É um pouco amargo para mim que estejamos chegando ao fim dessa especialização, mas estou ansioso por esta semana, compartilhando com vocês algumas ideias interessantes sobre o aprendizado por reforço. No aprendizado de máquina, o aprendizado por reforço é uma daquelas ideias que, embora ainda não seja amplamente aplicada em aplicativos comerciais, é um dos pilares do aprendizado de máquina. E tem muitas pesquisas interessantes que o apoiam e o aprimoram a cada dia. Então, vamos começar dando uma olhada no que é aprendizado por reforço. Vamos começar com um exemplo. Aqui está uma foto de um helicóptero autônomo. Na verdade, este é o helicóptero autônomo de Stanford, pesa 32 libras e está no meu escritório agora. Como muitos outros helicópteros autônomos, ele é instrumentado com um computador de bordo, GPS, acelerômetros e giroscópios e a bússola magnética, para que ele saiba onde está o tempo todo com bastante precisão. E se eu lhe desse as chaves desse helicóptero e pedisse que você escrevesse um programa para pilotá-lo, como você faria isso? Helicópteros controlados por rádio são controlados com joysticks como esses e , portanto, a tarefa é dez vezes por segundo: você recebe a posição, orientação, velocidade e assim por diante do helicóptero. E você precisa decidir como mover esses dois manípulos de controle para manter o helicóptero equilibrado no ar. A propósito, eu mesmo pilotei helicópteros controlados por rádio, bem como drones de rotor quádruplo. E helicópteros controlados por rádio são, na verdade, um pouco mais difíceis de pilotar, um pouco mais difíceis de manter o equilíbrio no ar. Então, como você escreve um programa para fazer isso automaticamente? Deixe-me mostrar um vídeo divertido de algo que temos que fazer em um helicóptero autônomo de Stanford. Aqui está um vídeo dele voando sob o controle de um algoritmo de aprendizado por reforço. E deixe-me reproduzir o vídeo. Na verdade, eu era o cinegrafista naquele dia e este é o helicóptero voando no controle do computador. Se eu diminuir o zoom do vídeo, você vê as árvores plantadas no céu. Então, usando o aprendizado por reforço, fizemos com que esse helicóptero aprendesse a voar de cabeça para baixo. Dissemos que ele voasse de cabeça para baixo. Portanto, o aprendizado reforçado tem sido usado para fazer helicópteros realizarem uma ampla variedade de acrobacias, ou as chamamos de manobras acrobáticas. A propósito, se você estiver interessado em ver outros vídeos, você também pode conferi-los neste URL. Então, como fazer um helicóptero voar sozinho usando o aprendizado por reforço? A tarefa é dada à posição do helicóptero para decidir como mover os manípulos de controle. No aprendizado por reforço, chamamos a posição, orientação, velocidade e assim por diante do helicóptero de estado s. Então, a tarefa é encontrar uma função que mapeie o estado do helicóptero até uma ação a, ou seja, até onde empurrar as duas alavancas de controle para manter o helicóptero equilibrado no ar, voando e sem cair. Uma maneira de resolver esse problema é usar o aprendizado supervisionado. Acontece que essa não é uma boa abordagem para voar de helicóptero autônomo. Mas você poderia dizer, bem, se pudéssemos obter um monte de observações de estados e talvez um piloto humano especialista nos dissesse qual é a melhor ação a ser tomada. Você poderia então treinar uma rede neural usando aprendizado supervisionado para aprender diretamente o mapeamento dos estados s que estou chamando de x aqui, para uma ação que estou chamando de rótulo y aqui. Mas acontece que quando o helicóptero está se movendo pelo ar é, na verdade, muito ambíguo, qual é a ação correta a ser tomada. Você se inclina um pouco para a esquerda ou muito mais para a esquerda ou aumenta um pouco ou muito o estresse do helicóptero? Na verdade, é muito difícil obter um conjunto de dados de x e a ação ideal y. É por isso que, para muitas tarefas de controlar um robô, como um helicóptero e outros robôs, a abordagem de aprendizado supervisionado não funciona bem e, em vez disso, usamos o aprendizado por reforço. Agora, uma entrada fundamental para o aprendizado por reforço é algo chamado recompensa ou função de recompensa, que informa ao helicóptero quando está indo bem e quando está mal. Então, a maneira como eu gosto de pensar na função de recompensa é um pouco como treinar um cachorro. Quando eu estava crescendo, minha família tinha um cachorro e era meu trabalho treinar o cachorro ou o filhote para se comportar. Então, como fazer um filhote se comportar bem? Bem, você não pode demonstrar muito para o cachorrinho. Em vez disso, você o deixa fazer seu trabalho e sempre que faz algo bom, você vai, bom cachorro. E sempre que eles faziam algo ruim, você diz, cachorro mau. E então, espero que ele aprenda sozinho a fazer mais coisas com cães bons e menos com cães maus. Então, treinar com o algoritmo de aprendizado por reforço é assim. Quando o helicóptero está voando bem, você vai, bom helicóptero e se ele faz algo ruim, como colidir, você vai, helicóptero ruim. E então, o trabalho do algoritmo de aprendizado por reforço é descobrir como obter mais resultados de um helicóptero bom e menos de um helicóptero ruim. Uma maneira de pensar por que o aprendizado por reforço é tão poderoso é dizer a ele o que fazer e não como fazê-lo. E especificar a função de recompensa em vez da ação ideal oferece muito mais flexibilidade na forma como você projeta o sistema. Concretamente, por pilotar o helicóptero, sempre que ele estiver voando bem, você pode dar a ele uma recompensa de mais um a cada segundo em que ele estiver voando bem. E talvez, sempre que estiver voando mal, você lhe dê uma recompensa negativa ou, se alguma vez cair, você possa dar a ele uma recompensa negativa muito grande, como menos 1.000. Então, isso incentivaria o helicóptero a passar muito mais tempo voando bem e, com sorte, nunca cair. Mas aqui está outro vídeo divertido. Eu usei a analogia do cachorro bom e do cachorro mau para o aprendizado por reforço por muitos anos. E então, um dia, consegui colocar minhas mãos em um cão robótico e pude realmente usar essa metodologia de aprendizado por reforço de cães bons e cães maus para treinar um cão-robô a superar obstáculos. Então, este é um vídeo de um cão-robô que, usando o aprendizado por reforço, que o recompensa, movendo-se para a esquerda da tela, aprendeu a posicionar os pés com cuidado ou a escalar uma variedade de obstáculos. E se você pensar no que é preciso para programar um cachorro assim, não tenho ideia, realmente não sei como dizer a ele qual é a melhor maneira de posicionar as pernas para superar um determinado obstáculo. Todas essas coisas foram descobertas automaticamente pelo robô apenas dando recompensas que o incentivam, progredindo em direção à meta à esquerda da tela. Hoje, o aprendizado por reforço tem sido aplicado com sucesso a uma variedade de aplicações, desde o controle de robôs. E, de fato, no final desta semana, no laboratório prático, você implementará para si mesmo um algoritmo de aprendizado por reforço para pousar uma sonda lunar em simulação. Também foi usado para otimização de fábrica. Como você reorganiza as coisas na fábrica para maximizar a produtividade e a eficiência, bem como a negociação financeira de ações. Por exemplo, um dos meus amigos estava trabalhando na execução eficiente de ações. Portanto, se você decidir vender um milhão de ações nos próximos dias, talvez não queira despejar um milhão de ações no mercado de ações repentinamente, porque isso moverá os preços contra você. Então, qual é a melhor maneira de sequenciar suas negociações ao longo do tempo para que você possa vender as ações que deseja vender e, com sorte, obter o melhor preço possível por elas? Finalmente, também existem muitas aplicações de aprendizado por reforço e jogos, desde damas até xadrez e o jogo de cartas de bridge to go, bem como para jogar muitos videogames. Então, isso é aprendizado por reforço. Embora o aprendizado por reforço não seja usado tanto quanto o aprendizado supervisionado, ele ainda é usado em algumas aplicações hoje. E a ideia principal é que, em vez de você precisar dizer ao algoritmo qual é a saída correta y para cada entrada, tudo o que você precisa fazer é especificar uma função de recompensa que informe quando está indo bem e quando está indo mal. E o trabalho do algoritmo é descobrir automaticamente como escolher boas ações. Com isso, vamos agora para o próximo vídeo, onde formalizaremos o problema do aprendizado por reforço e também começaremos a desenvolver algoritmos para escolher automaticamente boas ações.
Obrigatória
pt-BR
​


