Desenvolvemos um formalismo de aprendizado por reforço usando o exemplo do rover marciano de seis estados. Vamos fazer uma rápida revisão dos principais conceitos e também ver como esse conjunto de conceitos também pode ser usado para outros aplicativos. Alguns dos conceitos que discutimos são estados de um problema de aprendizado por reforço, o conjunto de ações, as recompensas, um fator de desconto, a forma como as recompensas e o fator de desconto são usados em conjunto para calcular o retorno e, finalmente, uma política cujo trabalho é ajudá-lo a escolher ações para maximizar o retorno. Para o exemplo do rover de Marte, tínhamos seis estados numerados de 1 a 6 e as ações eram ir para a esquerda ou para a direita. As recompensas eram 100 para o estado mais à esquerda, 40 para o estado mais à direita e zero no meio, e eu estava usando um fator de desconto de 0,5. O retorno foi dado por essa fórmula e poderíamos ter políticas diferentes. Pi descreva ações dependendo do estado em que você está. Esse mesmo formalismo ou estados, ações , recompensas e assim por diante também pode ser usado para muitas outras aplicações. Resolva o problema ou encontre um helicóptero autônomo. Definir um estado seria o conjunto de posições, orientações e velocidades possíveis e assim por diante do helicóptero. As ações possíveis seriam o conjunto de maneiras possíveis de mover a alavanca de controle de um helicóptero, e as recompensas podem ser uma vantagem se ele estiver voando bem e menos 1.000 se ele não cair muito mal ou cair. Função de recompensa que indica o quão bem o helicóptero está voando. O fator de desconto, um número um pouco menor que um, talvez, 0,99 e, com base nas recompensas e no fator de desconto, você calcula o retorno usando a mesma fórmula. O trabalho de um algoritmo de aprendizado por reforço seria encontrar alguma política Pi de s para que, dada como entrada, a posição do helicóptero s, ele lhe diga qual ação tomar. Ou seja, mostra como mover os manípulos de controle. Aqui está mais um exemplo. Aqui está um jogo para jogar. Digamos que você queira usar o aprendizado por reforço para aprender a jogar xadrez. O estado desse problema seria a posição de todas as peças no tabuleiro. A propósito, se você joga xadrez e conhece bem as regras, sei que um pouco mais de informação do que apenas a posição das peças é importante para o xadrez, mas vou simplificar um pouco para este vídeo. As ações são as possíveis jogadas legais no jogo, e então uma escolha comum de recompensa seria dar ao seu sistema uma recompensa de mais um se ele vencer um jogo, menos um se perder o jogo e uma recompensa de zero se empatar um jogo. Para o xadrez, geralmente um fator de desconto muito próximo a um será usado, então talvez 0,99 ou mesmo 0,995 ou 0,999 e o retorno use a mesma fórmula dos outros aplicativos. Mais uma vez, a meta é atribuir uma posição no conselho para escolher uma boa ação usando uma política Pi. Esse formalismo de um aplicativo de aprendizado por reforço na verdade tem um nome. É chamado de processo de decisão de Markov, e eu sei que soa como um termo técnico muito complicado. Mas se você já ouviu esse termo processo de decisão de Markov ou MDP, para abreviar, esse é apenas o formalismo sobre o qual falamos nos últimos vídeos. O termo Markov no processo de decisão do MDP ou Markov se refere ao fato de que o futuro depende apenas do estado atual e não de nada que possa ter ocorrido antes de chegar ao estado atual. Em outras palavras, em um processo de decisão de Markov, o futuro depende apenas de onde você está agora, não de como você chegou aqui. Outra maneira de pensar no formalismo do processo de decisão de Markov é que temos um robô ou algum outro agente que desejamos controlar e o que podemos fazer é escolher ações e, com base nessas ações, algo acontecerá no mundo ou no meio ambiente, como nossa posição no mundo mudar ou conseguirmos amostrar um pedaço de rocha e executar a missão científica. A maneira como escolhemos a ação a é com uma política Pi e, com base no que acontece no mundo , podemos ver ou observar em que estado estamos, bem como quais recompensas recebemos. Às vezes, você vê diferentes autores usarem um diagrama como esse para representar o processo de decisão de Markov ou o formalismo do MDP, mas essa é apenas outra maneira de ilustrar o conjunto de conceitos que você aprendeu nos últimos vídeos. Agora você sabe como funciona um problema de aprendizado por reforço. No próximo vídeo, começaremos a desenvolver um algoritmo para escolher boas ações. O primeiro passo para isso será definir e, eventualmente, aprender a calcular a função de valor da ação estadual. Isso acaba sendo uma das principais quantidades para quando queremos desenvolver um algoritmo de aprendizado. Vamos ao próximo vídeo para ver o que é isso, função de valor de ação de estado.
