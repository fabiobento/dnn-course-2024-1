Neste vídeo, veremos como você pode usar a biblioteca scikit-learn para implementar o PCA. Essas são as etapas principais. Primeiro, se seus recursos assumirem faixas de valores muito diferentes, você poderá realizar o pré-processamento para escalar as feições para assumir faixas de valores comparáveis. Se você estivesse analisando as características de diferentes países, essas características assumem faixas de valores muito diferentes. O PIB pode estar em trilhões de dólares, enquanto outras características são inferiores a 100. O dimensionamento de recursos em aplicativos como esse seria importante para ajudar o PCA a encontrar uma boa escolha de eixos para você. A próxima etapa, então, é executar o algoritmo PCA para “ajustar” os dados para obter dois ou três novos eixos, Z_1, Z_2 e talvez Z_3. Aqui, suponho que você queira dois ou três eixos se quiser visualizar os dados em 2D ou 3D. Se você tem um aplicativo em que deseja mais de dois ou três eixos, a implementação do PCA também pode fornecer mais de dois ou três eixos, mas seria mais difícil de visualizar. No scikit-learn, você usará a função de ajuste ou o método de ajuste para fazer isso. A função de ajuste no PCA realiza automaticamente a normalização média, ela subtrai a média de cada recurso. Portanto, você não precisa realizar separadamente a normalização média.
Reproduza o vídeo começando em :1:40 e siga a transcrição1:40
Depois de executar a função de ajuste, você obteria os novos eixos, Z_1, Z_2, talvez Z_3, e no PCA, também os chamamos de componentes principais, onde Z_1 é o primeiro componente principal, Z_2 o segundo componente principal e Z_3 o terceiro componente principal. Depois disso, eu recomendaria dar uma olhada em quanto cada um desses novos eixos, ou cada um desses novos componentes principais, explica a variação em seus dados. Mostrarei um exemplo concreto do que isso significa no próximo slide, mas isso permite que você tenha uma ideia se a projeção dos dados nesses eixos ajuda ou não a reter a maior parte da variabilidade ou a maioria das informações no conjunto de dados original. Isso é feito usando a função de razão de variância explicada. Finalmente, você pode transformar, ou seja, apenas projetar os dados nos novos eixos, nos novos componentes principais, o que você fará com o método de transformação. Então, para cada exemplo de treinamento, você teria apenas dois ou três números. Em seguida, pode representar graficamente esses dois ou três números para visualizar seus dados. Em detalhes, é assim que o PCA no código se parece. Aqui está o conjunto de dados X com seis exemplos. X é igual à matriz NumPy, os seis exemplos aqui.
Reproduza o vídeo começando em :3:7 e siga a transcrição3:07
Para executar o PCA para reduzir esses dados de dois números, X_1, X_2 para apenas um número Z, você executaria o PCA e solicitaria que ele se encaixasse em um componente principal.
Reproduza o vídeo começando em :3:19 e siga a transcrição3:19
N componentes aqui são iguais a um e ajustam PCA a X. Pca_1 aqui está minha notação para PCA com um único componente principal, com um único eixo. Acontece que, se você imprimisse pca_1.explained_variance_ratio, isso seria 0,992. Isso indica que, neste exemplo, quando você escolhe um eixo, ele captura 99,2% da variabilidade ou das informações no conjunto de dados original. Finalmente, se você quiser pegar cada uma dessas amostras de treinamento e projetá-las em um único número, ligue para isso, e isso resultará em uma matriz com seis números correspondentes aos seus seis exemplos de treinamento. Por exemplo, o primeiro exemplo de treinamento 1,1, projetado no eixo Z, fornece esse número, 1,383, e assim por diante. Então, se você visualizasse esse conjunto de dados usando apenas uma dimensão, esse seria o número que eu usaria para representar o primeiro exemplo. O segundo exemplo é projetado para ser esse número e assim por diante. Espero que você dê uma olhada no laboratório opcional, onde você vê que esses seis exemplos foram projetados para baixo neste eixo, nesta linha que agora é Y. Todos os seis exemplos agora estão nesta linha que se parece com esta. O primeiro exemplo de treinamento, que era 1,1, foi mapeado para este exemplo, que tem uma distância de 1,38 da origem, então é por isso que isso é 1,38. Só mais um exemplo rápido. Esses dados são dados bidimensionais e nós os reduzimos para uma dimensão. E se você computasse dois componentes principais? Começa com duas dimensões e depois também termina com duas dimensões. Isso não é muito útil para visualização, mas pode nos ajudar a entender melhor como o PCA e como eles codificam para o PCA funcionam. Aqui está o mesmo código, exceto que eu mudei n componentes para dois. Vou pedir ao algoritmo que encontre dois componentes principais. Se você fizer isso, a proporção de explicação de pca_2 se tornará 0,992, 0,008.
Reproduza o vídeo começando em :5:49 e siga a transcrição5:49
O que isso significa é que z_1, os primeiros componentes do princípio, ainda contínuos, explicam 99,2% da variância, Z_2, os segundos componentes do princípio, ou o segundo eixo, explica 0,8% da variância. Esses dois números juntos somam um. Porque, embora esses dados sejam bidimensionais, os dois eixos, Z_1 e Z_2, juntos explicam 100% da variância nos dados.
Reproduza o vídeo começando em :6:16 e siga a transcrição6:16
Se você transformasse os dados do nosso projeto nos eixos Z_1 e Z_2, isso é o que você obtém. Agora, o primeiro exemplo de treinamento é vinculado a esses dois números, correspondendo à sua projeção em z_1 e z_2, e o segundo exemplo, que é projetado em z_1 e z_2, se torna esses dois números.
Reproduza o vídeo começando em :6:43 e siga a transcrição6:43
Se você fosse reconstruir os dados originais aproximadamente, isso é z_1, e esse z_2, então o primeiro exemplo de treinamento que foi a [1, 1] tem uma distância de 1,38 no eixo z_1, tem esse número e a distância aqui de 0,29, portanto, essa distância no eixo z_2, e a reconstrução realmente parece exatamente igual aos dados originais. Porque, se você reduzir ou não realmente reduzir dados bidimensionais em dados bidimensionais, não há aproximação e você pode voltar ao seu conjunto de dados original, com as projeções em z_1 e z_2. Essa é a aparência do código para executar o PCA. Espero que você dê uma olhada no laboratório opcional, onde você mesmo pode brincar mais com isso. Além disso, tente variar os parâmetros, veja um exemplo específico para aprofundar sua intuição sobre como o PCA funciona. Antes de terminar, gostaria de compartilhar alguns conselhos para aplicar o PCA. O PCA é frequentemente usado para visualização, na qual você reduz os dados para dois ou três números para poder plotá-los. Como você viu em um vídeo anterior com os dados de diferentes países para que você possa visualizar diferentes países. Existem alguns outros aplicativos do PCA sobre os quais você pode ouvir falar ocasionalmente. Isso costumava ser mais popular, talvez 10,15, 20 anos atrás, mas muito menos agora. Outro uso possível do PCA é a compressão de dados. Por exemplo, se você tem um banco de dados de vários carros diferentes e tem 50 recursos por carro, mas está ocupando muito espaço no seu banco de dados ou talvez transmitindo 50 números pela Internet, isso leva muito tempo. Então, uma coisa que você pode fazer é reduzir esses 50 recursos para um número menor de recursos. Podem ser 10 recursos com 10 eixos ou 10 componentes principais. Você pode visualizar dados em 10 dimensões com facilidade, mas isso representa 1/5 do espaço de armazenamento, ou talvez 1/5 dos custos de transmissão de rede necessários. Há muitos anos, vi o PCA ser usado para esse aplicativo e com mais frequência, mas hoje, com o armazenamento moderno, é capaz de armazenar conjuntos de dados muito grandes e redes modernas, capaz de transmitir mais rápido e mais dados do que nunca. Eu vejo esse uso com muito menos frequência como uma aplicação do PCA. Uma das aplicações do PCA que mais uma vez costumava ser mais comum talvez 10 anos atrás, 20 anos atrás, mas muito menos agora é usá-lo para acelerar o treinamento de um modelo de aprendizado supervisionado. Onde está a ideia, se você tivesse 1.000 recursos e 1.000 recursos, talvez o algoritmo de aprendizado supervisionado o deixasse lento. Talvez você possa reduzi-lo para 100 recursos usando o PCA e, em seguida, seu conjunto de dados seja basicamente menor e seu algoritmo de aprendizado supervisionado possa ser executado mais rápido. Isso costumava fazer a diferença no tempo de execução de algumas das gerações mais antigas de algoritmos de aprendizado, como se você tivesse máquinas vetoriais de suporte. Isso acelerará uma máquina vetorial de suporte. Mas acontece que com algoritmos modernos de aprendizado de máquina, algoritmos como o aprendizado profundo, isso não vale muito, e é muito mais comum simplesmente pegar o conjunto de dados de alta dimensão e inseri-lo, digamos, em sua rede neural. Em vez de executar o PCA porque o PCA também tem algum custo computacional. Você pode ouvir sobre isso em alguns dos outros trabalhos de pesquisa, mas eu realmente não vejo mais isso acontecer. Mas a coisa mais comum para a qual eu uso o PCA hoje é a visualização e, então, acho muito útil reduzir os dados dimensionais para visualizá-los. Obrigado por ficar comigo até o final dos vídeos opcionais desta semana. Espero que você goste de aprender sobre o PCA e que ache útil obter um novo conjunto de dados para reduzir a dimensão do conjunto de dados para duas ou três dimensões, para que você possa visualizá-lo e, com sorte, obter novos insights sobre seus conjuntos de dados. Isso me ajudou muitas vezes a entender meus próprios conjuntos de dados e espero que você também os ache igualmente úteis. Obrigado por assistir esses vídeos e espero ver você na próxima semana.
