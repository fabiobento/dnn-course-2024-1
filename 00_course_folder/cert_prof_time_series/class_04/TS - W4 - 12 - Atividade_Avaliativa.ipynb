{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/cert_prof_time_series/class_04/TS%20-%20W4%20-%2012%20-%20Atividade_Avaliativa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adaptado de [Certificado Profissional Desenvolvedor do TensorFlow](https://www.coursera.org/professional-certificates/tensorflow-in-practice) de [Laurence Moroney](https://laurencemoroney.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando dados do mundo real\n",
    "\n",
    "Bem-vindo! Até agora, você trabalhou exclusivamente com dados sintéticos nas atividades avaliativas.\n",
    "\n",
    "Desta vez, você usará o conjunto de dados [Daily Minimum Temperatures in Melbourne](https://github.com/jbrownlee/Datasets/blob/master/daily-min-temperatures.csv), que contém dados das temperaturas mínimas diárias registradas em Melbourne de 1981 a 1990.\n",
    "\n",
    "Além de usar as camadas do Tensorflow para processar dados de sequência, como as camadas recorrentes ou LSTMs, você também usará as camadas convolucionais para melhorar o desempenho do modelo.\n",
    "\n",
    "Vamos começar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar arquivos adicionais para o laboratório\n",
    "!wget https://github.com/fabiobento/dnn-course-2024-1/raw/main/00_course_folder/cert_prof_time_series/class_04/data.zip\n",
    "!unzip -n -q data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56XEQOGknrAk",
    "outputId": "6b8ab5ea-ed49-40d2-a27c-d2ba08710ba0"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comece examinando a estrutura do arquivo csv que contém os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURES_CSV = './data/daily-min-temperatures.csv'\n",
    "\n",
    "with open(TEMPERATURES_CSV, 'r') as csvfile:\n",
    "    print(f\"O cabeçalho tem a seguinte aparência:\\n\\n{csvfile.readline()}\")    \n",
    "    print(f\"O primeiro ponto de dados tem a seguinte aparências:\\n\\n{csvfile.readline()}\")\n",
    "    print(f\"O segundo ponto de dados tem a seguinte aparência:\\n\\n{csvfile.readline()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, cada ponto de dados é composto pela data e pela temperatura mínima registrada para essa data.\n",
    "\n",
    "\n",
    "No primeiro exercício, você codificará uma função para ler os dados do csv, mas, por enquanto, execute a próxima célula para carregar uma função auxiliar para plotar a série temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLl52leVp5wU"
   },
   "outputs": [],
   "source": [
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Tempo\")\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando os dados brutos\n",
    "Agora você precisa ler os dados do arquivo csv. Para isso, complete a função `parse_data_from_file`.\n",
    "\n",
    "Alguns aspectos a serem observados:\n",
    "\n",
    "- Você deve omitir a primeira linha, pois o arquivo contém cabeçalhos.\n",
    "- Não há necessidade de salvar os pontos de dados como matrizes numpy; listas regulares são suficientes.\n",
    "- Para ler a partir de arquivos csv, use `csv.reader` passando os argumentos apropriados.\n",
    "- O `csv.reader` retorna um iterável que retorna cada linha em cada iteração. Portanto, a temperatura pode ser acessada por meio de row[1] e a data pode ser descartada.\n",
    "- A lista `times` deve conter cada intervalo de tempo (começando em zero), que é apenas uma sequência de números ordenados com o mesmo comprimento da lista `temperatures`.\n",
    "- Os valores de `temperaturas` devem ser do tipo `float`. Você pode usar a função `float` integrada do Python para garantir isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "NcG9r1eClbTh",
    "outputId": "7acf6ba9-e852-4f06-e06e-b0ff1b9e2ddd"
   },
   "outputs": [],
   "source": [
    "def parse_data_from_file(filename):\n",
    "    \n",
    "    times = []\n",
    "    temperatures = []\n",
    "\n",
    "    with open(filename) as csvfile:\n",
    "        \n",
    "        ### INICIE SEU CÓDGO AQUI\n",
    "        \n",
    "        reader = csv.reader(None, delimiter=None)\n",
    "        \n",
    "        ### TERMINE SEU CÓDGO AQUI\n",
    "            \n",
    "    return times, temperatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A próxima célula usará sua função para calcular `times` e `temperatures` e os salvará como matrizes numpy dentro da classe de dados `G`. Essa célula também plotará a série temporal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste sua função e salve todas as variáveis \"globais\" na classe G (G significa global)\n",
    "@dataclass\n",
    "class G:\n",
    "    TEMPERATURES_CSV = './data/daily-min-temperatures.csv'\n",
    "    times, temperatures = parse_data_from_file(TEMPERATURES_CSV)\n",
    "    TIME = np.array(times)\n",
    "    SERIES = np.array(temperatures)\n",
    "    SPLIT_TIME = 2500\n",
    "    WINDOW_SIZE = 64\n",
    "    BATCH_SIZE = 32\n",
    "    SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(G.TIME, G.SERIES)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada:**\n",
    "<div>\n",
    "<img src=\"images/temp-series.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos dados\n",
    "\n",
    "Como você já codificou as funções `train_val_split` e `windowed_dataset` durante as tarefas da semana passada, desta vez elas são fornecidas para você:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(time, series, time_step=G.SPLIT_TIME):\n",
    "\n",
    "    time_train = time[:time_step]\n",
    "    series_train = series[:time_step]\n",
    "    time_valid = time[time_step:]\n",
    "    series_valid = series[time_step:]\n",
    "\n",
    "    return time_train, series_train, time_valid, series_valid\n",
    "\n",
    "\n",
    "# Dividir o conjunto de dados\n",
    "time_train, series_train, time_valid, series_valid = train_val_split(G.TIME, G.SERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJwUUZscnG38"
   },
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE, shuffle_buffer=G.SHUFFLE_BUFFER_SIZE):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Aplicar a transformação ao conjunto de treinamento\n",
    "train_set = windowed_dataset(series_train, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE, shuffle_buffer=G.SHUFFLE_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição da arquitetura do modelo\n",
    "\n",
    "Agora que você tem uma função que processará os dados antes que eles sejam inseridos na rede neural para treinamento, é hora de definir a arquitetura da camada. Assim como na tarefa da semana passada, você fará a definição e a compilação da camada em duas etapas separadas. Comece completando a função `create_uncompiled_model` abaixo.\n",
    "\n",
    "Isso é feito para que você possa reutilizar as camadas do seu modelo para o ajuste da taxa de aprendizado e o treinamento real.\n",
    "\n",
    "Dica:\n",
    "\n",
    "- As camadas `Lambda` não são necessárias.\n",
    "- Use uma combinação de camadas `Conv1D` e `LSTM` seguidas de camadas `Dense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uncompiled_model():\n",
    "\n",
    "    ### INICIE SEU CÓDGO AQUI\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "    ]) \n",
    "    \n",
    "    ### TERMINE SEU CÓDGO AQUI\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste seu modelo não compilado\n",
    "uncompiled_model = create_uncompiled_model()\n",
    "\n",
    "try:\n",
    "    uncompiled_model.predict(train_set)\n",
    "except:\n",
    "    print(\"Sua arquitetura atual é incompatível com o conjunto de dados em janela, tente ajustá-la.\")\n",
    "else:\n",
    "    print(\"Sua arquitetura atual é compatível com o conjunto de dados com janelas! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste da taxa de aprendizado\n",
    "\n",
    "Como você viu na aula, é possível aproveitar os _callbacks_ do Tensorflow para variar dinamicamente a taxa de aprendizagem durante o treinamento. Isso pode ser útil para ter uma noção melhor de qual taxa de aprendizado se adapta melhor ao problema em questão.\n",
    "\n",
    "**Observe que isso só altera a taxa de aprendizagem durante o processo de treinamento para lhe dar uma ideia de qual é uma taxa de aprendizagem razoável e não deve ser confundido com a seleção da melhor taxa de aprendizagem, o que é conhecido como otimização de hiperparâmetros.**\n",
    "\n",
    "Para os otimizadores, você pode experimentar:\n",
    "\n",
    "- tf.keras.optimizers.Adam\n",
    "- tf.keras.optimizers.SGD com um momentum de 0,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(dataset):\n",
    "    \n",
    "    model = create_uncompiled_model()\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\n",
    "    \n",
    "    ### INICIE SEU CÓDIGO AQUI\n",
    "    \n",
    "    # Selecione seu otimizador\n",
    "    optimizer = None\n",
    "\n",
    "    # Compile o modelo passando a perda apropriadas\n",
    "    model.compile(loss=None,\n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"mae\"]) \n",
    "    \n",
    "    ### TERMINE SEU CÓDIGO AQUI\n",
    "    \n",
    "    history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o treinamento com LR dinâmico\n",
    "lr_history = adjust_learning_rate(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "vVcKmg7Q_7rD",
    "outputId": "27cf16ae-eb85-47c3-fc86-18e72e528619"
   },
   "outputs": [],
   "source": [
    "plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"loss\"])\n",
    "plt.axis([1e-4, 10, 0, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilação do modelo\n",
    "Agora que você treinou o modelo variando a taxa de aprendizado, é hora de fazer o treinamento real que será usado para prever a série temporal. Para isso, complete a função `create_model` abaixo.\n",
    "\n",
    "Observe que você está reutilizando a arquitetura definida anteriormente em `create_uncompiled_model`. Agora você só precisa compilar esse modelo usando a perda, o otimizador (e a taxa de aprendizado) apropriados.\n",
    "\n",
    "Dicas:\n",
    "\n",
    "- O treinamento deve ser muito rápido, portanto, se perceber que cada época está demorando mais do que alguns segundos, considere tentar uma arquitetura diferente.\n",
    "\n",
    "\n",
    "- Se, após a primeira época, você obtiver uma saída como esta: loss: nan - mae: nan, é muito provável que sua rede esteja sofrendo com a explosão de gradientes. Esse é um problema comum se você usou o SGD como otimizador e definiu uma taxa de aprendizado muito alta. Se você encontrar esse problema, considere reduzir a taxa de aprendizado ou usar o Adam com a taxa de aprendizado padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    \n",
    "    model = create_uncompiled_model()\n",
    "\n",
    "    ### INICIE SEU CÓDIGO AQUI\n",
    "\n",
    "    model.compile(loss=None,\n",
    "                  optimizer=None,\n",
    "                  metrics=[\"mae\"])  \n",
    "    \n",
    "\n",
    "    ### FINALIZE SEU CÓDIGO AQUI\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar uma instância do modelo\n",
    "model = create_model()\n",
    "\n",
    "# Treine-o\n",
    "history = model.fit(train_set, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação da previsão\n",
    "\n",
    "Agora é hora de avaliar o desempenho da previsão. Para isso, você pode usar a função `compute_metrics` que codificou em uma tarefa anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_series, forecast):\n",
    "    \n",
    "    mse = tf.keras.metrics.mean_squared_error(true_series, forecast).numpy()\n",
    "    mae = tf.keras.metrics.mean_absolute_error(true_series, forecast).numpy()\n",
    "\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste ponto, apenas o modelo que realizará a previsão está pronto, mas você ainda precisa calcular a previsão real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previsões de modelo mais rápidas\n",
    "\n",
    "Anteriormente, você viu uma abordagem mais rápida em comparação com o uso de um loop for para calcular as previsões para cada ponto da sequência. Lembre-se de que essa abordagem mais rápida usa lotes de dados. \n",
    "\n",
    "O código para implementar isso é fornecido no `model_forecast` abaixo. Observe que o código é muito semelhante ao da função `windowed_dataset` com a diferença de que:\n",
    "- O conjunto de dados é janelado usando `window_size` em vez de `window_size + 1`\n",
    "- Não deve ser usado shuffle\n",
    "- Não há necessidade de dividir os dados em recursos e rótulos\n",
    "- Um modelo é usado para prever lotes do conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XwGrf-A_wF0"
   },
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora calcule a previsão real:\n",
    "\n",
    "**Nota:** Não modifique a célula abaixo. \n",
    "\n",
    "O avaliador usa o mesmo fatiamento para obter a previsão, portanto, se você alterar a célula abaixo, poderá ter problemas ao enviar o modelo para avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "PrktQX3hKYex",
    "outputId": "1914662d-6bdd-4e17-8697-8f5a29e89b87"
   },
   "outputs": [],
   "source": [
    "# Calcular a previsão para todas as séries\n",
    "rnn_forecast = model_forecast(model, G.SERIES, G.WINDOW_SIZE).squeeze()\n",
    "\n",
    "# Corte a previsão para obter apenas as previsões para o conjunto de validação\n",
    "rnn_forecast = rnn_forecast[G.SPLIT_TIME - G.WINDOW_SIZE:-1]\n",
    "\n",
    "# Plotar a previsão\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, series_valid)\n",
    "plot_series(time_valid, rnn_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, mae = compute_metrics(series_valid, rnn_forecast)\n",
    "\n",
    "print(f\"mse: {mse:.2f}, mae: {mae:.2f} for forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para ser aprovado nesta tarefa, sua previsão deve atingir um MSE de 6 ou menos e um MAE de 2 ou menos.**\n",
    "\n",
    "- Se a sua previsão não atingir esse limite, tente treinar novamente o modelo com uma arquitetura diferente (será necessário executar novamente as funções `create_uncompiled_model` e `create_model`) ou ajustar os parâmetros do otimizador.\n",
    "\n",
    "\n",
    "- Se a sua previsão atingiu esse limite, execute a célula a seguir para salvar o modelo no formato SavedModel, que será usado para avaliação e, depois disso, envie sua tarefa para avaliação.\n",
    "\n",
    "\n",
    "- Esse ambiente inclui um diretório SavedModel fictício que contém um modelo fictício treinado para uma época. **Para substituir esse arquivo pelo seu modelo real, você precisa executar a próxima célula antes de enviá-lo para avaliação.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salve seu modelo no formato SavedModel\n",
    "model.save('saved_model/my_model')\n",
    "\n",
    "# Comprimir o diretório usando tar\n",
    "! tar -czvf saved_model.tar.gz saved_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parabéns por terminar a atividade avaliativa desse módulo!\n",
    "\n",
    "Você implementou com sucesso uma rede neural capaz de prever séries temporais aproveitando uma combinação de camadas do Tensorflow, como Convolutional e LSTMs! Isso resultou em uma previsão que supera todas as que você fez anteriormente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
