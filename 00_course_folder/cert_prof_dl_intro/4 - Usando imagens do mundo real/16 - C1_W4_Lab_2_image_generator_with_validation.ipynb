{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHBkAdENa5FY"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/cert_prof_dl_intro/4%20-%20Usando%20imagens%20do%20mundo%20real/16%20-%20C1_W4_Lab_2_image_generator_with_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adaptado de [Certificado Profissional Desenvolvedor do TensorFlow](https://www.coursera.org/professional-certificates/tensorflow-in-practice) de [Laurence Moroney](https://laurencemoroney.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB2cQUShkXNm"
   },
   "source": [
    "# ImageDataGenerator Com conjunto de Validação\n",
    "\n",
    "Neste laboratório, você continuará a usar a classe `ImageDataGenerator` para preparar o conjunto de dados `Horses or Humans`.\n",
    "\n",
    "Desta vez, você adicionará um conjunto de validação para que também possa medir o desempenho do modelo em dados que ele ainda não viu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5FfBGV5yUjb"
   },
   "source": [
    "Execute os blocos de código abaixo para fazer download dos conjuntos de dados `horse-or-human.zip` e `validation-horse-or-human.zip`, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXZT2UsyIVe_"
   },
   "outputs": [],
   "source": [
    "# Faça o download do conjunto de treinamento\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mLij6qde6Ox"
   },
   "outputs": [],
   "source": [
    "# Baixar o conjunto de validação\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9brUxyTpYZHy"
   },
   "source": [
    "Em seguida, descompacte os dois arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLy3pthUS0D2"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Descompactar o conjunto de treinamento\n",
    "local_zip = './horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./horse-or-human')\n",
    "\n",
    "# Descompactar o conjunto de validação\n",
    "local_zip = './validation-horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./validation-horse-or-human')\n",
    "\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-qUPyfO7Qr8"
   },
   "source": [
    "Da mesma forma que no laboratório anterior, você definirá os diretórios que contêm as imagens.\n",
    "\n",
    "Desta vez, você incluirá aqueles com dados de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR_M9nWN-K8B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Diretório com fotos de cavalos de treinamento\n",
    "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
    "\n",
    "# Diretório com imagens humanas de treinamento\n",
    "train_human_dir = os.path.join('./horse-or-human/humans')\n",
    "\n",
    "# Diretório com imagens de cavalos de validação\n",
    "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
    "\n",
    "# Diretório com imagens humanas de validação\n",
    "validation_human_dir = os.path.join('./validation-horse-or-human/humans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuBYtA_Zd8_T"
   },
   "source": [
    "Agora veja como são os nomes dos arquivos nesses diretórios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PIP1rkmeAYS"
   },
   "outputs": [],
   "source": [
    "train_horse_names = os.listdir(train_horse_dir)\n",
    "print(f'CONJUNTO DE TREINO, CAVALOS: {train_horse_names[:10]}')\n",
    "\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(f'CONJUNTO DE TREINO, HUMANOS: {train_human_names[:10]}')\n",
    "\n",
    "validation_horse_names = os.listdir(validation_horse_dir)\n",
    "print(f'CONJUNTO DE VALIDAÇÃO, CAVALOS: {validation_horse_names[:10]}')\n",
    "\n",
    "validation_human_names = os.listdir(validation_human_dir)\n",
    "print(f'CONJUNTO DE VALIDAÇÃO, HUMANOS: {validation_human_names[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlqN5KbafhLI"
   },
   "source": [
    "Você pode descobrir o número total de imagens de cavalos e humanos nos diretórios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4XHh2xSfgie"
   },
   "outputs": [],
   "source": [
    "print(f'total de imagens de treinamento de cavalos: {len(os.listdir(train_horse_dir))}')\n",
    "print(f'total de imagens de treinamento de humanos: {len(os.listdir(train_human_dir))}')\n",
    "print(f'total de imagens de validação de cavalos: {len(os.listdir(validation_horse_dir))}')\n",
    "print(f'total de imagens de validação de humanos: {len(os.listdir(validation_human_dir))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3WZABE9eX-8"
   },
   "source": [
    "Agora, dê uma olhada em algumas imagens para ter uma noção melhor de como elas são. Primeiro, configure os parâmetros do `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2_Q0-_5UAv-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parâmetros para nosso gráfico; produziremos imagens em uma configuração 4x4\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Índice para iteração de imagens\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTvHzGCxXkqp"
   },
   "source": [
    "Agora, exiba um lote de 8 imagens de cavalos e 8 imagens de humanos. Você pode executar novamente a célula para ver um novo lote a cada vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wpr8GxjOU8in"
   },
   "outputs": [],
   "source": [
    "# Configure o matplotlib fig e dimensione-o para caber em fotos 4x4\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
    "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
    "next_human_pix = [os.path.join(train_human_dir, fname) \n",
    "                for fname in train_human_names[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
    "  # Configure o subplot; os índices do subplot começam em 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Não mostre os eixos (ou linhas de grade)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oqBkNBJmtUv"
   },
   "source": [
    "## Criando um modelo pequeno a partir do zero\n",
    "\n",
    "Você definirá a mesma arquitetura de modelo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvfZg3LQbD-5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Observe que a forma de entrada é o tamanho desejado da imagem 300x300 com 3 bytes de cor\n",
    "    # Essa é a primeira convolução\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # A segunda convolução\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A terceira convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A quarta convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A quinta convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Achatar os resultados para alimentar uma DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Camada oculta de 512 neurônios\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Apenas 1 neurônio de saída. Ele conterá um valor de 0 a 1, sendo 0 para uma classe (\"cavalos\") e 1 para a outra (\"humanos\")\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9EaFDP5srBa"
   },
   "source": [
    "Você pode revisar a arquitetura da rede e as formas de saída com `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZKj8392nbgP"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEkKSpZlvJXA"
   },
   "source": [
    "Você também usará as mesmas configurações de compilação de antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DHWhFP_uhq3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn9m9D3UimHM"
   },
   "source": [
    "### Pré-processamento de dados\n",
    "\n",
    "Agora você configurará os geradores de dados.\n",
    "\n",
    "Em geral, serão parecidos com os anteriores, mas observe o código adicional para preparar também os dados de validação. \n",
    "\n",
    "Ele precisará ser instanciado separadamente e também dimensionado para ter um intervalo de `[0,1]` de valores de pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClebU9NJg99G"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Todas as imagens serão redimensionadas em 1,/255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Imagens de treinamento de fluxo em lotes de 128 usando o gerador train_datagen\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './horse-or-human/',  # Esse é o diretório de origem das imagens de treinamento\n",
    "        target_size=(300, 300),  # Todas as imagens serão redimensionadas para 300x300\n",
    "        batch_size=128,\n",
    "        # Como você usa a perda binary_crossentropy, precisa de rótulos binários\n",
    "        class_mode='binary')\n",
    "\n",
    "# Imagens de validação de fluxo em lotes de 128 usando o gerador validation_datagen\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        './validation-horse-or-human/',  # Esse é o diretório de origem das imagens de validação\n",
    "        target_size=(300, 300),  # Todas as imagens serão redimensionadas para 300x300\n",
    "        batch_size=32,\n",
    "        # Como você usa a perda binary_crossentropy, precisa de rótulos binários\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu3Jdwkjwax4"
   },
   "source": [
    "### Treinamento\n",
    "\n",
    "Agora, treine o modelo por 15 épocas. Aqui, você passará parâmetros para `validation_data` e `validation_steps`.\n",
    "\n",
    "Com eles, você notará saídas adicionais nas instruções de impressão: `val_loss` e `val_accuracy`.\n",
    "\n",
    "Observe que, à medida que você treina com mais épocas, a precisão do treinamento pode aumentar, mas a precisão da validação diminui.\n",
    "\n",
    "Isso pode ser um sinal de ajuste excessivo e você precisa evitar que seu modelo chegue a esse ponto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb1_lgobv81m"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=15,\n",
    "      verbose=1,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6vSHzPR2ghH"
   },
   "source": [
    "### Previsão do modelo\n",
    "\n",
    "Agora, dê uma olhada na execução de uma previsão usando o modelo. Esse código permitirá que você escolha um ou mais arquivos do seu sistema de arquivos, carregue-os e execute-os por meio do modelo, fornecendo uma indicação de que o objeto é um cavalo ou um humano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoWp43WxJDNT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import files\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  # Previsão de imagens\n",
    "  path = '/content/' + fn\n",
    "  img = load_img(path, target_size=(300, 300))\n",
    "  x = img_to_array(img)\n",
    "  x /= 255\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(classes[0])\n",
    "  if classes[0]>0.5:\n",
    "    print(fn + \" é um humano\")\n",
    "  else:\n",
    "    print(fn + \" é um cavalo\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8EHQyWGDvWz"
   },
   "source": [
    "### Visualizando representações intermediárias\n",
    "\n",
    "Como antes, você pode plotar como os recursos são transformados à medida que passam por cada camada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5tES8rXFjux"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# Defina um novo modelo que receberá uma imagem como entrada e produzirá\n",
    "# representações intermediárias para todas as camadas do modelo anterior após\n",
    "# a primeira.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Prepare uma imagem de entrada aleatória do conjunto de treinamento.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = load_img(img_path, target_size=(300, 300))  # esta é uma imagem PIL\n",
    "x = img_to_array(img)  # Matriz Numpy com formato (300, 300, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Matriz Numpy com formato (1, 300, 300, 3)\n",
    "\n",
    "# Escale em 1/255\n",
    "x /= 255\n",
    "\n",
    "# Execute a imagem na rede, obtendo assim todas as\n",
    "# representações intermediárias para essa imagem.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# Esses são os nomes das camadas, para que você possa tê-las como parte do gráfico\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Exibir as representações\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  if len(feature_map.shape) == 4:\n",
    "\n",
    "    # Faça isso apenas para as camadas conv / maxpool, não para as camadas totalmente conectadas\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "\n",
    "    # O mapa de recursos tem a forma (1, tamanho, tamanho, n_recursos)\n",
    "    size = feature_map.shape[1]\n",
    "    \n",
    "    # Colocar as imagens em mosaico nessa matriz\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "      x = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std()\n",
    "      x *= 64\n",
    "      x += 128\n",
    "      x = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "      # Colocar cada filtro em uma grade horizontal grande\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "    \n",
    "    # Exibir a grade\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4IBgYCYooGD"
   },
   "source": [
    "## Limpar\n",
    "\n",
    "Antes de executar o próximo exercício, execute a seguinte célula para encerrar o kernel e liberar recursos de memória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "651IgjLyo-Jx"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W4_Lab_2_image_generator_with_validation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
