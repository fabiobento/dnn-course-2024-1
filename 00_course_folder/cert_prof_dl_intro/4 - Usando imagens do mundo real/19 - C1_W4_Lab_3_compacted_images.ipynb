{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/cert_prof_dl_intro/4%20-%20Usando%20imagens%20do%20mundo%20real/19%20-%20C1_W4_Lab_3_compacted_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adaptado de [Certificado Profissional Desenvolvedor do TensorFlow](https://www.coursera.org/professional-certificates/tensorflow-in-practice) de [Laurence Moroney](https://laurencemoroney.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR8Am0lBtRAx"
   },
   "source": [
    "# Efeito de imagens compactadas no treinamento\n",
    "\n",
    "Neste notebook, você verá como a redução do tamanho alvo das imagens do gerador afetará a arquitetura e o desempenho do seu modelo. \n",
    "\n",
    "Essa é uma técnica útil caso você precise acelerar o treinamento ou economizar recursos de computação. Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxY7KvGQ2Qdr"
   },
   "source": [
    "Como antes, comece a fazer o download dos conjuntos de treinamento e validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXZT2UsyIVe_"
   },
   "outputs": [],
   "source": [
    "# Faça o download do conjunto de treinamento\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mLij6qde6Ox"
   },
   "outputs": [],
   "source": [
    "# Baixar o conjunto de validação\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9brUxyTpYZHy"
   },
   "source": [
    "Em seguida, descompacte-os:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLy3pthUS0D2"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Descompactar o conjunto de treinamento\n",
    "local_zip = './horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./horse-or-human')\n",
    "\n",
    "# Descompactar o conjunto de validação\n",
    "local_zip = './validation-horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./validation-horse-or-human')\n",
    "\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-qUPyfO7Qr8"
   },
   "source": [
    "Em seguida, defina os diretórios que contêm as imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR_M9nWN-K8B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Diretório com fotos de cavalos de treinamento\n",
    "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
    "\n",
    "# Diretório com imagens humanas de treinamento\n",
    "train_human_dir = os.path.join('./horse-or-human/humans')\n",
    "\n",
    "# Diretório com imagens de cavalos de validação\n",
    "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
    "\n",
    "# Diretório com imagens humanas de validação\n",
    "validation_human_dir = os.path.join('./validation-horse-or-human/humans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1wrZCxTPw4m"
   },
   "source": [
    "Você pode verificar se os diretórios não estão vazios e se o conjunto de trens tem mais imagens do que o conjunto de validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_horse_names = os.listdir(train_horse_dir)\n",
    "print(f'CONJUNTO DE TREINO, CAVALOS: {train_horse_names[:10]}')\n",
    "\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(f'CONJUNTO DE TREINO, HUMANOS: {train_human_names[:10]}')\n",
    "\n",
    "validation_horse_names = os.listdir(validation_horse_dir)\n",
    "print(f'CONJUNTO DE VALIDAÇÃO, CAVALOS: {validation_horse_names[:10]}')\n",
    "\n",
    "validation_human_names = os.listdir(validation_human_dir)\n",
    "print(f'CONJUNTO DE VALIDAÇÃO, HUMANOS: {validation_human_names[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTpdVrBg2LZC"
   },
   "outputs": [],
   "source": [
    "print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n",
    "print(f'total training human images: {len(os.listdir(train_human_dir))}')\n",
    "print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n",
    "print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oqBkNBJmtUv"
   },
   "source": [
    "## Construir o modelo\n",
    "\n",
    "O modelo seguirá a mesma arquitetura de antes, mas a principal diferença está no parâmetro `input_shape` da primeira camada `Conv2D`.\n",
    "\n",
    "Como você compactará as imagens posteriormente no gerador, é necessário especificar o tamanho esperado da imagem aqui.\n",
    "\n",
    "Portanto, em vez de 300x300 como nos dois laboratórios anteriores, você especifica uma matriz menor de 150x150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PixZ2s5QbYQ3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Observe que a forma de entrada é o tamanho desejado da imagem 150x150 com 3 bytes de cor\n",
    "    # Essa é a primeira convolução\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # A segunda convolução\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A terceira convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A quarta convolução (Você pode descomentar a 4ª e a 5ª camadas de convolução mais tarde para ver o efeito)\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A quinta convolução\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Achatar os resultados para alimentar um DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9EaFDP5srBa"
   },
   "source": [
    "Você pode ver a diferença em relação aos modelos anteriores ao imprimir o `model.summary()`.\n",
    "\n",
    "Como esperado, haverá menos entradas para a camada `Dense` no final do modelo em comparação com os laboratórios anteriores.\n",
    "\n",
    "Isso ocorre porque você usou o mesmo número de camadas de pooling máximo em seu modelo.\n",
    "\n",
    "E como você tem uma imagem menor para começar (150 x 150), a saída após todas as camadas de pooling também será menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZKj8392nbgP"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEkKSpZlvJXA"
   },
   "source": [
    "Você usará as mesmas configurações para o treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DHWhFP_uhq3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn9m9D3UimHM"
   },
   "source": [
    "### Pré-processamento de dados\n",
    "\n",
    "Agora você instanciará os geradores de dados. Como mencionado anteriormente, você compactará a imagem especificando o parâmetro `target_size`. Veja a alteração simples abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClebU9NJg99G"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Todas as imagens serão redimensionadas em 1,/255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Imagens de treinamento de fluxo em lotes de 128 usando o gerador train_datagen\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './horse-or-human/',  # Esse é o diretório de origem das imagens de treinamento\n",
    "        target_size=(150, 150),  # Todas as imagens serão redimensionadas para 150x150\n",
    "        batch_size=128,\n",
    "        # Como você usou a perda binary_crossentropy, você precisa de rótulos binários\n",
    "        class_mode='binary')\n",
    "\n",
    "# Imagens de treinamento de fluxo em lotes de 128 usando o gerador train_datagen\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        './validation-horse-or-human/',  # Esse é o diretório de origem das imagens de treinamento\n",
    "        target_size=(150, 150),  # Todas as imagens serão redimensionadas para 150x150\n",
    "        batch_size=32,\n",
    "        # Como você usou a perda binary_crossentropy, você precisa de rótulos binários\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu3Jdwkjwax4"
   },
   "source": [
    "### Treinamento\n",
    "\n",
    "Agora você está pronto para treinar e ver os resultados.\n",
    "\n",
    "Anote suas observações sobre a rapidez com que o modelo é treinado e a precisão obtida nos conjuntos de treinamento e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb1_lgobv81m"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=15,\n",
    "      verbose=1,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6vSHzPR2ghH"
   },
   "source": [
    "### Previsão do modelo\n",
    "\n",
    "Como de costume, também é uma boa prática tentar executar seu modelo em algumas imagens escolhidas a dedo. Veja se você obteve um desempenho melhor, pior ou igual ao do laboratório anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoWp43WxJDNT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import files\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  # predicting images\n",
    "  path = '/content/' + fn\n",
    "  img = load_img(path, target_size=(150, 150))\n",
    "  x = img_to_array(img)\n",
    "  x /= 255\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(classes[0])\n",
    "  if classes[0]>0.5:\n",
    "    print(fn + \" is a human\")\n",
    "  else:\n",
    "    print(fn + \" is a horse\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8EHQyWGDvWz"
   },
   "source": [
    "### Visualizando representações intermediárias\n",
    "\n",
    "Você também pode olhar novamente para as representações intermediárias.\n",
    "\n",
    "Você perceberá que a saída na última camada de convolução é ainda mais abstrata porque contém menos pixels do que antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5tES8rXFjux"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# Defina um novo modelo que receberá uma imagem como entrada e produzirá\n",
    "# representações intermediárias para todas as camadas do modelo anterior após\n",
    "# a primeira.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Prepare uma imagem de entrada aleatória do conjunto de treinamento.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "img = load_img(img_path, target_size=(150, 150))  # esta é uma imagem PIL\n",
    "x = img_to_array(img)  # Matriz Numpy com formato (150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Matriz Numpy com formato (1, 150, 150, 3)\n",
    "\n",
    "# Escalar de 1/255\n",
    "x /= 255\n",
    "\n",
    "# Execute a imagem na rede, obtendo assim todas as\n",
    "# representações intermediárias para essa imagem.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# Esses são os nomes das camadas, para que você possa tê-las como parte do gráfico\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Exibir as representações\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  if len(feature_map.shape) == 4:\n",
    "\n",
    "    # Faça isso apenas para as camadas conv / maxpool, não para as camadas totalmente conectadas\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "\n",
    "    # O mapa de recursos tem a forma (1, tamanho, tamanho, n_recursos)\n",
    "    size = feature_map.shape[1]\n",
    "    \n",
    "    # Colocar as imagens em mosaico nessa matriz\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "      x = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std()\n",
    "      x *= 64\n",
    "      x += 128\n",
    "      x = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "      # Colocar cada filtro em uma grade horizontal grande\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "    \n",
    "    # Exibir a grade\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4IBgYCYooGD"
   },
   "source": [
    "## Limpar\n",
    "\n",
    "Execute a seguinte célula para encerrar o kernel e liberar recursos de memória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "651IgjLyo-Jx"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFnBvcIrXWW2"
   },
   "source": [
    "## Resumo\n",
    "\n",
    "Neste laboratório, você viu como a compactação de imagens afetou o modelo anterior. Essa é uma técnica que você deve ter em mente, especialmente quando ainda estiver na fase exploratória de seus próprios projetos. Você pode ver se um modelo menor se comporta tão bem quanto um modelo grande para que o treinamento seja mais rápido. Você também viu como é fácil personalizar suas imagens para esse ajuste de tamanho simplesmente alterando um parâmetro na classe `ImageDataGenerator`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W4_Lab_3_compacted_images.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/adding_C1/C1/W4/ungraded_labs/C1_W4_Lab_3_compacted_images.ipynb",
     "timestamp": 1639112249467
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
