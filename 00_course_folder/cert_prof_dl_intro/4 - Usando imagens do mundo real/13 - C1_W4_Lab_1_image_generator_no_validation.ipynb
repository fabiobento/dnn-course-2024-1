{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2024-1/blob/main/00_course_folder/cert_prof_dl_intro/4%20-%20Usando%20imagens%20do%20mundo%20real/13%20-%20C1_W4_Lab_1_image_generator_no_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adaptado de [Certificado Profissional Desenvolvedor do TensorFlow para DeepLearning.AI](https://www.coursera.org/professional-certificates/tensorflow-in-practice) de [Laurence Moroney](https://laurencemoroney.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-74XLLwqPlcw"
   },
   "source": [
    "# Treinando com ImageDataGenerator\n",
    "\n",
    "Neste laboratório, você criará e treinará um modelo no conjunto de dados [Horses or Humans](https://www.tensorflow.org/datasets/catalog/horses_or_humans).\n",
    "\n",
    "Ele contém mais de mil imagens de cavalos e humanos com poses e tamanhos de arquivo variados.\n",
    "\n",
    "Você usará a classe [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) para preparar esse conjunto de dados para que ele possa ser alimentado em uma rede neural convolucional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYFguQkJvpV3"
   },
   "source": [
    "Execute o código abaixo para fazer o download do conjunto de dados compactado `horse-or-human.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXZT2UsyIVe_"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9brUxyTpYZHy"
   },
   "source": [
    "Em seguida, você pode descompactar o arquivo usando o módulo [zipfile](https://docs.python.org/3/library/zipfile.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLy3pthUS0D2"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Descompactar o conjunto de dados\n",
    "local_zip = './horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./horse-or-human')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-qUPyfO7Qr8"
   },
   "source": [
    "O conteúdo do .zip é extraído para o diretório base `./horse-or-human`, que, por sua vez, contém subdiretórios `horses` e `humans`.\n",
    "\n",
    "Resumindo: o conjunto de treinamento são os dados usados para informar ao modelo de rede neural que \"é assim que um cavalo se parece\" e \"é assim que um humano se parece\".\n",
    "\n",
    "Um aspecto a ser observado nesse exemplo: Não rotulamos explicitamente as imagens como cavalos ou humanos. Em vez disso, você usará a API ImageDataGenerator, que é codificada para rotular automaticamente as imagens de acordo com os nomes e a estrutura do diretório. Assim, por exemplo, você terá um diretório de \"treinamento\" contendo um diretório de \"cavalos\" e um de \"humanos\". O `ImageDataGenerator` rotulará as imagens adequadamente para você, reduzindo uma etapa de codificação. \n",
    "\n",
    "Agora você pode definir cada um desses diretórios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR_M9nWN-K8B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Diretório com nossas fotos de cavalos de treinamento\n",
    "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
    "\n",
    "# Diretório com nossas imagens humanas de treinamento\n",
    "train_human_dir = os.path.join('./horse-or-human/humans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuBYtA_Zd8_T"
   },
   "source": [
    "Agora veja como são os nomes dos arquivos nos diretórios de treinamento `horses` e `humans`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PIP1rkmeAYS"
   },
   "outputs": [],
   "source": [
    "train_horse_names = os.listdir(train_horse_dir)\n",
    "print(train_horse_names[:10])\n",
    "\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(train_human_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlqN5KbafhLI"
   },
   "source": [
    "Você também pode descobrir o número total de imagens de cavalos e humanos nos diretórios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4XHh2xSfgie"
   },
   "outputs": [],
   "source": [
    "print('total de imagens de de cavalos para treinamento:', len(os.listdir(train_horse_dir)))\n",
    "print('total de imagens de humanos para treinamento:', len(os.listdir(train_human_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3WZABE9eX-8"
   },
   "source": [
    "Agora, dê uma olhada em algumas imagens para ter uma noção melhor de como elas são. Primeiro, configure os parâmetros do `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2_Q0-_5UAv-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parâmetros para nosso gráfico; produziremos imagens em uma configuração 4x4\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Índice para iteração de imagens\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTvHzGCxXkqp"
   },
   "source": [
    "Agora, exiba um lote de 8 imagens de cavalos e 8 imagens de humanos.\n",
    "\n",
    "Você pode executar novamente a célula para ver um novo lote a cada vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wpr8GxjOU8in"
   },
   "outputs": [],
   "source": [
    "# Configure o matplotlib fig e dimensione-o para caber em fotos 4x4\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
    "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
    "next_human_pix = [os.path.join(train_human_dir, fname) \n",
    "                for fname in train_human_names[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
    "  # Configure o subplot; os índices do subplot começam em 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off')  # Não mostre os eixos (ou linhas de grade)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oqBkNBJmtUv"
   },
   "source": [
    "## Criando um modelo pequeno do zero\n",
    "\n",
    "Agora você pode definir a arquitetura do modelo que será treinado.\n",
    "\n",
    "A etapa 1 será a importação do tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvfZg3LQbD-5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnhYCP4tdqjC"
   },
   "source": [
    "Em seguida, você adiciona camadas convolucionais como no exemplo anterior e achata o resultado final para alimentar as camadas densamente conectadas.\n",
    "\n",
    "Observe que, como esse é um problema de classificação de duas classes, ou seja, um *problema de classificação binária*, você terminará sua rede com uma ativação [*sigmoidal*](https://wikipedia.org/wiki/Sigmoid_function). Isso faz com que o valor de saída de sua rede seja um único escalar entre 0 e 1, codificando a probabilidade de a imagem atual ser da classe 1 (em oposição à classe 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PixZ2s5QbYQ3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Observe que a forma de entrada é o tamanho desejado da imagem 300x300 com 3 bytes de cor\n",
    "    # Essa é a primeira convolução\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # A segunda convolução\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A terceira convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A quarta convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # A quinta convolução\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Achatar os resultados para alimentar um DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Camada oculta de 512 neurônios\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Apenas 1 neurônio de saída. Ele conterá um valor de 0 a 1, sendo 0 para uma classe (\"cavalos\") e 1 para a outra (\"humanos\")    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9EaFDP5srBa"
   },
   "source": [
    "Você pode revisar a arquitetura da rede e as formas de saída com `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZKj8392nbgP"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmtkTn06pKxF"
   },
   "source": [
    "A coluna \"output shape\" mostra como o tamanho do mapa de recursos evolui em cada camada sucessiva. Como você viu em uma lição anterior, as camadas de convolução removem os pixels mais externos da imagem, e cada camada de pooling reduz as dimensões pela metade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEkKSpZlvJXA"
   },
   "source": [
    "Em seguida, você configurará as especificações para o treinamento do modelo. Você treinará o modelo com a perda [`binary_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) porque se trata de um problema de classificação binária e a ativação final é um sigmoide. (Para uma atualização sobre métricas de perda, consulte este [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture).) Você usará o otimizador `rmsprop` com uma taxa de aprendizado de `0,001`. Durante o treinamento, você desejará monitorar a acurácia da classificação.\n",
    "\n",
    "**NOTA**: Nesse caso, o uso do [algoritmo de otimização RMSprop] (https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) é preferível ao [stochastic gradient descent](https://developers.google.com/machine-learning/glossary/#SGD) (SGD), porque o RMSprop automatiza o ajuste da taxa de aprendizado para nós. (Outros otimizadores, como [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) e [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), também adaptam automaticamente a taxa de aprendizado durante o treinamento e funcionariam igualmente bem aqui)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DHWhFP_uhq3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn9m9D3UimHM"
   },
   "source": [
    "### Pré-processamento de dados\n",
    "\n",
    "A próxima etapa é configurar os geradores de dados que lerão as imagens nas pastas de origem, convertê-las em tensores `float32` e alimentá-las (com seus rótulos) ao modelo.Você terá um gerador para as imagens de treinamento e outro para as imagens de validação.Esses geradores produzirão lotes de imagens de tamanho 300x300 e seus rótulos (binários).\n",
    "\n",
    "Como você já deve saber, os dados que entram nas redes neurais geralmente devem ser normalizados de alguma forma para torná-los mais fáceis de serem processados pela rede (ou seja, não é comum alimentar uma ConvNet com pixels brutos). Nesse caso, você pré-processará as imagens normalizando os valores de pixel para que fiquem no intervalo `[0, 1]` (originalmente todos os valores estão no intervalo `[0, 255]`).\n",
    "\n",
    "No Keras, isso pode ser feito por meio da classe `keras.preprocessing.image.ImageDataGenerator` usando o parâmetro `rescale`.Essa classe `ImageDataGenerator` permite que você instancie geradores de lotes de imagens aumentadas (e seus rótulos) por meio de `.flow(data, labels)` ou `.flow_from_directory(directory)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClebU9NJg99G"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Todas as imagens serão redimensionadas em 1,/255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Imagens de treinamento de fluxo em lotes de 128 usando o gerador train_datagen\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './horse-or-human/',  # Esse é o diretório de origem das imagens de treinamento\n",
    "        target_size=(300, 300),  # Todas as imagens serão redimensionadas para 300x300\n",
    "        batch_size=128,\n",
    "        # Como usamos a perda binary_crossentropy, precisamos de rótulos binários\n",
    "        class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu3Jdwkjwax4"
   },
   "source": [
    "### Treinamento\n",
    "\n",
    "Você pode iniciar o treinamento para 15 épocas - isso pode levar alguns minutos para ser executado.\n",
    "\n",
    "Observe os valores por época.\n",
    "\n",
    "A \"perda\" e a \"acurácia\" são ótimos indicadores do progresso do treinamento. O `loss` mede a previsão do modelo atual em relação aos rótulos conhecidos, calculando o resultado. A \"acurácia\", por outro lado, é a porção de suposições corretas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb1_lgobv81m"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=15,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6vSHzPR2ghH"
   },
   "source": [
    "### Previsão do modelo\n",
    "\n",
    "Agora, dê uma olhada na execução de uma previsão usando o modelo. Esse código permitirá que você escolha um ou mais arquivos do seu sistema de arquivos, carregue-os e execute-os por meio do modelo, fornecendo uma indicação de que o objeto é um cavalo ou um humano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoWp43WxJDNT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import files\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "   # Previsão de imagens\n",
    "  path = '/content/' + fn\n",
    "  img = load_img(path, target_size=(300, 300))\n",
    "  x = img_to_array(img)\n",
    "  x /= 255\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(classes[0])\n",
    "    \n",
    "  if classes[0]>0.5:\n",
    "    print(fn + \" é um humano\")\n",
    "  else:\n",
    "    print(fn + \" é um cavalo\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8EHQyWGDvWz"
   },
   "source": [
    "### Visualizando representações intermediárias\n",
    "\n",
    "Para ter uma ideia do tipo de recursos que sua CNN aprendeu, uma coisa divertida a se fazer é visualizar como uma entrada é transformada à medida que passa pelo modelo.\n",
    "\n",
    "Você pode escolher uma imagem aleatória do conjunto de treinamento e, em seguida, gerar uma figura em que cada linha é a saída de uma camada e cada imagem na linha é um filtro específico nesse mapa de recursos de saída. Execute novamente essa célula para gerar representações intermediárias para uma variedade de imagens de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5tES8rXFjux"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# Defina um novo modelo que receberá uma imagem como entrada e produzirá\n",
    "# representações intermediárias para todas as camadas do modelo anterior após\n",
    "# a primeira.\n",
    "\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Prepare uma imagem de entrada aleatória do conjunto de treinamento.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = load_img(img_path, target_size=(300, 300))  # esta é uma imagem PIL\n",
    "x = img_to_array(img) # Matriz Numpy com formato (300, 300, 3)\n",
    "x = x.reshape((1,) + x.shape)   # Matriz Numpy com formato (1, 300, 300, 3)\n",
    "\n",
    "# Escalar de 1/255\n",
    "x /= 255\n",
    "\n",
    "# Execute a imagem na rede, obtendo assim todas as\n",
    "# representações intermediárias para essa imagem.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# Esses são os nomes das camadas, para que você possa tê-las como parte do gráfico\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Exibir as representações\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  if len(feature_map.shape) == 4:\n",
    "\n",
    "    # Faça isso apenas para as camadas conv / maxpool, não para as camadas totalmente conectadas\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "\n",
    "    # Faça isso apenas para as camadas conv / maxpool, não para as camadas totalmente conectadas\n",
    "    size = feature_map.shape[1]\n",
    "    \n",
    "    # Colocar as imagens em mosaico nessa matriz\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "      x = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std()\n",
    "      x *= 64\n",
    "      x += 128\n",
    "      x = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "      # Colocar cada filtro em uma grade horizontal grande\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "    \n",
    "    # Exibir a grade\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuqK2arJL0wo"
   },
   "source": [
    "Você pode ver acima como os pixels destacados se transformam em representações cada vez mais abstratas e compactas, especialmente na grade inferior. \n",
    "\n",
    "As representações a jusante começam a destacar aquilo a que a rede presta atenção e mostram cada vez menos recursos sendo \"ativados\"; a maioria é definida como zero.\n",
    "\n",
    "Isso é chamado de _esparsidade de representação_ e é um recurso fundamental da aprendizagem profunda.\n",
    "\n",
    "Essas representações carregam cada vez menos informações sobre os pixels originais da imagem, mas informações cada vez mais refinadas sobre a classe da imagem.\n",
    "\n",
    "Você pode pensar em uma convnet (ou em uma rede profunda em geral) como um pipeline de destilação de informações em que cada camada filtra os recursos mais úteis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4IBgYCYooGD"
   },
   "source": [
    "## Limpar\n",
    "\n",
    "Você continuará com um exercício semelhante no próximo laboratório, mas, antes disso, execute a seguinte célula para encerrar o kernel e liberar recursos de memória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "651IgjLyo-Jx"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W4_Lab_1_image_generator_no_validation.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
